{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base e encoder manual ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    '../data/heart/processed/heart.csv',\n",
    "    sep = ';', encoding = 'utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encod_manual = pd.DataFrame.copy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encod_manual['Sex'].replace({\n",
    "    'M': 0,\n",
    "    'F': 1\n",
    "}, inplace = True)\n",
    "\n",
    "df_encod_manual['ChestPainType'].replace({\n",
    "    'TA': 0,\n",
    "    'ATA': 1,\n",
    "    'NAP': 2,\n",
    "    'ASY': 3\n",
    "}, inplace = True)\n",
    "\n",
    "df_encod_manual['RestingECG'].replace({\n",
    "    'Normal': 0,\n",
    "    'ST': 1,\n",
    "    'LVH': 2\n",
    "}, inplace = True)\n",
    "\n",
    "df_encod_manual['ExerciseAngina'].replace({\n",
    "    'N': 0,\n",
    "    'Y': 1\n",
    "}, inplace = True)\n",
    "\n",
    "df_encod_manual['ST_Slope'].replace({\n",
    "    'Up': 0,\n",
    "    'Flat': 1,\n",
    "    'Down': 2\n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação da base em previsores e classe alvo ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = df_encod_manual.iloc[:, 0:11].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alvo = df_encod_manual.iloc[:, 11].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalonamento ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_esc = StandardScaler().fit_transform(previsores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_df = pd.DataFrame(previsores_esc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabelEncoder ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_label = df.iloc[:, 0:11].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_label[:, 1] = LabelEncoder().fit_transform(previsores[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_label[:, 2] = LabelEncoder().fit_transform(previsores_label[:, 2])\n",
    "previsores_label[:, 6] = LabelEncoder().fit_transform(previsores_label[:, 6])\n",
    "previsores_label[:, 8] = LabelEncoder().fit_transform(previsores_label[:, 8])\n",
    "previsores_label[:, 10] = LabelEncoder().fit_transform(previsores_label[:, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncoder ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_hot = ColumnTransformer(\n",
    "    transformers = [('OneHot', OneHotEncoder(), [1, 2, 6, 8, 10])],\n",
    "    remainder = 'passthrough'\n",
    ").fit_transform(previsores_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_hot_df = pd.DataFrame(previsores_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHot + Escalonamento ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoresHot_esc = StandardScaler().fit_transform(previsores_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoresHot_esc_df = pd.DataFrame(previsoresHot_esc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação dos dados em treino e teste ##\n",
    "<span style=\"font-size: small;\"> \n",
    "- <strong>arrays:</strong> nomes dos atributos previsores e alvo.</br>\n",
    "- <strong>test_size:</strong> tamanho em porcentagem dos dados de teste. default é none. </br> \n",
    "- <strong>train_size:</strong> tamanho em porcentagem dos dados de treinamento.default é none. </br>  \n",
    "- <strong>random_state:</strong> nomeação de um estado aleatório. </br>\n",
    "- <strong>shuffle:</strong> embaralhamento dos dados aleatórios. Associado com o random_state ocorre o mesmo embaralhamento sempre. Default é True. </br>\n",
    "- <strong>stratify:</strong> Possibilidade de dividir os dados de forma estratificada. Default é None (nesse caso é mantido a proporção, isto é, se tem 30% de zeros e 70% de 1 no dataframe, na separação em treinamento e teste se manterá essa proporção). </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    previsores, alvo,\n",
    "    test_size = 0.3, random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsores #\n",
    "<span style=\"font-size: 13px;\">\n",
    "<li> <strong>previsores</strong> -> Atributos codificados manualmente sem escalonamento.</li></br>\n",
    "<li> <strong>previsoresHot_esc</strong> -> Atributos codificados com LabelEncoder e OneHotEncoder e escalonados.</li></br>\n",
    "<li> <strong>previsores_esc</strong> -> Atributos codificados manualmente e escalonados.</li></br>\n",
    "<li> <strong>previsores_hot</strong> -> Atributos codificados com OneHotEncoder sem escalonamento.</li></br>\n",
    "<li> <strong>previsores_label</strong> -> Atributos codificados com LabelEncoder e sem escalonamento. </li></br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM #\n",
    "\n",
    "<span style=\"font-size: 13px;\">\n",
    "O LightGBM baseia-se no conceito de Gradient Boosting, que combina vários modelos de aprendizado de máquina fracos para criar um modelo mais robusto. Ele utiliza árvores de decisão como modelos base.\n",
    "\n",
    "**Gradient Boosting:**\n",
    "\n",
    "O Gradient Boosting é uma técnica de aprendizado de máquina que cria um modelo preditivo combinando várias iterações de modelos simples. Cada modelo é ajustado para corrigir os erros do modelo anterior.\n",
    "\n",
    "**Árvores de Decisão:**\n",
    "\n",
    "As árvores de decisão são estruturas de árvore que dividem os dados em subconjuntos com base nas características, buscando maximizar a precisão da previsão.\n",
    "\n",
    "**Fórmulas**\n",
    "\n",
    "**Função de Perda (Loss Function):**\n",
    "\n",
    "A função de perda é usada para medir a discrepância entre as previsões do modelo e os valores reais. Exemplos incluem a função de perda de entropia cruzada para classificação e a função de perda de erro quadrático médio para regressão.\n",
    "\n",
    "**Gradiente e Hessiana:**\n",
    "\n",
    "O Gradient Boosting otimiza a função de perda ajustando os modelos subsequentes na direção do gradiente da função de perda. O Hessiano é usado para ajustar a taxa de aprendizado durante o treinamento.\n",
    "\n",
    "\n",
    "**Hiperparâmetros:**\n",
    "\n",
    "- `num_leaves`: O número máximo de folhas em cada árvore.\n",
    "- `learning_rate`: Taxa de aprendizado para atualizar os modelos.\n",
    "- `max_depth`: A profundidade máxima de cada árvore.\n",
    "- `min_data_in_leaf`: O número mínimo de amostras em cada folha.\n",
    "\n",
    "**Parâmetros:**\n",
    "\n",
    "- `objective`: O tipo de problema a ser resolvido, como \"regressão\" ou \"classificação\".\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração do LightGBM ##\n",
    "<span style=\"font-size: 13px;\">\n",
    "\n",
    "**Hiperparâmetros** </br>\n",
    "\n",
    "**Controle de ajuste** </br>\n",
    "<li>num_leaves : define o número de folhas a serem formadas em uma árvore. Não tem uma relação direta entre num_leaves e max_depth e, portanto, os dois não devem estar vinculados um ao outro. </li>\n",
    "\n",
    "<li>max_depth : especifica a profundidade máxima ou nível até o qual a árvore pode crescer.  </li>\n",
    "\n",
    "**Controle de velocidade** </br>\n",
    "\n",
    "<li>learning_rate: taxa de aprendizagem, determina o impacto de cada árvore no resultado final.</li>\n",
    "\n",
    "<li>max_bin : O valor menor de max_bin reduz muito tempo de procesamento, pois agrupa os valores do recurso em caixas discretas, o que é computacionalmente mais barato.</li>\n",
    "\n",
    "**Controle de precisão** </br>\n",
    "\n",
    "<li>num_leaves : valor alto produz árvores mais profundas com maior precisão, mas leva ao overfitting.</li>\n",
    "\n",
    "<li>max_bin : valores altos tem efeito semelhante ao causado pelo aumento do valor de num_leaves e também torna mais lento o procedimento de treinamento.</li>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criar um DataSet para treino**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = lgb.Dataset(\n",
    "    x_train, label = y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dict de parametros e treinamento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'num_leaves': 150,\n",
    "    'objective': 'binary',\n",
    "    'max_depth': 2,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_bin': 200,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 352, number of negative: 289\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 394\n",
      "[LightGBM] [Info] Number of data points in the train set: 641, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.549142 -> initscore=0.197204\n",
      "[LightGBM] [Info] Start training from score 0.197204\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "lgbm = lgb.train(\n",
    "    param, dataset,\n",
    "    num_boost_round = 150,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previsoes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes_test = lgbm.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As previsões não retornam valores binários, por isso devemos manualmente realizar a binarização, no caso das previsões acima, quando o valor for menor que 5 iremos considerar 0, e quando for maior ou igual a 5 iremos considerar como 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 276):\n",
    "    if previsoes_test[i] >= .5:\n",
    "        previsoes_test[i] = 1\n",
    "    else:\n",
    "        previsoes_test[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Métricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 85.51%\n"
     ]
    }
   ],
   "source": [
    "acuracia_test = accuracy_score(y_test, previsoes_test) * 100.0\n",
    "print(\"Acurácia: %.2f%%\" % acuracia_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101  20]\n",
      " [ 20 135]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, previsoes_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83       121\n",
      "           1       0.87      0.87      0.87       155\n",
      "\n",
      "    accuracy                           0.86       276\n",
      "   macro avg       0.85      0.85      0.85       276\n",
      "weighted avg       0.86      0.86      0.86       276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, previsoes_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análise dos dados de treino(verificar overfitting)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes_train = lgbm.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 641):\n",
    "    if previsoes_train[i] >= .5:\n",
    "        previsoes_train[i] = 1\n",
    "    else:\n",
    "        previsoes_train[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 91.26%\n"
     ]
    }
   ],
   "source": [
    "acuracia_train = accuracy_score(y_train, previsoes_train) * 100.0\n",
    "print(\"Acurácia: %.2f%%\" % acuracia_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[252  37]\n",
      " [ 19 333]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, previsoes_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits = 30, shuffle = True, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 489, number of negative: 397\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 436\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551919 -> initscore=0.208426\n",
      "[LightGBM] [Info] Start training from score 0.208426\n",
      "[LightGBM] [Info] Number of positive: 490, number of negative: 396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 439\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.553047 -> initscore=0.212991\n",
      "[LightGBM] [Info] Start training from score 0.212991\n",
      "[LightGBM] [Info] Number of positive: 486, number of negative: 400\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 432\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.548533 -> initscore=0.194744\n",
      "[LightGBM] [Info] Start training from score 0.194744\n",
      "[LightGBM] [Info] Number of positive: 487, number of negative: 399\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 436\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.549661 -> initscore=0.199303\n",
      "[LightGBM] [Info] Start training from score 0.199303\n",
      "[LightGBM] [Info] Number of positive: 490, number of negative: 396\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 436\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.553047 -> initscore=0.212991\n",
      "[LightGBM] [Info] Start training from score 0.212991\n",
      "[LightGBM] [Info] Number of positive: 489, number of negative: 397\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 437\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551919 -> initscore=0.208426\n",
      "[LightGBM] [Info] Start training from score 0.208426\n",
      "[LightGBM] [Info] Number of positive: 490, number of negative: 396\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 435\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.553047 -> initscore=0.212991\n",
      "[LightGBM] [Info] Start training from score 0.212991\n",
      "[LightGBM] [Info] Number of positive: 491, number of negative: 395\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 435\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.554176 -> initscore=0.217558\n",
      "[LightGBM] [Info] Start training from score 0.217558\n",
      "[LightGBM] [Info] Number of positive: 491, number of negative: 395\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 437\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.554176 -> initscore=0.217558\n",
      "[LightGBM] [Info] Start training from score 0.217558\n",
      "[LightGBM] [Info] Number of positive: 490, number of negative: 396\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 437\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.553047 -> initscore=0.212991\n",
      "[LightGBM] [Info] Start training from score 0.212991\n",
      "[LightGBM] [Info] Number of positive: 492, number of negative: 394\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 438\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.555305 -> initscore=0.222128\n",
      "[LightGBM] [Info] Start training from score 0.222128\n",
      "[LightGBM] [Info] Number of positive: 484, number of negative: 402\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 437\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546275 -> initscore=0.185633\n",
      "[LightGBM] [Info] Start training from score 0.185633\n",
      "[LightGBM] [Info] Number of positive: 489, number of negative: 397\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 435\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551919 -> initscore=0.208426\n",
      "[LightGBM] [Info] Start training from score 0.208426\n",
      "[LightGBM] [Info] Number of positive: 489, number of negative: 397\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 439\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551919 -> initscore=0.208426\n",
      "[LightGBM] [Info] Start training from score 0.208426\n",
      "[LightGBM] [Info] Number of positive: 494, number of negative: 392\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 438\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.557562 -> initscore=0.231274\n",
      "[LightGBM] [Info] Start training from score 0.231274\n",
      "[LightGBM] [Info] Number of positive: 488, number of negative: 398\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 437\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.550790 -> initscore=0.203863\n",
      "[LightGBM] [Info] Start training from score 0.203863\n",
      "[LightGBM] [Info] Number of positive: 489, number of negative: 397\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000129 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 438\n",
      "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551919 -> initscore=0.208426\n",
      "[LightGBM] [Info] Start training from score 0.208426\n",
      "[LightGBM] [Info] Number of positive: 488, number of negative: 399\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 436\n",
      "[LightGBM] [Info] Number of data points in the train set: 887, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.550169 -> initscore=0.201354\n",
      "[LightGBM] [Info] Start training from score 0.201354\n",
      "[LightGBM] [Info] Number of positive: 490, number of negative: 397\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 436\n",
      "[LightGBM] [Info] Number of data points in the train set: 887, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552424 -> initscore=0.210469\n",
      "[LightGBM] [Info] Start training from score 0.210469\n",
      "[LightGBM] [Info] Number of positive: 490, number of negative: 397\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 434\n",
      "[LightGBM] [Info] Number of data points in the train set: 887, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552424 -> initscore=0.210469\n",
      "[LightGBM] [Info] Start training from score 0.210469\n",
      "[LightGBM] [Info] Number of positive: 491, number of negative: 396\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 434\n",
      "[LightGBM] [Info] Number of data points in the train set: 887, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.553551 -> initscore=0.215030\n",
      "[LightGBM] [Info] Start training from score 0.215030\n",
      "[LightGBM] [Info] Number of positive: 491, number of negative: 396\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 435\n",
      "[LightGBM] [Info] Number of data points in the train set: 887, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.553551 -> initscore=0.215030\n",
      "[LightGBM] [Info] Start training from score 0.215030\n",
      "[LightGBM] [Info] Number of positive: 493, number of negative: 394\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 437\n",
      "[LightGBM] [Info] Number of data points in the train set: 887, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.555806 -> initscore=0.224158\n",
      "[LightGBM] [Info] Start training from score 0.224158\n",
      "[LightGBM] [Info] Number of positive: 494, number of negative: 393\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 437\n",
      "[LightGBM] [Info] Number of data points in the train set: 887, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.556933 -> initscore=0.228726\n",
      "[LightGBM] [Info] Start training from score 0.228726\n",
      "[LightGBM] [Info] Number of positive: 493, number of negative: 394\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 436\n",
      "[LightGBM] [Info] Number of data points in the train set: 887, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.555806 -> initscore=0.224158\n",
      "[LightGBM] [Info] Start training from score 0.224158\n",
      "[LightGBM] [Info] Number of positive: 487, number of negative: 400\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 437\n",
      "[LightGBM] [Info] Number of data points in the train set: 887, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.549042 -> initscore=0.196800\n",
      "[LightGBM] [Info] Start training from score 0.196800\n",
      "[LightGBM] [Info] Number of positive: 489, number of negative: 398\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 435\n",
      "[LightGBM] [Info] Number of data points in the train set: 887, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551297 -> initscore=0.205910\n",
      "[LightGBM] [Info] Start training from score 0.205910\n",
      "[LightGBM] [Info] Number of positive: 492, number of negative: 395\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 437\n",
      "[LightGBM] [Info] Number of data points in the train set: 887, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.554679 -> initscore=0.219593\n",
      "[LightGBM] [Info] Start training from score 0.219593\n",
      "[LightGBM] [Info] Number of positive: 493, number of negative: 394\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 436\n",
      "[LightGBM] [Info] Number of data points in the train set: 887, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.555806 -> initscore=0.224158\n",
      "[LightGBM] [Info] Start training from score 0.224158\n",
      "[LightGBM] [Info] Number of positive: 494, number of negative: 393\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 436\n",
      "[LightGBM] [Info] Number of data points in the train set: 887, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.556933 -> initscore=0.228726\n",
      "[LightGBM] [Info] Start training from score 0.228726\n",
      "Acurácia média: 85.82%\n"
     ]
    }
   ],
   "source": [
    "modelo = lgb.LGBMClassifier(\n",
    "    num_leaves = 150,\n",
    "    objective = 'binary',\n",
    "    max_depth = 2,\n",
    "    learning_rate = 0.05,\n",
    "    max_bin = 200\n",
    ")\n",
    "resultado = cross_val_score(modelo, previsores, alvo, cv = kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 85.82%\n"
     ]
    }
   ],
   "source": [
    "print(\"Acurácia média: %.2f%%\" % (resultado.mean() * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
