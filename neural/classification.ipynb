{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base e encoder manual ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    '../data/heart/processed/heart.csv',\n",
    "    sep = ';', encoding = 'utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encod_manual = pd.DataFrame.copy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encod_manual['Sex'].replace({\n",
    "    'M': 0,\n",
    "    'F': 1\n",
    "}, inplace = True)\n",
    "\n",
    "df_encod_manual['ChestPainType'].replace({\n",
    "    'TA': 0,\n",
    "    'ATA': 1,\n",
    "    'NAP': 2,\n",
    "    'ASY': 3\n",
    "}, inplace = True)\n",
    "\n",
    "df_encod_manual['RestingECG'].replace({\n",
    "    'Normal': 0,\n",
    "    'ST': 1,\n",
    "    'LVH': 2\n",
    "}, inplace = True)\n",
    "\n",
    "df_encod_manual['ExerciseAngina'].replace({\n",
    "    'N': 0,\n",
    "    'Y': 1\n",
    "}, inplace = True)\n",
    "\n",
    "df_encod_manual['ST_Slope'].replace({\n",
    "    'Up': 0,\n",
    "    'Flat': 1,\n",
    "    'Down': 2\n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação da base em previsores e classe alvo ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = df_encod_manual.iloc[:, 0:11].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "alvo = df_encod_manual.iloc[:, 11].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalonamento ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_esc = StandardScaler().fit_transform(previsores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_df = pd.DataFrame(previsores_esc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabelEncoder ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_label = df.iloc[:, 0:11].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_label[:, 1] = LabelEncoder().fit_transform(previsores[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_label[:, 2] = LabelEncoder().fit_transform(previsores_label[:, 2])\n",
    "previsores_label[:, 6] = LabelEncoder().fit_transform(previsores_label[:, 6])\n",
    "previsores_label[:, 8] = LabelEncoder().fit_transform(previsores_label[:, 8])\n",
    "previsores_label[:, 10] = LabelEncoder().fit_transform(previsores_label[:, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncoder ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_hot = ColumnTransformer(\n",
    "    transformers = [('OneHot', OneHotEncoder(), [1, 2, 6, 8, 10])],\n",
    "    remainder = 'passthrough'\n",
    ").fit_transform(previsores_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_hot_df = pd.DataFrame(previsores_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHot + Escalonamento ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoresHot_esc = StandardScaler().fit_transform(previsores_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoresHot_esc_df = pd.DataFrame(previsoresHot_esc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação dos dados em treino e teste ##\n",
    "<span style=\"font-size: small;\"> \n",
    "- <strong>arrays:</strong> nomes dos atributos previsores e alvo.</br>\n",
    "- <strong>test_size:</strong> tamanho em porcentagem dos dados de teste. default é none. </br> \n",
    "- <strong>train_size:</strong> tamanho em porcentagem dos dados de treinamento.default é none. </br>  \n",
    "- <strong>random_state:</strong> nomeação de um estado aleatório. </br>\n",
    "- <strong>shuffle:</strong> embaralhamento dos dados aleatórios. Associado com o random_state ocorre o mesmo embaralhamento sempre. Default é True. </br>\n",
    "- <strong>stratify:</strong> Possibilidade de dividir os dados de forma estratificada. Default é None (nesse caso é mantido a proporção, isto é, se tem 30% de zeros e 70% de 1 no dataframe, na separação em treinamento e teste se manterá essa proporção). </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    previsoresHot_esc, alvo,\n",
    "    test_size = 0.3, random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1405, 20)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(603, 20)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1405,)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(603,)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo do pré-processamento\n",
    "\n",
    "<span style=\"font-size: 13px; font-family: 'Trebuchet MS', sans-serif;\">\n",
    "\n",
    "- alvo = variável que se pretende atingir (tem ou não doença cardíaca).</br> </br>\n",
    "- previsores = conjunto de variáveis previsoras com as variáveis categóricas transformadas em numéricas manualmente, sem escalonar.</br></br>\n",
    "- previsores_esc = conjunto de variáveis previsoras com as variáveis categóricas transformadas em numéricas, escalonada. </br></br>\n",
    "- previsores_label = conjunto de variáveis previsoras com as variáveis categóricas transformadas em numéricas pelo labelencoder. </br></br>\n",
    "- previsores_hot = conjunto de variáveis previsoras transformadas pelo labelencoder e onehotencoder, sem escalonar.</br></br>\n",
    "- previsores_hot_esc = conjunto de variáveis previsoras transformadas pelo labelencoder e onehotencoder escalonada.</br></br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação com redes neurais \n",
    "<span style=\"font-size: 13px; font-family: 'Trebuchet MS', sans-serif;\">\n",
    "\n",
    "As redes neurais artificiais (RNAs) são modelos computacionais inspirados no funcionamento do cérebro humano. </br> \n",
    "Elas são compostas por neurônios artificiais interconectados, organizados em camadas, e são capazes de aprender a partir de dados. </br>\n",
    "\n",
    "**Estrutura das RNAs** </br>\n",
    "Uma RNA é composta por várias camadas: </br>\n",
    "- **Camada de Entrada:** Recebe os dados de entrada e transmite para as camadas ocultas. </br>\n",
    "- **Camadas Ocultas:** Processam os dados de entrada através de uma série de transformações não-lineares. </br>\n",
    "- **Camada de Saída:** Produz a saída final da rede após o processamento nas camadas ocultas. </br>\n",
    "\n",
    "**Funcionamento das RNAs** </br>\n",
    "As RNAs funcionam através de duas etapas principais: feedforward e backpropagation. </br>\n",
    "- **Feedforward:** Durante esta etapa, os dados de entrada são propagados através da rede, camada por camada, até que a saída seja calculada. </br>\n",
    "- **Backpropagation:** Durante esta etapa, o erro entre a saída prevista e a saída real é retropropagado pela rede, ajustando os pesos das conexões para minimizar o erro. </br>\n",
    "\n",
    "**Treinamento das RNAs** </br>\n",
    "- As RNAs são treinadas utilizando algoritmos de otimização, como o gradiente descendente, para ajustar os pesos das conexões de forma a minimizar uma função de custo ou perda.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetros MLPClassifier\n",
    "<span style=\"font-size: 13px; font-family: 'Trebuchet MS', sans-serif;\">\n",
    "\n",
    "- **hidden_layer_sizes (camadas escondidas)**: default (100) </br>\n",
    "- **NE** = Neuronios de ENTRADA </br>\n",
    "- **NS** = Neuronios de SAIDA </br>\n",
    "\n",
    "**FÓRMULA:**\n",
    "\n",
    "- **Quant.**= $(NE+NS)/2$ = $(11+1)/2$ = $6$ neurônios </br>\n",
    "- **Quant.**= $2/3.(NE)$ + $NS = 2/3.11+1$ = $8$ neurônios </br>\n",
    "---\n",
    "**activation**: Função de ativação default= 'relu' </br>\n",
    "- **'identity'**: $f(x) = x$ </br>\n",
    "Esta função retorna simplesmente a entrada, útil quando nenhuma ativação é desejada.\n",
    "- **'logistic'**: $f(x) = \\frac{1}{1 + e^{-x}}$ </br>\n",
    "Também conhecida como função sigmóide, esta função transforma a entrada em um intervalo entre 0 e 1.\n",
    "\n",
    "- **'tanh'**: $f(x) = \\tanh(x)$ </br>\n",
    "A função tangente hiperbólica, que mapeia a entrada para um intervalo entre -1 e 1.\n",
    "\n",
    "- **'relu'**: $f(x) = \\max(0, x)$ </br>\n",
    "A função de ativação ReLU (Unidade Linear Retificada), que retorna 0 para valores negativos e a própria entrada para valores positivos.\n",
    "\n",
    "---\n",
    "\n",
    "**solver:** \n",
    "algoritmo matemático. Default='adam' </br>\n",
    "- **adam** é para datasets grandes = acima de 1000 amostras\n",
    "- **lbfgs** é para datasets pequenos.\n",
    "- **sgd** é com a descida do gradiente estocástico (recomendado testar).\n",
    "- **alpha**: parâmetro para o termo de regularização de ajuste de pesos.  \n",
    "Aumento de alpha estimula pesos menores e diminuição de alpha estimula pesos maiores. Default=0.0001. </br>  \n",
    "\n",
    "---\n",
    "**batch_size: tamanho dos mini lotes.**\n",
    "- default=min(200, n_samples). </br>\n",
    "*Não usar com o solver lbfgs.*\n",
    "\n",
    "---\n",
    "\n",
    "**learning_rate:** taxa de aprendizagem. default='constant'. Três tipos:\n",
    "\n",
    "- **'constant'**: uma taxa de aprendizado constante dada pela taxa de aprendizagem inicial.\n",
    "- **'invscaling'**: diminui gradualmente por:  taxa efetiva = taxa inicial / $ t^power_t $\n",
    "- **'adaptive'**: a taxa é dividida por 5 cada vez que em duas épocas consecutivas não diminuir o erro\n",
    "\n",
    "---\n",
    "\n",
    "**learning_rate_init**: taxa de aprendizagem inicial. \n",
    "- Default=0.001  \n",
    "\n",
    "---\n",
    "\n",
    "**max_iter int**: Número máximo de iterações. \n",
    "- default = 200.\n",
    "('sgd', 'adam').  \n",
    "\n",
    "---\n",
    "\n",
    "**max_fun**: Número máximo de chamadas de função de perda.  \n",
    "- Para 'lbfgs'. Default: 15000  \n",
    "\n",
    "---\n",
    "\n",
    "**shuffle**: default = True  \n",
    "- Usado apenas quando solver = 'sgd' ou 'adam'.  \n",
    "\n",
    "---\n",
    "\n",
    "**random_state**: \n",
    "- default = None  \n",
    "\n",
    "---\n",
    "\n",
    "**tol**: Tolerância para a otimização. \n",
    "- Default=0.0001  \n",
    "\n",
    "---\n",
    "\n",
    "**momentum**: otimização do algoritmo 'sgd'. \n",
    "- Default: 0.9. \n",
    "\n",
    "---\n",
    "\n",
    "**n_iter_no_change**: Número máximo de épocas que não atinge a tolerância de melhoria. default = 10.  \n",
    "- Apenas para solver = 'sgd' ou 'adam'  \n",
    "\n",
    "---\n",
    "\n",
    "**verbose**: Mostra o progresso. \n",
    "- default=False.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "redes = MLPClassifier(\n",
    "    hidden_layer_sizes = (7, 7, 7), activation = 'relu',\n",
    "    solver = 'adam', max_iter = 1000, alpha = 0.1,\n",
    "    batch_size = 128, learning_rate = 'adaptive',\n",
    "    learning_rate_init = 0.001,\n",
    "    tol = 0.0001, random_state = 3, verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.77171416\n",
      "Iteration 2, loss = 0.75299658\n",
      "Iteration 3, loss = 0.73646631\n",
      "Iteration 4, loss = 0.72196969\n",
      "Iteration 5, loss = 0.70970739\n",
      "Iteration 6, loss = 0.69898574\n",
      "Iteration 7, loss = 0.68935610\n",
      "Iteration 8, loss = 0.68019868\n",
      "Iteration 9, loss = 0.67112192\n",
      "Iteration 10, loss = 0.66117823\n",
      "Iteration 11, loss = 0.65043337\n",
      "Iteration 12, loss = 0.63829468\n",
      "Iteration 13, loss = 0.62472763\n",
      "Iteration 14, loss = 0.61140656\n",
      "Iteration 15, loss = 0.59756722\n",
      "Iteration 16, loss = 0.58388369\n",
      "Iteration 17, loss = 0.57178924\n",
      "Iteration 18, loss = 0.56086005\n",
      "Iteration 19, loss = 0.55062185\n",
      "Iteration 20, loss = 0.54158110\n",
      "Iteration 21, loss = 0.53371071\n",
      "Iteration 22, loss = 0.52701105\n",
      "Iteration 23, loss = 0.52039578\n",
      "Iteration 24, loss = 0.51450930\n",
      "Iteration 25, loss = 0.50924772\n",
      "Iteration 26, loss = 0.50359208\n",
      "Iteration 27, loss = 0.49837053\n",
      "Iteration 28, loss = 0.49360359\n",
      "Iteration 29, loss = 0.48914713\n",
      "Iteration 30, loss = 0.48501683\n",
      "Iteration 31, loss = 0.48105252\n",
      "Iteration 32, loss = 0.47682837\n",
      "Iteration 33, loss = 0.47315644\n",
      "Iteration 34, loss = 0.46929017\n",
      "Iteration 35, loss = 0.46566165\n",
      "Iteration 36, loss = 0.46236949\n",
      "Iteration 37, loss = 0.45906632\n",
      "Iteration 38, loss = 0.45581407\n",
      "Iteration 39, loss = 0.45282720\n",
      "Iteration 40, loss = 0.44989957\n",
      "Iteration 41, loss = 0.44695618\n",
      "Iteration 42, loss = 0.44418076\n",
      "Iteration 43, loss = 0.44165277\n",
      "Iteration 44, loss = 0.43879531\n",
      "Iteration 45, loss = 0.43619159\n",
      "Iteration 46, loss = 0.43341222\n",
      "Iteration 47, loss = 0.43089377\n",
      "Iteration 48, loss = 0.42842348\n",
      "Iteration 49, loss = 0.42602162\n",
      "Iteration 50, loss = 0.42355788\n",
      "Iteration 51, loss = 0.42101766\n",
      "Iteration 52, loss = 0.41895859\n",
      "Iteration 53, loss = 0.41644745\n",
      "Iteration 54, loss = 0.41443556\n",
      "Iteration 55, loss = 0.41198519\n",
      "Iteration 56, loss = 0.41028189\n",
      "Iteration 57, loss = 0.40765185\n",
      "Iteration 58, loss = 0.40561642\n",
      "Iteration 59, loss = 0.40357888\n",
      "Iteration 60, loss = 0.40158010\n",
      "Iteration 61, loss = 0.39976092\n",
      "Iteration 62, loss = 0.39791664\n",
      "Iteration 63, loss = 0.39580764\n",
      "Iteration 64, loss = 0.39379109\n",
      "Iteration 65, loss = 0.39204951\n",
      "Iteration 66, loss = 0.39006580\n",
      "Iteration 67, loss = 0.38839230\n",
      "Iteration 68, loss = 0.38646097\n",
      "Iteration 69, loss = 0.38457113\n",
      "Iteration 70, loss = 0.38287977\n",
      "Iteration 71, loss = 0.38121604\n",
      "Iteration 72, loss = 0.37960086\n",
      "Iteration 73, loss = 0.37793046\n",
      "Iteration 74, loss = 0.37630016\n",
      "Iteration 75, loss = 0.37462161\n",
      "Iteration 76, loss = 0.37311753\n",
      "Iteration 77, loss = 0.37153492\n",
      "Iteration 78, loss = 0.37005639\n",
      "Iteration 79, loss = 0.36843092\n",
      "Iteration 80, loss = 0.36704150\n",
      "Iteration 81, loss = 0.36564651\n",
      "Iteration 82, loss = 0.36418339\n",
      "Iteration 83, loss = 0.36285536\n",
      "Iteration 84, loss = 0.36152815\n",
      "Iteration 85, loss = 0.36038736\n",
      "Iteration 86, loss = 0.35867187\n",
      "Iteration 87, loss = 0.35748662\n",
      "Iteration 88, loss = 0.35616986\n",
      "Iteration 89, loss = 0.35503684\n",
      "Iteration 90, loss = 0.35349628\n",
      "Iteration 91, loss = 0.35238394\n",
      "Iteration 92, loss = 0.35106516\n",
      "Iteration 93, loss = 0.34977876\n",
      "Iteration 94, loss = 0.34859251\n",
      "Iteration 95, loss = 0.34743390\n",
      "Iteration 96, loss = 0.34617443\n",
      "Iteration 97, loss = 0.34544367\n",
      "Iteration 98, loss = 0.34392948\n",
      "Iteration 99, loss = 0.34272909\n",
      "Iteration 100, loss = 0.34159675\n",
      "Iteration 101, loss = 0.34072639\n",
      "Iteration 102, loss = 0.33927043\n",
      "Iteration 103, loss = 0.33823462\n",
      "Iteration 104, loss = 0.33704319\n",
      "Iteration 105, loss = 0.33599467\n",
      "Iteration 106, loss = 0.33486551\n",
      "Iteration 107, loss = 0.33384327\n",
      "Iteration 108, loss = 0.33269632\n",
      "Iteration 109, loss = 0.33167321\n",
      "Iteration 110, loss = 0.33057947\n",
      "Iteration 111, loss = 0.32980680\n",
      "Iteration 112, loss = 0.32851046\n",
      "Iteration 113, loss = 0.32765123\n",
      "Iteration 114, loss = 0.32644157\n",
      "Iteration 115, loss = 0.32524971\n",
      "Iteration 116, loss = 0.32434114\n",
      "Iteration 117, loss = 0.32333840\n",
      "Iteration 118, loss = 0.32242784\n",
      "Iteration 119, loss = 0.32137302\n",
      "Iteration 120, loss = 0.32037364\n",
      "Iteration 121, loss = 0.31944420\n",
      "Iteration 122, loss = 0.31866744\n",
      "Iteration 123, loss = 0.31762093\n",
      "Iteration 124, loss = 0.31669257\n",
      "Iteration 125, loss = 0.31578126\n",
      "Iteration 126, loss = 0.31515962\n",
      "Iteration 127, loss = 0.31396450\n",
      "Iteration 128, loss = 0.31354815\n",
      "Iteration 129, loss = 0.31254933\n",
      "Iteration 130, loss = 0.31151830\n",
      "Iteration 131, loss = 0.31071471\n",
      "Iteration 132, loss = 0.30981283\n",
      "Iteration 133, loss = 0.30895663\n",
      "Iteration 134, loss = 0.30795788\n",
      "Iteration 135, loss = 0.30707435\n",
      "Iteration 136, loss = 0.30653709\n",
      "Iteration 137, loss = 0.30553769\n",
      "Iteration 138, loss = 0.30450198\n",
      "Iteration 139, loss = 0.30376828\n",
      "Iteration 140, loss = 0.30299950\n",
      "Iteration 141, loss = 0.30223075\n",
      "Iteration 142, loss = 0.30122577\n",
      "Iteration 143, loss = 0.30047117\n",
      "Iteration 144, loss = 0.29969933\n",
      "Iteration 145, loss = 0.29890501\n",
      "Iteration 146, loss = 0.29819274\n",
      "Iteration 147, loss = 0.29728425\n",
      "Iteration 148, loss = 0.29654576\n",
      "Iteration 149, loss = 0.29589043\n",
      "Iteration 150, loss = 0.29502336\n",
      "Iteration 151, loss = 0.29419176\n",
      "Iteration 152, loss = 0.29340164\n",
      "Iteration 153, loss = 0.29290927\n",
      "Iteration 154, loss = 0.29190473\n",
      "Iteration 155, loss = 0.29133459\n",
      "Iteration 156, loss = 0.29034391\n",
      "Iteration 157, loss = 0.28984476\n",
      "Iteration 158, loss = 0.28908969\n",
      "Iteration 159, loss = 0.28833058\n",
      "Iteration 160, loss = 0.28762775\n",
      "Iteration 161, loss = 0.28697223\n",
      "Iteration 162, loss = 0.28654929\n",
      "Iteration 163, loss = 0.28579296\n",
      "Iteration 164, loss = 0.28484767\n",
      "Iteration 165, loss = 0.28438813\n",
      "Iteration 166, loss = 0.28350869\n",
      "Iteration 167, loss = 0.28286216\n",
      "Iteration 168, loss = 0.28272983\n",
      "Iteration 169, loss = 0.28162140\n",
      "Iteration 170, loss = 0.28093601\n",
      "Iteration 171, loss = 0.28022336\n",
      "Iteration 172, loss = 0.27979385\n",
      "Iteration 173, loss = 0.27991747\n",
      "Iteration 174, loss = 0.27827001\n",
      "Iteration 175, loss = 0.27773596\n",
      "Iteration 176, loss = 0.27702513\n",
      "Iteration 177, loss = 0.27639207\n",
      "Iteration 178, loss = 0.27572487\n",
      "Iteration 179, loss = 0.27535865\n",
      "Iteration 180, loss = 0.27475975\n",
      "Iteration 181, loss = 0.27409251\n",
      "Iteration 182, loss = 0.27341585\n",
      "Iteration 183, loss = 0.27293507\n",
      "Iteration 184, loss = 0.27254617\n",
      "Iteration 185, loss = 0.27160241\n",
      "Iteration 186, loss = 0.27132862\n",
      "Iteration 187, loss = 0.27096067\n",
      "Iteration 188, loss = 0.27028067\n",
      "Iteration 189, loss = 0.26956969\n",
      "Iteration 190, loss = 0.26896739\n",
      "Iteration 191, loss = 0.26864420\n",
      "Iteration 192, loss = 0.26787522\n",
      "Iteration 193, loss = 0.26790432\n",
      "Iteration 194, loss = 0.26675506\n",
      "Iteration 195, loss = 0.26691656\n",
      "Iteration 196, loss = 0.26585754\n",
      "Iteration 197, loss = 0.26553929\n",
      "Iteration 198, loss = 0.26496236\n",
      "Iteration 199, loss = 0.26473816\n",
      "Iteration 200, loss = 0.26399989\n",
      "Iteration 201, loss = 0.26357337\n",
      "Iteration 202, loss = 0.26301082\n",
      "Iteration 203, loss = 0.26243430\n",
      "Iteration 204, loss = 0.26205802\n",
      "Iteration 205, loss = 0.26149883\n",
      "Iteration 206, loss = 0.26111487\n",
      "Iteration 207, loss = 0.26072219\n",
      "Iteration 208, loss = 0.26007520\n",
      "Iteration 209, loss = 0.25978746\n",
      "Iteration 210, loss = 0.25966720\n",
      "Iteration 211, loss = 0.25901406\n",
      "Iteration 212, loss = 0.25841437\n",
      "Iteration 213, loss = 0.25796706\n",
      "Iteration 214, loss = 0.25762053\n",
      "Iteration 215, loss = 0.25707211\n",
      "Iteration 216, loss = 0.25649054\n",
      "Iteration 217, loss = 0.25670217\n",
      "Iteration 218, loss = 0.25578331\n",
      "Iteration 219, loss = 0.25554549\n",
      "Iteration 220, loss = 0.25477274\n",
      "Iteration 221, loss = 0.25475693\n",
      "Iteration 222, loss = 0.25399108\n",
      "Iteration 223, loss = 0.25369092\n",
      "Iteration 224, loss = 0.25329090\n",
      "Iteration 225, loss = 0.25254263\n",
      "Iteration 226, loss = 0.25207961\n",
      "Iteration 227, loss = 0.25168610\n",
      "Iteration 228, loss = 0.25129187\n",
      "Iteration 229, loss = 0.25087925\n",
      "Iteration 230, loss = 0.25059859\n",
      "Iteration 231, loss = 0.25049028\n",
      "Iteration 232, loss = 0.24972792\n",
      "Iteration 233, loss = 0.24942248\n",
      "Iteration 234, loss = 0.24942848\n",
      "Iteration 235, loss = 0.24857332\n",
      "Iteration 236, loss = 0.24856391\n",
      "Iteration 237, loss = 0.24823059\n",
      "Iteration 238, loss = 0.24815762\n",
      "Iteration 239, loss = 0.24718706\n",
      "Iteration 240, loss = 0.24701496\n",
      "Iteration 241, loss = 0.24626928\n",
      "Iteration 242, loss = 0.24625172\n",
      "Iteration 243, loss = 0.24584242\n",
      "Iteration 244, loss = 0.24539143\n",
      "Iteration 245, loss = 0.24507001\n",
      "Iteration 246, loss = 0.24466324\n",
      "Iteration 247, loss = 0.24427984\n",
      "Iteration 248, loss = 0.24390225\n",
      "Iteration 249, loss = 0.24369080\n",
      "Iteration 250, loss = 0.24331367\n",
      "Iteration 251, loss = 0.24281689\n",
      "Iteration 252, loss = 0.24257383\n",
      "Iteration 253, loss = 0.24248764\n",
      "Iteration 254, loss = 0.24197591\n",
      "Iteration 255, loss = 0.24182587\n",
      "Iteration 256, loss = 0.24156341\n",
      "Iteration 257, loss = 0.24098362\n",
      "Iteration 258, loss = 0.24094018\n",
      "Iteration 259, loss = 0.24078158\n",
      "Iteration 260, loss = 0.24015938\n",
      "Iteration 261, loss = 0.24005766\n",
      "Iteration 262, loss = 0.23953811\n",
      "Iteration 263, loss = 0.23940654\n",
      "Iteration 264, loss = 0.23872051\n",
      "Iteration 265, loss = 0.23875388\n",
      "Iteration 266, loss = 0.23872784\n",
      "Iteration 267, loss = 0.23788135\n",
      "Iteration 268, loss = 0.23800663\n",
      "Iteration 269, loss = 0.23747902\n",
      "Iteration 270, loss = 0.23717364\n",
      "Iteration 271, loss = 0.23697618\n",
      "Iteration 272, loss = 0.23682643\n",
      "Iteration 273, loss = 0.23646145\n",
      "Iteration 274, loss = 0.23608817\n",
      "Iteration 275, loss = 0.23580120\n",
      "Iteration 276, loss = 0.23543094\n",
      "Iteration 277, loss = 0.23559964\n",
      "Iteration 278, loss = 0.23564545\n",
      "Iteration 279, loss = 0.23444343\n",
      "Iteration 280, loss = 0.23474628\n",
      "Iteration 281, loss = 0.23434062\n",
      "Iteration 282, loss = 0.23398284\n",
      "Iteration 283, loss = 0.23384510\n",
      "Iteration 284, loss = 0.23316215\n",
      "Iteration 285, loss = 0.23311773\n",
      "Iteration 286, loss = 0.23330840\n",
      "Iteration 287, loss = 0.23277281\n",
      "Iteration 288, loss = 0.23266875\n",
      "Iteration 289, loss = 0.23301363\n",
      "Iteration 290, loss = 0.23125258\n",
      "Iteration 291, loss = 0.23191268\n",
      "Iteration 292, loss = 0.23122606\n",
      "Iteration 293, loss = 0.23078758\n",
      "Iteration 294, loss = 0.23055087\n",
      "Iteration 295, loss = 0.23096234\n",
      "Iteration 296, loss = 0.23022216\n",
      "Iteration 297, loss = 0.22991658\n",
      "Iteration 298, loss = 0.23011854\n",
      "Iteration 299, loss = 0.22933331\n",
      "Iteration 300, loss = 0.22935140\n",
      "Iteration 301, loss = 0.22953130\n",
      "Iteration 302, loss = 0.22866271\n",
      "Iteration 303, loss = 0.22909048\n",
      "Iteration 304, loss = 0.22834459\n",
      "Iteration 305, loss = 0.22827059\n",
      "Iteration 306, loss = 0.22774429\n",
      "Iteration 307, loss = 0.22752977\n",
      "Iteration 308, loss = 0.22741056\n",
      "Iteration 309, loss = 0.22741222\n",
      "Iteration 310, loss = 0.22690932\n",
      "Iteration 311, loss = 0.22717744\n",
      "Iteration 312, loss = 0.22665214\n",
      "Iteration 313, loss = 0.22646253\n",
      "Iteration 314, loss = 0.22627096\n",
      "Iteration 315, loss = 0.22584946\n",
      "Iteration 316, loss = 0.22569232\n",
      "Iteration 317, loss = 0.22541275\n",
      "Iteration 318, loss = 0.22540976\n",
      "Iteration 319, loss = 0.22512950\n",
      "Iteration 320, loss = 0.22477218\n",
      "Iteration 321, loss = 0.22462078\n",
      "Iteration 322, loss = 0.22446082\n",
      "Iteration 323, loss = 0.22450923\n",
      "Iteration 324, loss = 0.22414290\n",
      "Iteration 325, loss = 0.22414418\n",
      "Iteration 326, loss = 0.22380555\n",
      "Iteration 327, loss = 0.22347304\n",
      "Iteration 328, loss = 0.22341850\n",
      "Iteration 329, loss = 0.22318919\n",
      "Iteration 330, loss = 0.22317737\n",
      "Iteration 331, loss = 0.22272870\n",
      "Iteration 332, loss = 0.22276090\n",
      "Iteration 333, loss = 0.22258995\n",
      "Iteration 334, loss = 0.22239954\n",
      "Iteration 335, loss = 0.22184731\n",
      "Iteration 336, loss = 0.22192038\n",
      "Iteration 337, loss = 0.22226692\n",
      "Iteration 338, loss = 0.22163576\n",
      "Iteration 339, loss = 0.22159541\n",
      "Iteration 340, loss = 0.22131692\n",
      "Iteration 341, loss = 0.22082879\n",
      "Iteration 342, loss = 0.22083580\n",
      "Iteration 343, loss = 0.22069224\n",
      "Iteration 344, loss = 0.22054597\n",
      "Iteration 345, loss = 0.22067928\n",
      "Iteration 346, loss = 0.22008609\n",
      "Iteration 347, loss = 0.22019048\n",
      "Iteration 348, loss = 0.22008072\n",
      "Iteration 349, loss = 0.21947813\n",
      "Iteration 350, loss = 0.21935001\n",
      "Iteration 351, loss = 0.21921915\n",
      "Iteration 352, loss = 0.21921106\n",
      "Iteration 353, loss = 0.21887160\n",
      "Iteration 354, loss = 0.21919224\n",
      "Iteration 355, loss = 0.21848858\n",
      "Iteration 356, loss = 0.21860160\n",
      "Iteration 357, loss = 0.21830524\n",
      "Iteration 358, loss = 0.21828315\n",
      "Iteration 359, loss = 0.21841632\n",
      "Iteration 360, loss = 0.21815797\n",
      "Iteration 361, loss = 0.21778182\n",
      "Iteration 362, loss = 0.21798283\n",
      "Iteration 363, loss = 0.21765920\n",
      "Iteration 364, loss = 0.21736629\n",
      "Iteration 365, loss = 0.21724420\n",
      "Iteration 366, loss = 0.21731515\n",
      "Iteration 367, loss = 0.21692234\n",
      "Iteration 368, loss = 0.21712094\n",
      "Iteration 369, loss = 0.21737961\n",
      "Iteration 370, loss = 0.21670995\n",
      "Iteration 371, loss = 0.21662363\n",
      "Iteration 372, loss = 0.21624835\n",
      "Iteration 373, loss = 0.21631368\n",
      "Iteration 374, loss = 0.21602722\n",
      "Iteration 375, loss = 0.21601569\n",
      "Iteration 376, loss = 0.21608914\n",
      "Iteration 377, loss = 0.21607525\n",
      "Iteration 378, loss = 0.21556735\n",
      "Iteration 379, loss = 0.21547534\n",
      "Iteration 380, loss = 0.21582351\n",
      "Iteration 381, loss = 0.21540163\n",
      "Iteration 382, loss = 0.21515765\n",
      "Iteration 383, loss = 0.21493023\n",
      "Iteration 384, loss = 0.21496136\n",
      "Iteration 385, loss = 0.21477396\n",
      "Iteration 386, loss = 0.21452112\n",
      "Iteration 387, loss = 0.21443969\n",
      "Iteration 388, loss = 0.21435266\n",
      "Iteration 389, loss = 0.21438552\n",
      "Iteration 390, loss = 0.21426412\n",
      "Iteration 391, loss = 0.21441676\n",
      "Iteration 392, loss = 0.21425826\n",
      "Iteration 393, loss = 0.21423542\n",
      "Iteration 394, loss = 0.21471014\n",
      "Iteration 395, loss = 0.21347911\n",
      "Iteration 396, loss = 0.21364534\n",
      "Iteration 397, loss = 0.21323277\n",
      "Iteration 398, loss = 0.21334242\n",
      "Iteration 399, loss = 0.21343974\n",
      "Iteration 400, loss = 0.21338468\n",
      "Iteration 401, loss = 0.21316197\n",
      "Iteration 402, loss = 0.21283790\n",
      "Iteration 403, loss = 0.21290698\n",
      "Iteration 404, loss = 0.21329072\n",
      "Iteration 405, loss = 0.21280133\n",
      "Iteration 406, loss = 0.21264129\n",
      "Iteration 407, loss = 0.21242777\n",
      "Iteration 408, loss = 0.21223979\n",
      "Iteration 409, loss = 0.21198301\n",
      "Iteration 410, loss = 0.21186126\n",
      "Iteration 411, loss = 0.21208604\n",
      "Iteration 412, loss = 0.21202036\n",
      "Iteration 413, loss = 0.21181641\n",
      "Iteration 414, loss = 0.21170630\n",
      "Iteration 415, loss = 0.21158846\n",
      "Iteration 416, loss = 0.21162410\n",
      "Iteration 417, loss = 0.21152020\n",
      "Iteration 418, loss = 0.21132820\n",
      "Iteration 419, loss = 0.21137107\n",
      "Iteration 420, loss = 0.21104557\n",
      "Iteration 421, loss = 0.21106982\n",
      "Iteration 422, loss = 0.21106143\n",
      "Iteration 423, loss = 0.21078677\n",
      "Iteration 424, loss = 0.21089822\n",
      "Iteration 425, loss = 0.21084609\n",
      "Iteration 426, loss = 0.21078508\n",
      "Iteration 427, loss = 0.21109150\n",
      "Iteration 428, loss = 0.21073704\n",
      "Iteration 429, loss = 0.21036525\n",
      "Iteration 430, loss = 0.21026688\n",
      "Iteration 431, loss = 0.21031130\n",
      "Iteration 432, loss = 0.21113698\n",
      "Iteration 433, loss = 0.21027824\n",
      "Iteration 434, loss = 0.21000691\n",
      "Iteration 435, loss = 0.21042754\n",
      "Iteration 436, loss = 0.20933399\n",
      "Iteration 437, loss = 0.20973693\n",
      "Iteration 438, loss = 0.20977035\n",
      "Iteration 439, loss = 0.21037392\n",
      "Iteration 440, loss = 0.20985223\n",
      "Iteration 441, loss = 0.20945760\n",
      "Iteration 442, loss = 0.20917563\n",
      "Iteration 443, loss = 0.20945876\n",
      "Iteration 444, loss = 0.20933047\n",
      "Iteration 445, loss = 0.20886412\n",
      "Iteration 446, loss = 0.20909430\n",
      "Iteration 447, loss = 0.20873956\n",
      "Iteration 448, loss = 0.20877460\n",
      "Iteration 449, loss = 0.20882941\n",
      "Iteration 450, loss = 0.20882258\n",
      "Iteration 451, loss = 0.20945408\n",
      "Iteration 452, loss = 0.20893991\n",
      "Iteration 453, loss = 0.20856021\n",
      "Iteration 454, loss = 0.20845459\n",
      "Iteration 455, loss = 0.20834910\n",
      "Iteration 456, loss = 0.20830613\n",
      "Iteration 457, loss = 0.20827363\n",
      "Iteration 458, loss = 0.20854703\n",
      "Iteration 459, loss = 0.20796314\n",
      "Iteration 460, loss = 0.20801225\n",
      "Iteration 461, loss = 0.20811569\n",
      "Iteration 462, loss = 0.20795910\n",
      "Iteration 463, loss = 0.20777748\n",
      "Iteration 464, loss = 0.20785261\n",
      "Iteration 465, loss = 0.20777743\n",
      "Iteration 466, loss = 0.20797529\n",
      "Iteration 467, loss = 0.20731962\n",
      "Iteration 468, loss = 0.20811285\n",
      "Iteration 469, loss = 0.20833744\n",
      "Iteration 470, loss = 0.20725037\n",
      "Iteration 471, loss = 0.20811707\n",
      "Iteration 472, loss = 0.20772023\n",
      "Iteration 473, loss = 0.20789697\n",
      "Iteration 474, loss = 0.20727935\n",
      "Iteration 475, loss = 0.20787733\n",
      "Iteration 476, loss = 0.20747353\n",
      "Iteration 477, loss = 0.20736633\n",
      "Iteration 478, loss = 0.20706478\n",
      "Iteration 479, loss = 0.20702626\n",
      "Iteration 480, loss = 0.20697092\n",
      "Iteration 481, loss = 0.20783610\n",
      "Iteration 482, loss = 0.20650880\n",
      "Iteration 483, loss = 0.20686887\n",
      "Iteration 484, loss = 0.20675902\n",
      "Iteration 485, loss = 0.20651349\n",
      "Iteration 486, loss = 0.20651448\n",
      "Iteration 487, loss = 0.20649084\n",
      "Iteration 488, loss = 0.20633426\n",
      "Iteration 489, loss = 0.20617157\n",
      "Iteration 490, loss = 0.20696471\n",
      "Iteration 491, loss = 0.20666719\n",
      "Iteration 492, loss = 0.20607651\n",
      "Iteration 493, loss = 0.20613153\n",
      "Iteration 494, loss = 0.20633113\n",
      "Iteration 495, loss = 0.20577354\n",
      "Iteration 496, loss = 0.20631714\n",
      "Iteration 497, loss = 0.20631824\n",
      "Iteration 498, loss = 0.20641906\n",
      "Iteration 499, loss = 0.20573214\n",
      "Iteration 500, loss = 0.20599600\n",
      "Iteration 501, loss = 0.20594753\n",
      "Iteration 502, loss = 0.20603392\n",
      "Iteration 503, loss = 0.20560821\n",
      "Iteration 504, loss = 0.20544305\n",
      "Iteration 505, loss = 0.20536724\n",
      "Iteration 506, loss = 0.20543693\n",
      "Iteration 507, loss = 0.20538906\n",
      "Iteration 508, loss = 0.20555878\n",
      "Iteration 509, loss = 0.20523791\n",
      "Iteration 510, loss = 0.20537774\n",
      "Iteration 511, loss = 0.20539623\n",
      "Iteration 512, loss = 0.20533238\n",
      "Iteration 513, loss = 0.20530070\n",
      "Iteration 514, loss = 0.20555195\n",
      "Iteration 515, loss = 0.20549799\n",
      "Iteration 516, loss = 0.20521195\n",
      "Iteration 517, loss = 0.20511618\n",
      "Iteration 518, loss = 0.20498565\n",
      "Iteration 519, loss = 0.20469767\n",
      "Iteration 520, loss = 0.20499220\n",
      "Iteration 521, loss = 0.20491979\n",
      "Iteration 522, loss = 0.20475881\n",
      "Iteration 523, loss = 0.20476751\n",
      "Iteration 524, loss = 0.20456714\n",
      "Iteration 525, loss = 0.20463382\n",
      "Iteration 526, loss = 0.20498121\n",
      "Iteration 527, loss = 0.20419577\n",
      "Iteration 528, loss = 0.20457288\n",
      "Iteration 529, loss = 0.20478277\n",
      "Iteration 530, loss = 0.20428637\n",
      "Iteration 531, loss = 0.20448361\n",
      "Iteration 532, loss = 0.20451752\n",
      "Iteration 533, loss = 0.20446430\n",
      "Iteration 534, loss = 0.20453622\n",
      "Iteration 535, loss = 0.20392200\n",
      "Iteration 536, loss = 0.20415494\n",
      "Iteration 537, loss = 0.20435223\n",
      "Iteration 538, loss = 0.20417555\n",
      "Iteration 539, loss = 0.20404295\n",
      "Iteration 540, loss = 0.20423770\n",
      "Iteration 541, loss = 0.20440602\n",
      "Iteration 542, loss = 0.20397090\n",
      "Iteration 543, loss = 0.20393502\n",
      "Iteration 544, loss = 0.20442295\n",
      "Iteration 545, loss = 0.20383691\n",
      "Iteration 546, loss = 0.20397896\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "redes.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = redes.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 91.54%\n"
     ]
    }
   ],
   "source": [
    "print(\"Acurácia: %.2f%%\" % (accuracy_score(y_test, previsoes) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[317  21]\n",
      " [ 30 235]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, previsoes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93       338\n",
      "           1       0.92      0.89      0.90       265\n",
      "\n",
      "    accuracy                           0.92       603\n",
      "   macro avg       0.92      0.91      0.91       603\n",
      "weighted avg       0.92      0.92      0.92       603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes_train = redes.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia em treino: 93.67%\n"
     ]
    }
   ],
   "source": [
    "print(\"Acurácia em treino: %.2f%%\" % (accuracy_score(y_train, previsoes_train) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[723  52]\n",
      " [ 37 593]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, previsoes_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       775\n",
      "           1       0.92      0.94      0.93       630\n",
      "\n",
      "    accuracy                           0.94      1405\n",
      "   macro avg       0.94      0.94      0.94      1405\n",
      "weighted avg       0.94      0.94      0.94      1405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, previsoes_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "<span style=\"font-size: 14px; font-family: 'Trebuchet MS', sans-serif;\">\n",
    "\n",
    "- Inicialmente, o conjunto de dados é dividido em k partes (ou folds) de aproximadamente o mesmo tamanho.\n",
    "- O modelo é treinado k vezes. Em cada iteração, um dos k folds é retido como conjunto de teste e os outros k-1 folds são usados como conjunto de treinamento.\n",
    "- Após completar as k iterações, a métrica de avaliação é calculada como a média dos resultados obtidos em cada fold de teste.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(\n",
    "    n_splits = 40, shuffle = True, random_state = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.76910293\n",
      "Iteration 2, loss = 0.74298731\n",
      "Iteration 3, loss = 0.72203986\n",
      "Iteration 4, loss = 0.70500389\n",
      "Iteration 5, loss = 0.69087150\n",
      "Iteration 6, loss = 0.67748474\n",
      "Iteration 7, loss = 0.66403591\n",
      "Iteration 8, loss = 0.64853590\n",
      "Iteration 9, loss = 0.63068369\n",
      "Iteration 10, loss = 0.61144998\n",
      "Iteration 11, loss = 0.59205833\n",
      "Iteration 12, loss = 0.57411033\n",
      "Iteration 13, loss = 0.55805269\n",
      "Iteration 14, loss = 0.54391462\n",
      "Iteration 15, loss = 0.53195939\n",
      "Iteration 16, loss = 0.52155955\n",
      "Iteration 17, loss = 0.51183093\n",
      "Iteration 18, loss = 0.50353945\n",
      "Iteration 19, loss = 0.49580537\n",
      "Iteration 20, loss = 0.48891001\n",
      "Iteration 21, loss = 0.48255948\n",
      "Iteration 22, loss = 0.47686499\n",
      "Iteration 23, loss = 0.47129571\n",
      "Iteration 24, loss = 0.46609150\n",
      "Iteration 25, loss = 0.46119777\n",
      "Iteration 26, loss = 0.45667138\n",
      "Iteration 27, loss = 0.45249465\n",
      "Iteration 28, loss = 0.44831307\n",
      "Iteration 29, loss = 0.44430196\n",
      "Iteration 30, loss = 0.44058163\n",
      "Iteration 31, loss = 0.43658009\n",
      "Iteration 32, loss = 0.43310349\n",
      "Iteration 33, loss = 0.42963371\n",
      "Iteration 34, loss = 0.42613471\n",
      "Iteration 35, loss = 0.42255613\n",
      "Iteration 36, loss = 0.41930138\n",
      "Iteration 37, loss = 0.41607595\n",
      "Iteration 38, loss = 0.41298543\n",
      "Iteration 39, loss = 0.40990321\n",
      "Iteration 40, loss = 0.40659213\n",
      "Iteration 41, loss = 0.40384085\n",
      "Iteration 42, loss = 0.40098790\n",
      "Iteration 43, loss = 0.39784018\n",
      "Iteration 44, loss = 0.39536805\n",
      "Iteration 45, loss = 0.39263104\n",
      "Iteration 46, loss = 0.38994633\n",
      "Iteration 47, loss = 0.38720893\n",
      "Iteration 48, loss = 0.38466724\n",
      "Iteration 49, loss = 0.38226786\n",
      "Iteration 50, loss = 0.37973876\n",
      "Iteration 51, loss = 0.37737625\n",
      "Iteration 52, loss = 0.37506130\n",
      "Iteration 53, loss = 0.37270352\n",
      "Iteration 54, loss = 0.37040872\n",
      "Iteration 55, loss = 0.36819382\n",
      "Iteration 56, loss = 0.36617781\n",
      "Iteration 57, loss = 0.36390325\n",
      "Iteration 58, loss = 0.36206219\n",
      "Iteration 59, loss = 0.35985344\n",
      "Iteration 60, loss = 0.35729679\n",
      "Iteration 61, loss = 0.35572363\n",
      "Iteration 62, loss = 0.35358589\n",
      "Iteration 63, loss = 0.35163217\n",
      "Iteration 64, loss = 0.35020322\n",
      "Iteration 65, loss = 0.34781396\n",
      "Iteration 66, loss = 0.34633117\n",
      "Iteration 67, loss = 0.34418486\n",
      "Iteration 68, loss = 0.34231001\n",
      "Iteration 69, loss = 0.34052407\n",
      "Iteration 70, loss = 0.33880023\n",
      "Iteration 71, loss = 0.33706050\n",
      "Iteration 72, loss = 0.33546709\n",
      "Iteration 73, loss = 0.33389467\n",
      "Iteration 74, loss = 0.33222574\n",
      "Iteration 75, loss = 0.33062523\n",
      "Iteration 76, loss = 0.32933999\n",
      "Iteration 77, loss = 0.32756740\n",
      "Iteration 78, loss = 0.32615306\n",
      "Iteration 79, loss = 0.32486720\n",
      "Iteration 80, loss = 0.32298668\n",
      "Iteration 81, loss = 0.32170748\n",
      "Iteration 82, loss = 0.32012003\n",
      "Iteration 83, loss = 0.31866985\n",
      "Iteration 84, loss = 0.31716840\n",
      "Iteration 85, loss = 0.31579403\n",
      "Iteration 86, loss = 0.31400905\n",
      "Iteration 87, loss = 0.31311560\n",
      "Iteration 88, loss = 0.31132180\n",
      "Iteration 89, loss = 0.30973706\n",
      "Iteration 90, loss = 0.30812653\n",
      "Iteration 91, loss = 0.30683136\n",
      "Iteration 92, loss = 0.30527986\n",
      "Iteration 93, loss = 0.30398610\n",
      "Iteration 94, loss = 0.30266850\n",
      "Iteration 95, loss = 0.30132436\n",
      "Iteration 96, loss = 0.30042993\n",
      "Iteration 97, loss = 0.29923836\n",
      "Iteration 98, loss = 0.29782632\n",
      "Iteration 99, loss = 0.29720771\n",
      "Iteration 100, loss = 0.29530780\n",
      "Iteration 101, loss = 0.29453101\n",
      "Iteration 102, loss = 0.29364082\n",
      "Iteration 103, loss = 0.29211716\n",
      "Iteration 104, loss = 0.29113609\n",
      "Iteration 105, loss = 0.28996651\n",
      "Iteration 106, loss = 0.28943765\n",
      "Iteration 107, loss = 0.28828614\n",
      "Iteration 108, loss = 0.28689615\n",
      "Iteration 109, loss = 0.28612666\n",
      "Iteration 110, loss = 0.28471716\n",
      "Iteration 111, loss = 0.28393909\n",
      "Iteration 112, loss = 0.28273902\n",
      "Iteration 113, loss = 0.28199318\n",
      "Iteration 114, loss = 0.28058401\n",
      "Iteration 115, loss = 0.27960259\n",
      "Iteration 116, loss = 0.27886800\n",
      "Iteration 117, loss = 0.27774645\n",
      "Iteration 118, loss = 0.27693043\n",
      "Iteration 119, loss = 0.27601005\n",
      "Iteration 120, loss = 0.27524740\n",
      "Iteration 121, loss = 0.27414825\n",
      "Iteration 122, loss = 0.27336502\n",
      "Iteration 123, loss = 0.27251576\n",
      "Iteration 124, loss = 0.27156319\n",
      "Iteration 125, loss = 0.27084647\n",
      "Iteration 126, loss = 0.26991155\n",
      "Iteration 127, loss = 0.26917160\n",
      "Iteration 128, loss = 0.26864779\n",
      "Iteration 129, loss = 0.26768082\n",
      "Iteration 130, loss = 0.26701864\n",
      "Iteration 131, loss = 0.26601660\n",
      "Iteration 132, loss = 0.26548915\n",
      "Iteration 133, loss = 0.26489408\n",
      "Iteration 134, loss = 0.26396678\n",
      "Iteration 135, loss = 0.26314388\n",
      "Iteration 136, loss = 0.26240391\n",
      "Iteration 137, loss = 0.26301865\n",
      "Iteration 138, loss = 0.26109494\n",
      "Iteration 139, loss = 0.26119106\n",
      "Iteration 140, loss = 0.26070660\n",
      "Iteration 141, loss = 0.25916735\n",
      "Iteration 142, loss = 0.25866823\n",
      "Iteration 143, loss = 0.25795956\n",
      "Iteration 144, loss = 0.25767511\n",
      "Iteration 145, loss = 0.25680983\n",
      "Iteration 146, loss = 0.25615054\n",
      "Iteration 147, loss = 0.25548434\n",
      "Iteration 148, loss = 0.25543369\n",
      "Iteration 149, loss = 0.25447781\n",
      "Iteration 150, loss = 0.25410456\n",
      "Iteration 151, loss = 0.25362347\n",
      "Iteration 152, loss = 0.25296492\n",
      "Iteration 153, loss = 0.25229735\n",
      "Iteration 154, loss = 0.25174482\n",
      "Iteration 155, loss = 0.25115578\n",
      "Iteration 156, loss = 0.25085656\n",
      "Iteration 157, loss = 0.25011929\n",
      "Iteration 158, loss = 0.24958961\n",
      "Iteration 159, loss = 0.24904931\n",
      "Iteration 160, loss = 0.24941408\n",
      "Iteration 161, loss = 0.24770034\n",
      "Iteration 162, loss = 0.24746357\n",
      "Iteration 163, loss = 0.24662376\n",
      "Iteration 164, loss = 0.24649502\n",
      "Iteration 165, loss = 0.24560812\n",
      "Iteration 166, loss = 0.24515709\n",
      "Iteration 167, loss = 0.24458310\n",
      "Iteration 168, loss = 0.24413328\n",
      "Iteration 169, loss = 0.24340900\n",
      "Iteration 170, loss = 0.24317658\n",
      "Iteration 171, loss = 0.24275828\n",
      "Iteration 172, loss = 0.24239743\n",
      "Iteration 173, loss = 0.24164349\n",
      "Iteration 174, loss = 0.24116738\n",
      "Iteration 175, loss = 0.24097166\n",
      "Iteration 176, loss = 0.23995109\n",
      "Iteration 177, loss = 0.23987323\n",
      "Iteration 178, loss = 0.23946341\n",
      "Iteration 179, loss = 0.23930887\n",
      "Iteration 180, loss = 0.23863131\n",
      "Iteration 181, loss = 0.23817965\n",
      "Iteration 182, loss = 0.23751280\n",
      "Iteration 183, loss = 0.23709988\n",
      "Iteration 184, loss = 0.23640652\n",
      "Iteration 185, loss = 0.23622249\n",
      "Iteration 186, loss = 0.23608493\n",
      "Iteration 187, loss = 0.23576133\n",
      "Iteration 188, loss = 0.23495958\n",
      "Iteration 189, loss = 0.23463198\n",
      "Iteration 190, loss = 0.23488075\n",
      "Iteration 191, loss = 0.23380790\n",
      "Iteration 192, loss = 0.23357399\n",
      "Iteration 193, loss = 0.23369468\n",
      "Iteration 194, loss = 0.23261856\n",
      "Iteration 195, loss = 0.23251671\n",
      "Iteration 196, loss = 0.23189616\n",
      "Iteration 197, loss = 0.23152305\n",
      "Iteration 198, loss = 0.23115577\n",
      "Iteration 199, loss = 0.23101856\n",
      "Iteration 200, loss = 0.23069580\n",
      "Iteration 201, loss = 0.23039715\n",
      "Iteration 202, loss = 0.22988911\n",
      "Iteration 203, loss = 0.22940841\n",
      "Iteration 204, loss = 0.22900996\n",
      "Iteration 205, loss = 0.22861755\n",
      "Iteration 206, loss = 0.22864568\n",
      "Iteration 207, loss = 0.22841106\n",
      "Iteration 208, loss = 0.22792252\n",
      "Iteration 209, loss = 0.22736778\n",
      "Iteration 210, loss = 0.22709581\n",
      "Iteration 211, loss = 0.22708472\n",
      "Iteration 212, loss = 0.22660497\n",
      "Iteration 213, loss = 0.22635080\n",
      "Iteration 214, loss = 0.22642392\n",
      "Iteration 215, loss = 0.22583526\n",
      "Iteration 216, loss = 0.22539593\n",
      "Iteration 217, loss = 0.22518150\n",
      "Iteration 218, loss = 0.22512768\n",
      "Iteration 219, loss = 0.22510520\n",
      "Iteration 220, loss = 0.22454186\n",
      "Iteration 221, loss = 0.22462671\n",
      "Iteration 222, loss = 0.22443348\n",
      "Iteration 223, loss = 0.22355009\n",
      "Iteration 224, loss = 0.22363790\n",
      "Iteration 225, loss = 0.22375352\n",
      "Iteration 226, loss = 0.22321671\n",
      "Iteration 227, loss = 0.22316527\n",
      "Iteration 228, loss = 0.22286664\n",
      "Iteration 229, loss = 0.22228762\n",
      "Iteration 230, loss = 0.22235147\n",
      "Iteration 231, loss = 0.22220681\n",
      "Iteration 232, loss = 0.22189196\n",
      "Iteration 233, loss = 0.22150548\n",
      "Iteration 234, loss = 0.22180654\n",
      "Iteration 235, loss = 0.22162912\n",
      "Iteration 236, loss = 0.22087835\n",
      "Iteration 237, loss = 0.22082996\n",
      "Iteration 238, loss = 0.22062086\n",
      "Iteration 239, loss = 0.22035078\n",
      "Iteration 240, loss = 0.22069047\n",
      "Iteration 241, loss = 0.22012803\n",
      "Iteration 242, loss = 0.21989739\n",
      "Iteration 243, loss = 0.21955881\n",
      "Iteration 244, loss = 0.21947959\n",
      "Iteration 245, loss = 0.21895875\n",
      "Iteration 246, loss = 0.21921119\n",
      "Iteration 247, loss = 0.21923916\n",
      "Iteration 248, loss = 0.21845837\n",
      "Iteration 249, loss = 0.21822368\n",
      "Iteration 250, loss = 0.21850643\n",
      "Iteration 251, loss = 0.21791736\n",
      "Iteration 252, loss = 0.21766711\n",
      "Iteration 253, loss = 0.21739861\n",
      "Iteration 254, loss = 0.21745663\n",
      "Iteration 255, loss = 0.21750886\n",
      "Iteration 256, loss = 0.21677899\n",
      "Iteration 257, loss = 0.21660056\n",
      "Iteration 258, loss = 0.21699003\n",
      "Iteration 259, loss = 0.21634443\n",
      "Iteration 260, loss = 0.21618297\n",
      "Iteration 261, loss = 0.21577488\n",
      "Iteration 262, loss = 0.21582218\n",
      "Iteration 263, loss = 0.21563990\n",
      "Iteration 264, loss = 0.21483029\n",
      "Iteration 265, loss = 0.21488887\n",
      "Iteration 266, loss = 0.21464644\n",
      "Iteration 267, loss = 0.21436476\n",
      "Iteration 268, loss = 0.21419000\n",
      "Iteration 269, loss = 0.21416473\n",
      "Iteration 270, loss = 0.21318251\n",
      "Iteration 271, loss = 0.21339145\n",
      "Iteration 272, loss = 0.21250424\n",
      "Iteration 273, loss = 0.21287009\n",
      "Iteration 274, loss = 0.21239389\n",
      "Iteration 275, loss = 0.21190792\n",
      "Iteration 276, loss = 0.21188189\n",
      "Iteration 277, loss = 0.21165863\n",
      "Iteration 278, loss = 0.21100454\n",
      "Iteration 279, loss = 0.21079620\n",
      "Iteration 280, loss = 0.21034167\n",
      "Iteration 281, loss = 0.21050561\n",
      "Iteration 282, loss = 0.20994031\n",
      "Iteration 283, loss = 0.21031496\n",
      "Iteration 284, loss = 0.20985320\n",
      "Iteration 285, loss = 0.20938412\n",
      "Iteration 286, loss = 0.20913694\n",
      "Iteration 287, loss = 0.20842227\n",
      "Iteration 288, loss = 0.20813838\n",
      "Iteration 289, loss = 0.20821105\n",
      "Iteration 290, loss = 0.20744730\n",
      "Iteration 291, loss = 0.20744636\n",
      "Iteration 292, loss = 0.20724263\n",
      "Iteration 293, loss = 0.20703575\n",
      "Iteration 294, loss = 0.20667458\n",
      "Iteration 295, loss = 0.20636731\n",
      "Iteration 296, loss = 0.20579081\n",
      "Iteration 297, loss = 0.20546833\n",
      "Iteration 298, loss = 0.20505928\n",
      "Iteration 299, loss = 0.20622570\n",
      "Iteration 300, loss = 0.20553113\n",
      "Iteration 301, loss = 0.20487571\n",
      "Iteration 302, loss = 0.20379441\n",
      "Iteration 303, loss = 0.20351142\n",
      "Iteration 304, loss = 0.20323394\n",
      "Iteration 305, loss = 0.20283070\n",
      "Iteration 306, loss = 0.20294021\n",
      "Iteration 307, loss = 0.20247390\n",
      "Iteration 308, loss = 0.20273615\n",
      "Iteration 309, loss = 0.20171126\n",
      "Iteration 310, loss = 0.20185730\n",
      "Iteration 311, loss = 0.20111319\n",
      "Iteration 312, loss = 0.20071659\n",
      "Iteration 313, loss = 0.20042272\n",
      "Iteration 314, loss = 0.20030364\n",
      "Iteration 315, loss = 0.19939890\n",
      "Iteration 316, loss = 0.19940147\n",
      "Iteration 317, loss = 0.19889063\n",
      "Iteration 318, loss = 0.19899723\n",
      "Iteration 319, loss = 0.19944164\n",
      "Iteration 320, loss = 0.19803481\n",
      "Iteration 321, loss = 0.19763163\n",
      "Iteration 322, loss = 0.19725329\n",
      "Iteration 323, loss = 0.19769939\n",
      "Iteration 324, loss = 0.19723916\n",
      "Iteration 325, loss = 0.19688752\n",
      "Iteration 326, loss = 0.19660366\n",
      "Iteration 327, loss = 0.19638260\n",
      "Iteration 328, loss = 0.19623977\n",
      "Iteration 329, loss = 0.19585671\n",
      "Iteration 330, loss = 0.19586621\n",
      "Iteration 331, loss = 0.19629474\n",
      "Iteration 332, loss = 0.19567344\n",
      "Iteration 333, loss = 0.19517142\n",
      "Iteration 334, loss = 0.19450660\n",
      "Iteration 335, loss = 0.19453034\n",
      "Iteration 336, loss = 0.19444289\n",
      "Iteration 337, loss = 0.19394345\n",
      "Iteration 338, loss = 0.19363008\n",
      "Iteration 339, loss = 0.19361620\n",
      "Iteration 340, loss = 0.19329429\n",
      "Iteration 341, loss = 0.19318938\n",
      "Iteration 342, loss = 0.19287488\n",
      "Iteration 343, loss = 0.19265869\n",
      "Iteration 344, loss = 0.19224648\n",
      "Iteration 345, loss = 0.19239231\n",
      "Iteration 346, loss = 0.19227393\n",
      "Iteration 347, loss = 0.19179612\n",
      "Iteration 348, loss = 0.19152497\n",
      "Iteration 349, loss = 0.19162081\n",
      "Iteration 350, loss = 0.19135712\n",
      "Iteration 351, loss = 0.19098342\n",
      "Iteration 352, loss = 0.19104864\n",
      "Iteration 353, loss = 0.19051471\n",
      "Iteration 354, loss = 0.19064429\n",
      "Iteration 355, loss = 0.19006515\n",
      "Iteration 356, loss = 0.18997898\n",
      "Iteration 357, loss = 0.19115792\n",
      "Iteration 358, loss = 0.19025050\n",
      "Iteration 359, loss = 0.18974077\n",
      "Iteration 360, loss = 0.18898622\n",
      "Iteration 361, loss = 0.18932716\n",
      "Iteration 362, loss = 0.18885281\n",
      "Iteration 363, loss = 0.18877216\n",
      "Iteration 364, loss = 0.18844346\n",
      "Iteration 365, loss = 0.18824483\n",
      "Iteration 366, loss = 0.18797857\n",
      "Iteration 367, loss = 0.18781362\n",
      "Iteration 368, loss = 0.18761120\n",
      "Iteration 369, loss = 0.18764592\n",
      "Iteration 370, loss = 0.18794214\n",
      "Iteration 371, loss = 0.18684350\n",
      "Iteration 372, loss = 0.18726326\n",
      "Iteration 373, loss = 0.18678364\n",
      "Iteration 374, loss = 0.18710768\n",
      "Iteration 375, loss = 0.18690673\n",
      "Iteration 376, loss = 0.18655470\n",
      "Iteration 377, loss = 0.18612971\n",
      "Iteration 378, loss = 0.18595118\n",
      "Iteration 379, loss = 0.18573009\n",
      "Iteration 380, loss = 0.18645880\n",
      "Iteration 381, loss = 0.18593745\n",
      "Iteration 382, loss = 0.18600794\n",
      "Iteration 383, loss = 0.18546903\n",
      "Iteration 384, loss = 0.18523911\n",
      "Iteration 385, loss = 0.18529204\n",
      "Iteration 386, loss = 0.18498150\n",
      "Iteration 387, loss = 0.18539043\n",
      "Iteration 388, loss = 0.18486568\n",
      "Iteration 389, loss = 0.18440315\n",
      "Iteration 390, loss = 0.18428719\n",
      "Iteration 391, loss = 0.18469130\n",
      "Iteration 392, loss = 0.18413479\n",
      "Iteration 393, loss = 0.18388960\n",
      "Iteration 394, loss = 0.18384597\n",
      "Iteration 395, loss = 0.18362817\n",
      "Iteration 396, loss = 0.18343960\n",
      "Iteration 397, loss = 0.18347934\n",
      "Iteration 398, loss = 0.18348482\n",
      "Iteration 399, loss = 0.18335812\n",
      "Iteration 400, loss = 0.18303434\n",
      "Iteration 401, loss = 0.18285646\n",
      "Iteration 402, loss = 0.18288771\n",
      "Iteration 403, loss = 0.18327230\n",
      "Iteration 404, loss = 0.18264872\n",
      "Iteration 405, loss = 0.18276029\n",
      "Iteration 406, loss = 0.18240492\n",
      "Iteration 407, loss = 0.18261553\n",
      "Iteration 408, loss = 0.18225795\n",
      "Iteration 409, loss = 0.18245362\n",
      "Iteration 410, loss = 0.18213543\n",
      "Iteration 411, loss = 0.18244927\n",
      "Iteration 412, loss = 0.18208768\n",
      "Iteration 413, loss = 0.18176989\n",
      "Iteration 414, loss = 0.18170850\n",
      "Iteration 415, loss = 0.18172771\n",
      "Iteration 416, loss = 0.18146832\n",
      "Iteration 417, loss = 0.18153411\n",
      "Iteration 418, loss = 0.18116159\n",
      "Iteration 419, loss = 0.18139982\n",
      "Iteration 420, loss = 0.18166258\n",
      "Iteration 421, loss = 0.18076919\n",
      "Iteration 422, loss = 0.18097660\n",
      "Iteration 423, loss = 0.18105070\n",
      "Iteration 424, loss = 0.18151258\n",
      "Iteration 425, loss = 0.18024949\n",
      "Iteration 426, loss = 0.18049918\n",
      "Iteration 427, loss = 0.18022126\n",
      "Iteration 428, loss = 0.18030882\n",
      "Iteration 429, loss = 0.18066265\n",
      "Iteration 430, loss = 0.18043020\n",
      "Iteration 431, loss = 0.18010032\n",
      "Iteration 432, loss = 0.18035291\n",
      "Iteration 433, loss = 0.17989783\n",
      "Iteration 434, loss = 0.18004819\n",
      "Iteration 435, loss = 0.18003057\n",
      "Iteration 436, loss = 0.17990795\n",
      "Iteration 437, loss = 0.17968747\n",
      "Iteration 438, loss = 0.17937095\n",
      "Iteration 439, loss = 0.17952356\n",
      "Iteration 440, loss = 0.17956862\n",
      "Iteration 441, loss = 0.17963682\n",
      "Iteration 442, loss = 0.17953643\n",
      "Iteration 443, loss = 0.17890754\n",
      "Iteration 444, loss = 0.17920894\n",
      "Iteration 445, loss = 0.17904760\n",
      "Iteration 446, loss = 0.17910536\n",
      "Iteration 447, loss = 0.17942673\n",
      "Iteration 448, loss = 0.17897079\n",
      "Iteration 449, loss = 0.17904907\n",
      "Iteration 450, loss = 0.17857594\n",
      "Iteration 451, loss = 0.17867809\n",
      "Iteration 452, loss = 0.17877597\n",
      "Iteration 453, loss = 0.17822995\n",
      "Iteration 454, loss = 0.17836104\n",
      "Iteration 455, loss = 0.17877643\n",
      "Iteration 456, loss = 0.17821252\n",
      "Iteration 457, loss = 0.17826777\n",
      "Iteration 458, loss = 0.17825400\n",
      "Iteration 459, loss = 0.17800503\n",
      "Iteration 460, loss = 0.17802690\n",
      "Iteration 461, loss = 0.17839513\n",
      "Iteration 462, loss = 0.17839374\n",
      "Iteration 463, loss = 0.17860323\n",
      "Iteration 464, loss = 0.17844258\n",
      "Iteration 465, loss = 0.17807705\n",
      "Iteration 466, loss = 0.17752696\n",
      "Iteration 467, loss = 0.17783981\n",
      "Iteration 468, loss = 0.17845974\n",
      "Iteration 469, loss = 0.17858981\n",
      "Iteration 470, loss = 0.17738390\n",
      "Iteration 471, loss = 0.17760224\n",
      "Iteration 472, loss = 0.17722253\n",
      "Iteration 473, loss = 0.17838253\n",
      "Iteration 474, loss = 0.17765999\n",
      "Iteration 475, loss = 0.17702760\n",
      "Iteration 476, loss = 0.17705300\n",
      "Iteration 477, loss = 0.17710196\n",
      "Iteration 478, loss = 0.17700189\n",
      "Iteration 479, loss = 0.17757917\n",
      "Iteration 480, loss = 0.17760174\n",
      "Iteration 481, loss = 0.17748690\n",
      "Iteration 482, loss = 0.17739002\n",
      "Iteration 483, loss = 0.17726597\n",
      "Iteration 484, loss = 0.17698966\n",
      "Iteration 485, loss = 0.17701219\n",
      "Iteration 486, loss = 0.17748240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76935127\n",
      "Iteration 2, loss = 0.74393274\n",
      "Iteration 3, loss = 0.72180400\n",
      "Iteration 4, loss = 0.70487164\n",
      "Iteration 5, loss = 0.69049079\n",
      "Iteration 6, loss = 0.67673233\n",
      "Iteration 7, loss = 0.66278229\n",
      "Iteration 8, loss = 0.64659119\n",
      "Iteration 9, loss = 0.62806264\n",
      "Iteration 10, loss = 0.60878345\n",
      "Iteration 11, loss = 0.58923540\n",
      "Iteration 12, loss = 0.57137955\n",
      "Iteration 13, loss = 0.55551771\n",
      "Iteration 14, loss = 0.54176059\n",
      "Iteration 15, loss = 0.53049361\n",
      "Iteration 16, loss = 0.52066083\n",
      "Iteration 17, loss = 0.51155983\n",
      "Iteration 18, loss = 0.50331347\n",
      "Iteration 19, loss = 0.49585340\n",
      "Iteration 20, loss = 0.48886822\n",
      "Iteration 21, loss = 0.48250019\n",
      "Iteration 22, loss = 0.47671833\n",
      "Iteration 23, loss = 0.47101095\n",
      "Iteration 24, loss = 0.46568358\n",
      "Iteration 25, loss = 0.46103256\n",
      "Iteration 26, loss = 0.45651346\n",
      "Iteration 27, loss = 0.45213740\n",
      "Iteration 28, loss = 0.44852780\n",
      "Iteration 29, loss = 0.44395167\n",
      "Iteration 30, loss = 0.44003105\n",
      "Iteration 31, loss = 0.43602379\n",
      "Iteration 32, loss = 0.43257022\n",
      "Iteration 33, loss = 0.42861862\n",
      "Iteration 34, loss = 0.42503010\n",
      "Iteration 35, loss = 0.42173287\n",
      "Iteration 36, loss = 0.41838589\n",
      "Iteration 37, loss = 0.41540269\n",
      "Iteration 38, loss = 0.41199754\n",
      "Iteration 39, loss = 0.40906548\n",
      "Iteration 40, loss = 0.40597276\n",
      "Iteration 41, loss = 0.40283846\n",
      "Iteration 42, loss = 0.40001423\n",
      "Iteration 43, loss = 0.39714049\n",
      "Iteration 44, loss = 0.39423939\n",
      "Iteration 45, loss = 0.39178627\n",
      "Iteration 46, loss = 0.38909184\n",
      "Iteration 47, loss = 0.38658932\n",
      "Iteration 48, loss = 0.38403227\n",
      "Iteration 49, loss = 0.38164167\n",
      "Iteration 50, loss = 0.37917119\n",
      "Iteration 51, loss = 0.37692610\n",
      "Iteration 52, loss = 0.37445885\n",
      "Iteration 53, loss = 0.37226537\n",
      "Iteration 54, loss = 0.37016011\n",
      "Iteration 55, loss = 0.36787167\n",
      "Iteration 56, loss = 0.36581812\n",
      "Iteration 57, loss = 0.36378085\n",
      "Iteration 58, loss = 0.36165038\n",
      "Iteration 59, loss = 0.35953722\n",
      "Iteration 60, loss = 0.35733787\n",
      "Iteration 61, loss = 0.35562947\n",
      "Iteration 62, loss = 0.35345606\n",
      "Iteration 63, loss = 0.35178709\n",
      "Iteration 64, loss = 0.34966513\n",
      "Iteration 65, loss = 0.34789710\n",
      "Iteration 66, loss = 0.34642078\n",
      "Iteration 67, loss = 0.34386827\n",
      "Iteration 68, loss = 0.34205031\n",
      "Iteration 69, loss = 0.34036339\n",
      "Iteration 70, loss = 0.33835591\n",
      "Iteration 71, loss = 0.33650251\n",
      "Iteration 72, loss = 0.33482414\n",
      "Iteration 73, loss = 0.33314455\n",
      "Iteration 74, loss = 0.33147148\n",
      "Iteration 75, loss = 0.32977618\n",
      "Iteration 76, loss = 0.32811549\n",
      "Iteration 77, loss = 0.32645463\n",
      "Iteration 78, loss = 0.32499147\n",
      "Iteration 79, loss = 0.32369765\n",
      "Iteration 80, loss = 0.32212695\n",
      "Iteration 81, loss = 0.32058707\n",
      "Iteration 82, loss = 0.31911316\n",
      "Iteration 83, loss = 0.31774911\n",
      "Iteration 84, loss = 0.31665368\n",
      "Iteration 85, loss = 0.31478395\n",
      "Iteration 86, loss = 0.31371962\n",
      "Iteration 87, loss = 0.31215340\n",
      "Iteration 88, loss = 0.31104814\n",
      "Iteration 89, loss = 0.30941708\n",
      "Iteration 90, loss = 0.30809356\n",
      "Iteration 91, loss = 0.30636713\n",
      "Iteration 92, loss = 0.30535717\n",
      "Iteration 93, loss = 0.30402339\n",
      "Iteration 94, loss = 0.30287253\n",
      "Iteration 95, loss = 0.30153906\n",
      "Iteration 96, loss = 0.30004212\n",
      "Iteration 97, loss = 0.29865688\n",
      "Iteration 98, loss = 0.29785560\n",
      "Iteration 99, loss = 0.29635304\n",
      "Iteration 100, loss = 0.29501769\n",
      "Iteration 101, loss = 0.29431647\n",
      "Iteration 102, loss = 0.29306703\n",
      "Iteration 103, loss = 0.29153095\n",
      "Iteration 104, loss = 0.29035885\n",
      "Iteration 105, loss = 0.28933496\n",
      "Iteration 106, loss = 0.28781617\n",
      "Iteration 107, loss = 0.28719279\n",
      "Iteration 108, loss = 0.28594394\n",
      "Iteration 109, loss = 0.28550958\n",
      "Iteration 110, loss = 0.28396340\n",
      "Iteration 111, loss = 0.28255807\n",
      "Iteration 112, loss = 0.28175595\n",
      "Iteration 113, loss = 0.28082933\n",
      "Iteration 114, loss = 0.27959918\n",
      "Iteration 115, loss = 0.27899145\n",
      "Iteration 116, loss = 0.27747555\n",
      "Iteration 117, loss = 0.27644232\n",
      "Iteration 118, loss = 0.27575307\n",
      "Iteration 119, loss = 0.27409377\n",
      "Iteration 120, loss = 0.27366411\n",
      "Iteration 121, loss = 0.27235211\n",
      "Iteration 122, loss = 0.27135807\n",
      "Iteration 123, loss = 0.27116299\n",
      "Iteration 124, loss = 0.27066017\n",
      "Iteration 125, loss = 0.26901068\n",
      "Iteration 126, loss = 0.26774582\n",
      "Iteration 127, loss = 0.26715754\n",
      "Iteration 128, loss = 0.26587078\n",
      "Iteration 129, loss = 0.26486954\n",
      "Iteration 130, loss = 0.26444120\n",
      "Iteration 131, loss = 0.26315782\n",
      "Iteration 132, loss = 0.26254028\n",
      "Iteration 133, loss = 0.26147340\n",
      "Iteration 134, loss = 0.26074956\n",
      "Iteration 135, loss = 0.26034315\n",
      "Iteration 136, loss = 0.25888104\n",
      "Iteration 137, loss = 0.25862881\n",
      "Iteration 138, loss = 0.25796657\n",
      "Iteration 139, loss = 0.25700031\n",
      "Iteration 140, loss = 0.25606259\n",
      "Iteration 141, loss = 0.25529996\n",
      "Iteration 142, loss = 0.25471324\n",
      "Iteration 143, loss = 0.25398443\n",
      "Iteration 144, loss = 0.25329758\n",
      "Iteration 145, loss = 0.25263429\n",
      "Iteration 146, loss = 0.25193564\n",
      "Iteration 147, loss = 0.25117345\n",
      "Iteration 148, loss = 0.25058172\n",
      "Iteration 149, loss = 0.24949555\n",
      "Iteration 150, loss = 0.24896227\n",
      "Iteration 151, loss = 0.24820992\n",
      "Iteration 152, loss = 0.24814043\n",
      "Iteration 153, loss = 0.24684550\n",
      "Iteration 154, loss = 0.24600715\n",
      "Iteration 155, loss = 0.24554060\n",
      "Iteration 156, loss = 0.24492649\n",
      "Iteration 157, loss = 0.24453296\n",
      "Iteration 158, loss = 0.24388314\n",
      "Iteration 159, loss = 0.24285908\n",
      "Iteration 160, loss = 0.24233911\n",
      "Iteration 161, loss = 0.24188788\n",
      "Iteration 162, loss = 0.24143912\n",
      "Iteration 163, loss = 0.24091810\n",
      "Iteration 164, loss = 0.24083697\n",
      "Iteration 165, loss = 0.24018204\n",
      "Iteration 166, loss = 0.23941051\n",
      "Iteration 167, loss = 0.23841239\n",
      "Iteration 168, loss = 0.23827321\n",
      "Iteration 169, loss = 0.23735630\n",
      "Iteration 170, loss = 0.23685030\n",
      "Iteration 171, loss = 0.23757747\n",
      "Iteration 172, loss = 0.23656329\n",
      "Iteration 173, loss = 0.23539476\n",
      "Iteration 174, loss = 0.23508616\n",
      "Iteration 175, loss = 0.23446278\n",
      "Iteration 176, loss = 0.23404534\n",
      "Iteration 177, loss = 0.23322844\n",
      "Iteration 178, loss = 0.23328781\n",
      "Iteration 179, loss = 0.23267394\n",
      "Iteration 180, loss = 0.23181569\n",
      "Iteration 181, loss = 0.23127517\n",
      "Iteration 182, loss = 0.23122674\n",
      "Iteration 183, loss = 0.23060169\n",
      "Iteration 184, loss = 0.23013822\n",
      "Iteration 185, loss = 0.22988533\n",
      "Iteration 186, loss = 0.22971265\n",
      "Iteration 187, loss = 0.22915584\n",
      "Iteration 188, loss = 0.22824500\n",
      "Iteration 189, loss = 0.22806140\n",
      "Iteration 190, loss = 0.22738195\n",
      "Iteration 191, loss = 0.22724613\n",
      "Iteration 192, loss = 0.22656107\n",
      "Iteration 193, loss = 0.22604726\n",
      "Iteration 194, loss = 0.22560645\n",
      "Iteration 195, loss = 0.22599602\n",
      "Iteration 196, loss = 0.22526943\n",
      "Iteration 197, loss = 0.22476677\n",
      "Iteration 198, loss = 0.22478400\n",
      "Iteration 199, loss = 0.22428351\n",
      "Iteration 200, loss = 0.22354576\n",
      "Iteration 201, loss = 0.22319372\n",
      "Iteration 202, loss = 0.22270552\n",
      "Iteration 203, loss = 0.22265411\n",
      "Iteration 204, loss = 0.22196143\n",
      "Iteration 205, loss = 0.22174312\n",
      "Iteration 206, loss = 0.22124617\n",
      "Iteration 207, loss = 0.22106065\n",
      "Iteration 208, loss = 0.22087467\n",
      "Iteration 209, loss = 0.22139542\n",
      "Iteration 210, loss = 0.21991134\n",
      "Iteration 211, loss = 0.22023193\n",
      "Iteration 212, loss = 0.21994803\n",
      "Iteration 213, loss = 0.21989286\n",
      "Iteration 214, loss = 0.21907619\n",
      "Iteration 215, loss = 0.21825383\n",
      "Iteration 216, loss = 0.21791737\n",
      "Iteration 217, loss = 0.21800222\n",
      "Iteration 218, loss = 0.21748044\n",
      "Iteration 219, loss = 0.21724561\n",
      "Iteration 220, loss = 0.21818195\n",
      "Iteration 221, loss = 0.21708341\n",
      "Iteration 222, loss = 0.21695077\n",
      "Iteration 223, loss = 0.21614153\n",
      "Iteration 224, loss = 0.21627305\n",
      "Iteration 225, loss = 0.21722401\n",
      "Iteration 226, loss = 0.21701149\n",
      "Iteration 227, loss = 0.21500981\n",
      "Iteration 228, loss = 0.21539708\n",
      "Iteration 229, loss = 0.21447538\n",
      "Iteration 230, loss = 0.21439066\n",
      "Iteration 231, loss = 0.21458485\n",
      "Iteration 232, loss = 0.21410250\n",
      "Iteration 233, loss = 0.21390524\n",
      "Iteration 234, loss = 0.21376075\n",
      "Iteration 235, loss = 0.21307354\n",
      "Iteration 236, loss = 0.21318424\n",
      "Iteration 237, loss = 0.21282411\n",
      "Iteration 238, loss = 0.21279896\n",
      "Iteration 239, loss = 0.21251155\n",
      "Iteration 240, loss = 0.21211648\n",
      "Iteration 241, loss = 0.21196368\n",
      "Iteration 242, loss = 0.21154431\n",
      "Iteration 243, loss = 0.21136415\n",
      "Iteration 244, loss = 0.21132284\n",
      "Iteration 245, loss = 0.21085239\n",
      "Iteration 246, loss = 0.21091068\n",
      "Iteration 247, loss = 0.21045237\n",
      "Iteration 248, loss = 0.21038723\n",
      "Iteration 249, loss = 0.21045019\n",
      "Iteration 250, loss = 0.20999735\n",
      "Iteration 251, loss = 0.21000666\n",
      "Iteration 252, loss = 0.20932327\n",
      "Iteration 253, loss = 0.20921518\n",
      "Iteration 254, loss = 0.20988047\n",
      "Iteration 255, loss = 0.20960238\n",
      "Iteration 256, loss = 0.20917878\n",
      "Iteration 257, loss = 0.20842732\n",
      "Iteration 258, loss = 0.20819556\n",
      "Iteration 259, loss = 0.20839163\n",
      "Iteration 260, loss = 0.20779936\n",
      "Iteration 261, loss = 0.20786702\n",
      "Iteration 262, loss = 0.20764630\n",
      "Iteration 263, loss = 0.20718012\n",
      "Iteration 264, loss = 0.20766281\n",
      "Iteration 265, loss = 0.20703190\n",
      "Iteration 266, loss = 0.20749914\n",
      "Iteration 267, loss = 0.20716373\n",
      "Iteration 268, loss = 0.20665450\n",
      "Iteration 269, loss = 0.20592767\n",
      "Iteration 270, loss = 0.20648663\n",
      "Iteration 271, loss = 0.20655328\n",
      "Iteration 272, loss = 0.20528293\n",
      "Iteration 273, loss = 0.20620882\n",
      "Iteration 274, loss = 0.20524294\n",
      "Iteration 275, loss = 0.20517966\n",
      "Iteration 276, loss = 0.20536208\n",
      "Iteration 277, loss = 0.20469333\n",
      "Iteration 278, loss = 0.20454986\n",
      "Iteration 279, loss = 0.20475833\n",
      "Iteration 280, loss = 0.20435694\n",
      "Iteration 281, loss = 0.20432069\n",
      "Iteration 282, loss = 0.20425371\n",
      "Iteration 283, loss = 0.20372258\n",
      "Iteration 284, loss = 0.20351583\n",
      "Iteration 285, loss = 0.20345748\n",
      "Iteration 286, loss = 0.20348897\n",
      "Iteration 287, loss = 0.20321331\n",
      "Iteration 288, loss = 0.20337030\n",
      "Iteration 289, loss = 0.20331324\n",
      "Iteration 290, loss = 0.20317924\n",
      "Iteration 291, loss = 0.20268890\n",
      "Iteration 292, loss = 0.20296513\n",
      "Iteration 293, loss = 0.20241414\n",
      "Iteration 294, loss = 0.20226259\n",
      "Iteration 295, loss = 0.20244137\n",
      "Iteration 296, loss = 0.20247764\n",
      "Iteration 297, loss = 0.20224707\n",
      "Iteration 298, loss = 0.20208079\n",
      "Iteration 299, loss = 0.20181403\n",
      "Iteration 300, loss = 0.20158058\n",
      "Iteration 301, loss = 0.20133811\n",
      "Iteration 302, loss = 0.20147595\n",
      "Iteration 303, loss = 0.20129651\n",
      "Iteration 304, loss = 0.20112079\n",
      "Iteration 305, loss = 0.20108629\n",
      "Iteration 306, loss = 0.20107952\n",
      "Iteration 307, loss = 0.20114750\n",
      "Iteration 308, loss = 0.20104539\n",
      "Iteration 309, loss = 0.20136726\n",
      "Iteration 310, loss = 0.20139869\n",
      "Iteration 311, loss = 0.20046570\n",
      "Iteration 312, loss = 0.20057156\n",
      "Iteration 313, loss = 0.20028515\n",
      "Iteration 314, loss = 0.20020753\n",
      "Iteration 315, loss = 0.19987157\n",
      "Iteration 316, loss = 0.20009722\n",
      "Iteration 317, loss = 0.20059982\n",
      "Iteration 318, loss = 0.20052142\n",
      "Iteration 319, loss = 0.20008176\n",
      "Iteration 320, loss = 0.19955637\n",
      "Iteration 321, loss = 0.20045843\n",
      "Iteration 322, loss = 0.20038382\n",
      "Iteration 323, loss = 0.19923400\n",
      "Iteration 324, loss = 0.19945811\n",
      "Iteration 325, loss = 0.19918213\n",
      "Iteration 326, loss = 0.19901077\n",
      "Iteration 327, loss = 0.19936930\n",
      "Iteration 328, loss = 0.19844238\n",
      "Iteration 329, loss = 0.19867547\n",
      "Iteration 330, loss = 0.19859544\n",
      "Iteration 331, loss = 0.19852956\n",
      "Iteration 332, loss = 0.19842753\n",
      "Iteration 333, loss = 0.19859942\n",
      "Iteration 334, loss = 0.19907682\n",
      "Iteration 335, loss = 0.19826546\n",
      "Iteration 336, loss = 0.19800711\n",
      "Iteration 337, loss = 0.19830759\n",
      "Iteration 338, loss = 0.19877484\n",
      "Iteration 339, loss = 0.19773862\n",
      "Iteration 340, loss = 0.19755839\n",
      "Iteration 341, loss = 0.19748685\n",
      "Iteration 342, loss = 0.19743418\n",
      "Iteration 343, loss = 0.19741513\n",
      "Iteration 344, loss = 0.19732231\n",
      "Iteration 345, loss = 0.19748283\n",
      "Iteration 346, loss = 0.19754260\n",
      "Iteration 347, loss = 0.19760541\n",
      "Iteration 348, loss = 0.19724217\n",
      "Iteration 349, loss = 0.19701887\n",
      "Iteration 350, loss = 0.19675807\n",
      "Iteration 351, loss = 0.19725149\n",
      "Iteration 352, loss = 0.19715881\n",
      "Iteration 353, loss = 0.19714767\n",
      "Iteration 354, loss = 0.19707591\n",
      "Iteration 355, loss = 0.19690058\n",
      "Iteration 356, loss = 0.19666857\n",
      "Iteration 357, loss = 0.19640718\n",
      "Iteration 358, loss = 0.19638081\n",
      "Iteration 359, loss = 0.19689807\n",
      "Iteration 360, loss = 0.19648126\n",
      "Iteration 361, loss = 0.19588943\n",
      "Iteration 362, loss = 0.19616906\n",
      "Iteration 363, loss = 0.19591852\n",
      "Iteration 364, loss = 0.19634174\n",
      "Iteration 365, loss = 0.19603106\n",
      "Iteration 366, loss = 0.19641973\n",
      "Iteration 367, loss = 0.19587054\n",
      "Iteration 368, loss = 0.19552291\n",
      "Iteration 369, loss = 0.19564425\n",
      "Iteration 370, loss = 0.19609372\n",
      "Iteration 371, loss = 0.19546893\n",
      "Iteration 372, loss = 0.19593465\n",
      "Iteration 373, loss = 0.19593048\n",
      "Iteration 374, loss = 0.19538242\n",
      "Iteration 375, loss = 0.19544206\n",
      "Iteration 376, loss = 0.19547124\n",
      "Iteration 377, loss = 0.19488036\n",
      "Iteration 378, loss = 0.19487521\n",
      "Iteration 379, loss = 0.19494195\n",
      "Iteration 380, loss = 0.19504516\n",
      "Iteration 381, loss = 0.19450277\n",
      "Iteration 382, loss = 0.19488765\n",
      "Iteration 383, loss = 0.19471031\n",
      "Iteration 384, loss = 0.19442896\n",
      "Iteration 385, loss = 0.19477116\n",
      "Iteration 386, loss = 0.19481628\n",
      "Iteration 387, loss = 0.19446500\n",
      "Iteration 388, loss = 0.19442883\n",
      "Iteration 389, loss = 0.19472296\n",
      "Iteration 390, loss = 0.19430351\n",
      "Iteration 391, loss = 0.19445300\n",
      "Iteration 392, loss = 0.19456540\n",
      "Iteration 393, loss = 0.19392096\n",
      "Iteration 394, loss = 0.19420869\n",
      "Iteration 395, loss = 0.19409582\n",
      "Iteration 396, loss = 0.19384603\n",
      "Iteration 397, loss = 0.19376195\n",
      "Iteration 398, loss = 0.19386430\n",
      "Iteration 399, loss = 0.19396194\n",
      "Iteration 400, loss = 0.19380960\n",
      "Iteration 401, loss = 0.19391304\n",
      "Iteration 402, loss = 0.19386843\n",
      "Iteration 403, loss = 0.19361079\n",
      "Iteration 404, loss = 0.19353886\n",
      "Iteration 405, loss = 0.19355234\n",
      "Iteration 406, loss = 0.19331794\n",
      "Iteration 407, loss = 0.19321207\n",
      "Iteration 408, loss = 0.19356409\n",
      "Iteration 409, loss = 0.19442657\n",
      "Iteration 410, loss = 0.19445493\n",
      "Iteration 411, loss = 0.19440723\n",
      "Iteration 412, loss = 0.19373028\n",
      "Iteration 413, loss = 0.19315797\n",
      "Iteration 414, loss = 0.19314019\n",
      "Iteration 415, loss = 0.19301731\n",
      "Iteration 416, loss = 0.19302943\n",
      "Iteration 417, loss = 0.19309848\n",
      "Iteration 418, loss = 0.19276878\n",
      "Iteration 419, loss = 0.19325964\n",
      "Iteration 420, loss = 0.19266099\n",
      "Iteration 421, loss = 0.19290372\n",
      "Iteration 422, loss = 0.19219442\n",
      "Iteration 423, loss = 0.19258134\n",
      "Iteration 424, loss = 0.19217548\n",
      "Iteration 425, loss = 0.19236050\n",
      "Iteration 426, loss = 0.19149839\n",
      "Iteration 427, loss = 0.19168765\n",
      "Iteration 428, loss = 0.19251155\n",
      "Iteration 429, loss = 0.19166709\n",
      "Iteration 430, loss = 0.19124870\n",
      "Iteration 431, loss = 0.19114319\n",
      "Iteration 432, loss = 0.19103372\n",
      "Iteration 433, loss = 0.19165564\n",
      "Iteration 434, loss = 0.19127608\n",
      "Iteration 435, loss = 0.19128817\n",
      "Iteration 436, loss = 0.19081766\n",
      "Iteration 437, loss = 0.19048683\n",
      "Iteration 438, loss = 0.19042110\n",
      "Iteration 439, loss = 0.19053984\n",
      "Iteration 440, loss = 0.19063325\n",
      "Iteration 441, loss = 0.19052461\n",
      "Iteration 442, loss = 0.19003315\n",
      "Iteration 443, loss = 0.19030132\n",
      "Iteration 444, loss = 0.19005356\n",
      "Iteration 445, loss = 0.18995096\n",
      "Iteration 446, loss = 0.19000070\n",
      "Iteration 447, loss = 0.18969976\n",
      "Iteration 448, loss = 0.18969656\n",
      "Iteration 449, loss = 0.19020887\n",
      "Iteration 450, loss = 0.18963225\n",
      "Iteration 451, loss = 0.18980644\n",
      "Iteration 452, loss = 0.18957996\n",
      "Iteration 453, loss = 0.18930587\n",
      "Iteration 454, loss = 0.18930789\n",
      "Iteration 455, loss = 0.18920914\n",
      "Iteration 456, loss = 0.18977378\n",
      "Iteration 457, loss = 0.18901911\n",
      "Iteration 458, loss = 0.18883706\n",
      "Iteration 459, loss = 0.18911605\n",
      "Iteration 460, loss = 0.18895183\n",
      "Iteration 461, loss = 0.18938881\n",
      "Iteration 462, loss = 0.18849920\n",
      "Iteration 463, loss = 0.18848349\n",
      "Iteration 464, loss = 0.18844751\n",
      "Iteration 465, loss = 0.18853595\n",
      "Iteration 466, loss = 0.18789929\n",
      "Iteration 467, loss = 0.18811986\n",
      "Iteration 468, loss = 0.18752231\n",
      "Iteration 469, loss = 0.18815219\n",
      "Iteration 470, loss = 0.18735327\n",
      "Iteration 471, loss = 0.18747090\n",
      "Iteration 472, loss = 0.18687275\n",
      "Iteration 473, loss = 0.18666026\n",
      "Iteration 474, loss = 0.18673167\n",
      "Iteration 475, loss = 0.18729909\n",
      "Iteration 476, loss = 0.18639382\n",
      "Iteration 477, loss = 0.18590918\n",
      "Iteration 478, loss = 0.18598030\n",
      "Iteration 479, loss = 0.18567376\n",
      "Iteration 480, loss = 0.18578752\n",
      "Iteration 481, loss = 0.18624359\n",
      "Iteration 482, loss = 0.18624451\n",
      "Iteration 483, loss = 0.18560627\n",
      "Iteration 484, loss = 0.18531409\n",
      "Iteration 485, loss = 0.18533961\n",
      "Iteration 486, loss = 0.18510883\n",
      "Iteration 487, loss = 0.18504004\n",
      "Iteration 488, loss = 0.18482356\n",
      "Iteration 489, loss = 0.18497843\n",
      "Iteration 490, loss = 0.18475482\n",
      "Iteration 491, loss = 0.18446837\n",
      "Iteration 492, loss = 0.18433100\n",
      "Iteration 493, loss = 0.18445290\n",
      "Iteration 494, loss = 0.18445650\n",
      "Iteration 495, loss = 0.18420239\n",
      "Iteration 496, loss = 0.18402592\n",
      "Iteration 497, loss = 0.18373757\n",
      "Iteration 498, loss = 0.18376603\n",
      "Iteration 499, loss = 0.18383486\n",
      "Iteration 500, loss = 0.18375237\n",
      "Iteration 501, loss = 0.18387439\n",
      "Iteration 502, loss = 0.18409270\n",
      "Iteration 503, loss = 0.18375135\n",
      "Iteration 504, loss = 0.18355250\n",
      "Iteration 505, loss = 0.18340357\n",
      "Iteration 506, loss = 0.18326824\n",
      "Iteration 507, loss = 0.18310463\n",
      "Iteration 508, loss = 0.18331726\n",
      "Iteration 509, loss = 0.18284046\n",
      "Iteration 510, loss = 0.18285204\n",
      "Iteration 511, loss = 0.18289661\n",
      "Iteration 512, loss = 0.18298959\n",
      "Iteration 513, loss = 0.18315933\n",
      "Iteration 514, loss = 0.18291262\n",
      "Iteration 515, loss = 0.18273066\n",
      "Iteration 516, loss = 0.18269524\n",
      "Iteration 517, loss = 0.18246030\n",
      "Iteration 518, loss = 0.18250218\n",
      "Iteration 519, loss = 0.18281334\n",
      "Iteration 520, loss = 0.18330271\n",
      "Iteration 521, loss = 0.18509937\n",
      "Iteration 522, loss = 0.18435831\n",
      "Iteration 523, loss = 0.18243728\n",
      "Iteration 524, loss = 0.18221486\n",
      "Iteration 525, loss = 0.18224833\n",
      "Iteration 526, loss = 0.18200465\n",
      "Iteration 527, loss = 0.18215062\n",
      "Iteration 528, loss = 0.18221629\n",
      "Iteration 529, loss = 0.18179844\n",
      "Iteration 530, loss = 0.18175085\n",
      "Iteration 531, loss = 0.18190088\n",
      "Iteration 532, loss = 0.18210662\n",
      "Iteration 533, loss = 0.18151458\n",
      "Iteration 534, loss = 0.18172449\n",
      "Iteration 535, loss = 0.18148653\n",
      "Iteration 536, loss = 0.18149512\n",
      "Iteration 537, loss = 0.18203229\n",
      "Iteration 538, loss = 0.18120077\n",
      "Iteration 539, loss = 0.18157222\n",
      "Iteration 540, loss = 0.18155264\n",
      "Iteration 541, loss = 0.18171690\n",
      "Iteration 542, loss = 0.18188316\n",
      "Iteration 543, loss = 0.18152229\n",
      "Iteration 544, loss = 0.18112332\n",
      "Iteration 545, loss = 0.18132207\n",
      "Iteration 546, loss = 0.18131524\n",
      "Iteration 547, loss = 0.18079293\n",
      "Iteration 548, loss = 0.18106555\n",
      "Iteration 549, loss = 0.18112674\n",
      "Iteration 550, loss = 0.18102726\n",
      "Iteration 551, loss = 0.18100698\n",
      "Iteration 552, loss = 0.18103963\n",
      "Iteration 553, loss = 0.18084688\n",
      "Iteration 554, loss = 0.18070153\n",
      "Iteration 555, loss = 0.18060305\n",
      "Iteration 556, loss = 0.18095399\n",
      "Iteration 557, loss = 0.18077172\n",
      "Iteration 558, loss = 0.18068405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76919097\n",
      "Iteration 2, loss = 0.74318465\n",
      "Iteration 3, loss = 0.72113176\n",
      "Iteration 4, loss = 0.70467078\n",
      "Iteration 5, loss = 0.69091756\n",
      "Iteration 6, loss = 0.67761822\n",
      "Iteration 7, loss = 0.66365437\n",
      "Iteration 8, loss = 0.64775567\n",
      "Iteration 9, loss = 0.62969322\n",
      "Iteration 10, loss = 0.61049017\n",
      "Iteration 11, loss = 0.59080859\n",
      "Iteration 12, loss = 0.57254879\n",
      "Iteration 13, loss = 0.55665589\n",
      "Iteration 14, loss = 0.54235146\n",
      "Iteration 15, loss = 0.53086846\n",
      "Iteration 16, loss = 0.52026235\n",
      "Iteration 17, loss = 0.51086087\n",
      "Iteration 18, loss = 0.50261581\n",
      "Iteration 19, loss = 0.49506804\n",
      "Iteration 20, loss = 0.48821982\n",
      "Iteration 21, loss = 0.48201845\n",
      "Iteration 22, loss = 0.47601476\n",
      "Iteration 23, loss = 0.47056327\n",
      "Iteration 24, loss = 0.46529432\n",
      "Iteration 25, loss = 0.46046433\n",
      "Iteration 26, loss = 0.45568528\n",
      "Iteration 27, loss = 0.45155938\n",
      "Iteration 28, loss = 0.44729976\n",
      "Iteration 29, loss = 0.44346312\n",
      "Iteration 30, loss = 0.43939477\n",
      "Iteration 31, loss = 0.43556074\n",
      "Iteration 32, loss = 0.43167398\n",
      "Iteration 33, loss = 0.42805974\n",
      "Iteration 34, loss = 0.42481891\n",
      "Iteration 35, loss = 0.42092522\n",
      "Iteration 36, loss = 0.41779820\n",
      "Iteration 37, loss = 0.41452481\n",
      "Iteration 38, loss = 0.41110142\n",
      "Iteration 39, loss = 0.40802183\n",
      "Iteration 40, loss = 0.40495614\n",
      "Iteration 41, loss = 0.40219384\n",
      "Iteration 42, loss = 0.39924340\n",
      "Iteration 43, loss = 0.39688785\n",
      "Iteration 44, loss = 0.39364625\n",
      "Iteration 45, loss = 0.39084667\n",
      "Iteration 46, loss = 0.38849967\n",
      "Iteration 47, loss = 0.38574774\n",
      "Iteration 48, loss = 0.38324377\n",
      "Iteration 49, loss = 0.38072717\n",
      "Iteration 50, loss = 0.37846359\n",
      "Iteration 51, loss = 0.37590612\n",
      "Iteration 52, loss = 0.37357533\n",
      "Iteration 53, loss = 0.37131497\n",
      "Iteration 54, loss = 0.36905301\n",
      "Iteration 55, loss = 0.36716797\n",
      "Iteration 56, loss = 0.36529525\n",
      "Iteration 57, loss = 0.36316717\n",
      "Iteration 58, loss = 0.36084236\n",
      "Iteration 59, loss = 0.35877475\n",
      "Iteration 60, loss = 0.35664851\n",
      "Iteration 61, loss = 0.35466775\n",
      "Iteration 62, loss = 0.35278375\n",
      "Iteration 63, loss = 0.35082542\n",
      "Iteration 64, loss = 0.34921210\n",
      "Iteration 65, loss = 0.34734633\n",
      "Iteration 66, loss = 0.34543298\n",
      "Iteration 67, loss = 0.34372976\n",
      "Iteration 68, loss = 0.34190973\n",
      "Iteration 69, loss = 0.33991687\n",
      "Iteration 70, loss = 0.33798346\n",
      "Iteration 71, loss = 0.33612837\n",
      "Iteration 72, loss = 0.33450055\n",
      "Iteration 73, loss = 0.33245979\n",
      "Iteration 74, loss = 0.33078020\n",
      "Iteration 75, loss = 0.32906181\n",
      "Iteration 76, loss = 0.32730304\n",
      "Iteration 77, loss = 0.32639595\n",
      "Iteration 78, loss = 0.32432662\n",
      "Iteration 79, loss = 0.32269702\n",
      "Iteration 80, loss = 0.32095640\n",
      "Iteration 81, loss = 0.31932977\n",
      "Iteration 82, loss = 0.31779178\n",
      "Iteration 83, loss = 0.31637530\n",
      "Iteration 84, loss = 0.31477676\n",
      "Iteration 85, loss = 0.31379264\n",
      "Iteration 86, loss = 0.31174590\n",
      "Iteration 87, loss = 0.31039225\n",
      "Iteration 88, loss = 0.30905741\n",
      "Iteration 89, loss = 0.30761202\n",
      "Iteration 90, loss = 0.30651090\n",
      "Iteration 91, loss = 0.30503812\n",
      "Iteration 92, loss = 0.30363332\n",
      "Iteration 93, loss = 0.30264272\n",
      "Iteration 94, loss = 0.30112293\n",
      "Iteration 95, loss = 0.29959160\n",
      "Iteration 96, loss = 0.29876919\n",
      "Iteration 97, loss = 0.29740014\n",
      "Iteration 98, loss = 0.29606871\n",
      "Iteration 99, loss = 0.29496062\n",
      "Iteration 100, loss = 0.29395665\n",
      "Iteration 101, loss = 0.29295659\n",
      "Iteration 102, loss = 0.29191675\n",
      "Iteration 103, loss = 0.29075085\n",
      "Iteration 104, loss = 0.28955514\n",
      "Iteration 105, loss = 0.28913360\n",
      "Iteration 106, loss = 0.28748461\n",
      "Iteration 107, loss = 0.28659613\n",
      "Iteration 108, loss = 0.28549578\n",
      "Iteration 109, loss = 0.28468812\n",
      "Iteration 110, loss = 0.28353235\n",
      "Iteration 111, loss = 0.28275807\n",
      "Iteration 112, loss = 0.28189978\n",
      "Iteration 113, loss = 0.28086841\n",
      "Iteration 114, loss = 0.27991691\n",
      "Iteration 115, loss = 0.27928677\n",
      "Iteration 116, loss = 0.27824429\n",
      "Iteration 117, loss = 0.27738720\n",
      "Iteration 118, loss = 0.27636179\n",
      "Iteration 119, loss = 0.27595102\n",
      "Iteration 120, loss = 0.27460140\n",
      "Iteration 121, loss = 0.27409664\n",
      "Iteration 122, loss = 0.27303989\n",
      "Iteration 123, loss = 0.27239398\n",
      "Iteration 124, loss = 0.27156563\n",
      "Iteration 125, loss = 0.27085090\n",
      "Iteration 126, loss = 0.27044006\n",
      "Iteration 127, loss = 0.26934431\n",
      "Iteration 128, loss = 0.26898535\n",
      "Iteration 129, loss = 0.26953460\n",
      "Iteration 130, loss = 0.26765628\n",
      "Iteration 131, loss = 0.26664607\n",
      "Iteration 132, loss = 0.26582764\n",
      "Iteration 133, loss = 0.26512819\n",
      "Iteration 134, loss = 0.26460516\n",
      "Iteration 135, loss = 0.26362473\n",
      "Iteration 136, loss = 0.26293847\n",
      "Iteration 137, loss = 0.26301064\n",
      "Iteration 138, loss = 0.26153715\n",
      "Iteration 139, loss = 0.26120573\n",
      "Iteration 140, loss = 0.26028784\n",
      "Iteration 141, loss = 0.25977517\n",
      "Iteration 142, loss = 0.25989559\n",
      "Iteration 143, loss = 0.25864393\n",
      "Iteration 144, loss = 0.25779966\n",
      "Iteration 145, loss = 0.25725485\n",
      "Iteration 146, loss = 0.25695197\n",
      "Iteration 147, loss = 0.25609766\n",
      "Iteration 148, loss = 0.25568555\n",
      "Iteration 149, loss = 0.25510876\n",
      "Iteration 150, loss = 0.25444030\n",
      "Iteration 151, loss = 0.25419857\n",
      "Iteration 152, loss = 0.25333797\n",
      "Iteration 153, loss = 0.25369712\n",
      "Iteration 154, loss = 0.25203197\n",
      "Iteration 155, loss = 0.25143511\n",
      "Iteration 156, loss = 0.25016840\n",
      "Iteration 157, loss = 0.24987950\n",
      "Iteration 158, loss = 0.24887764\n",
      "Iteration 159, loss = 0.24854939\n",
      "Iteration 160, loss = 0.24803529\n",
      "Iteration 161, loss = 0.24719699\n",
      "Iteration 162, loss = 0.24677540\n",
      "Iteration 163, loss = 0.24619229\n",
      "Iteration 164, loss = 0.24564257\n",
      "Iteration 165, loss = 0.24501723\n",
      "Iteration 166, loss = 0.24433740\n",
      "Iteration 167, loss = 0.24404608\n",
      "Iteration 168, loss = 0.24358725\n",
      "Iteration 169, loss = 0.24263631\n",
      "Iteration 170, loss = 0.24267066\n",
      "Iteration 171, loss = 0.24210450\n",
      "Iteration 172, loss = 0.24131905\n",
      "Iteration 173, loss = 0.24149118\n",
      "Iteration 174, loss = 0.24022031\n",
      "Iteration 175, loss = 0.24021272\n",
      "Iteration 176, loss = 0.23969537\n",
      "Iteration 177, loss = 0.23889840\n",
      "Iteration 178, loss = 0.23890270\n",
      "Iteration 179, loss = 0.23769630\n",
      "Iteration 180, loss = 0.23737365\n",
      "Iteration 181, loss = 0.23705802\n",
      "Iteration 182, loss = 0.23652432\n",
      "Iteration 183, loss = 0.23604219\n",
      "Iteration 184, loss = 0.23535137\n",
      "Iteration 185, loss = 0.23507582\n",
      "Iteration 186, loss = 0.23440390\n",
      "Iteration 187, loss = 0.23386757\n",
      "Iteration 188, loss = 0.23357429\n",
      "Iteration 189, loss = 0.23316524\n",
      "Iteration 190, loss = 0.23263279\n",
      "Iteration 191, loss = 0.23315136\n",
      "Iteration 192, loss = 0.23148382\n",
      "Iteration 193, loss = 0.23163766\n",
      "Iteration 194, loss = 0.23092824\n",
      "Iteration 195, loss = 0.23060756\n",
      "Iteration 196, loss = 0.23028802\n",
      "Iteration 197, loss = 0.22933312\n",
      "Iteration 198, loss = 0.22923174\n",
      "Iteration 199, loss = 0.22918292\n",
      "Iteration 200, loss = 0.22856248\n",
      "Iteration 201, loss = 0.22812552\n",
      "Iteration 202, loss = 0.22774057\n",
      "Iteration 203, loss = 0.22726637\n",
      "Iteration 204, loss = 0.22755698\n",
      "Iteration 205, loss = 0.22642285\n",
      "Iteration 206, loss = 0.22636392\n",
      "Iteration 207, loss = 0.22649200\n",
      "Iteration 208, loss = 0.22535919\n",
      "Iteration 209, loss = 0.22543856\n",
      "Iteration 210, loss = 0.22466892\n",
      "Iteration 211, loss = 0.22497487\n",
      "Iteration 212, loss = 0.22509673\n",
      "Iteration 213, loss = 0.22373259\n",
      "Iteration 214, loss = 0.22332146\n",
      "Iteration 215, loss = 0.22347427\n",
      "Iteration 216, loss = 0.22299917\n",
      "Iteration 217, loss = 0.22241462\n",
      "Iteration 218, loss = 0.22261021\n",
      "Iteration 219, loss = 0.22181668\n",
      "Iteration 220, loss = 0.22179938\n",
      "Iteration 221, loss = 0.22138969\n",
      "Iteration 222, loss = 0.22097397\n",
      "Iteration 223, loss = 0.22073402\n",
      "Iteration 224, loss = 0.22057868\n",
      "Iteration 225, loss = 0.22026053\n",
      "Iteration 226, loss = 0.21945787\n",
      "Iteration 227, loss = 0.21967581\n",
      "Iteration 228, loss = 0.21894430\n",
      "Iteration 229, loss = 0.21860749\n",
      "Iteration 230, loss = 0.21845548\n",
      "Iteration 231, loss = 0.21801398\n",
      "Iteration 232, loss = 0.21801677\n",
      "Iteration 233, loss = 0.21739725\n",
      "Iteration 234, loss = 0.21749701\n",
      "Iteration 235, loss = 0.21724847\n",
      "Iteration 236, loss = 0.21687253\n",
      "Iteration 237, loss = 0.21621783\n",
      "Iteration 238, loss = 0.21613402\n",
      "Iteration 239, loss = 0.21565781\n",
      "Iteration 240, loss = 0.21554065\n",
      "Iteration 241, loss = 0.21509435\n",
      "Iteration 242, loss = 0.21505922\n",
      "Iteration 243, loss = 0.21446338\n",
      "Iteration 244, loss = 0.21424257\n",
      "Iteration 245, loss = 0.21409177\n",
      "Iteration 246, loss = 0.21399815\n",
      "Iteration 247, loss = 0.21354442\n",
      "Iteration 248, loss = 0.21353308\n",
      "Iteration 249, loss = 0.21397762\n",
      "Iteration 250, loss = 0.21334414\n",
      "Iteration 251, loss = 0.21272916\n",
      "Iteration 252, loss = 0.21254891\n",
      "Iteration 253, loss = 0.21215732\n",
      "Iteration 254, loss = 0.21208643\n",
      "Iteration 255, loss = 0.21138618\n",
      "Iteration 256, loss = 0.21162222\n",
      "Iteration 257, loss = 0.21111233\n",
      "Iteration 258, loss = 0.21090246\n",
      "Iteration 259, loss = 0.21033082\n",
      "Iteration 260, loss = 0.21004300\n",
      "Iteration 261, loss = 0.20999217\n",
      "Iteration 262, loss = 0.20959390\n",
      "Iteration 263, loss = 0.20936453\n",
      "Iteration 264, loss = 0.20953751\n",
      "Iteration 265, loss = 0.20853810\n",
      "Iteration 266, loss = 0.20887442\n",
      "Iteration 267, loss = 0.20851606\n",
      "Iteration 268, loss = 0.20835957\n",
      "Iteration 269, loss = 0.20804600\n",
      "Iteration 270, loss = 0.20760748\n",
      "Iteration 271, loss = 0.20780719\n",
      "Iteration 272, loss = 0.20748255\n",
      "Iteration 273, loss = 0.20745920\n",
      "Iteration 274, loss = 0.20708902\n",
      "Iteration 275, loss = 0.20659281\n",
      "Iteration 276, loss = 0.20641754\n",
      "Iteration 277, loss = 0.20613923\n",
      "Iteration 278, loss = 0.20639497\n",
      "Iteration 279, loss = 0.20572116\n",
      "Iteration 280, loss = 0.20645397\n",
      "Iteration 281, loss = 0.20572104\n",
      "Iteration 282, loss = 0.20570331\n",
      "Iteration 283, loss = 0.20551611\n",
      "Iteration 284, loss = 0.20522721\n",
      "Iteration 285, loss = 0.20502798\n",
      "Iteration 286, loss = 0.20453650\n",
      "Iteration 287, loss = 0.20480076\n",
      "Iteration 288, loss = 0.20472609\n",
      "Iteration 289, loss = 0.20414450\n",
      "Iteration 290, loss = 0.20418887\n",
      "Iteration 291, loss = 0.20378533\n",
      "Iteration 292, loss = 0.20316189\n",
      "Iteration 293, loss = 0.20320425\n",
      "Iteration 294, loss = 0.20279421\n",
      "Iteration 295, loss = 0.20269422\n",
      "Iteration 296, loss = 0.20283129\n",
      "Iteration 297, loss = 0.20191825\n",
      "Iteration 298, loss = 0.20183018\n",
      "Iteration 299, loss = 0.20161047\n",
      "Iteration 300, loss = 0.20121519\n",
      "Iteration 301, loss = 0.20088587\n",
      "Iteration 302, loss = 0.20068787\n",
      "Iteration 303, loss = 0.20047638\n",
      "Iteration 304, loss = 0.20044970\n",
      "Iteration 305, loss = 0.20104833\n",
      "Iteration 306, loss = 0.19969970\n",
      "Iteration 307, loss = 0.20024721\n",
      "Iteration 308, loss = 0.19957287\n",
      "Iteration 309, loss = 0.19911886\n",
      "Iteration 310, loss = 0.19897600\n",
      "Iteration 311, loss = 0.19912843\n",
      "Iteration 312, loss = 0.19883009\n",
      "Iteration 313, loss = 0.19869589\n",
      "Iteration 314, loss = 0.19844857\n",
      "Iteration 315, loss = 0.19932920\n",
      "Iteration 316, loss = 0.19800530\n",
      "Iteration 317, loss = 0.19769518\n",
      "Iteration 318, loss = 0.19694180\n",
      "Iteration 319, loss = 0.19761903\n",
      "Iteration 320, loss = 0.19680124\n",
      "Iteration 321, loss = 0.19656017\n",
      "Iteration 322, loss = 0.19619336\n",
      "Iteration 323, loss = 0.19668637\n",
      "Iteration 324, loss = 0.19556568\n",
      "Iteration 325, loss = 0.19580707\n",
      "Iteration 326, loss = 0.19566276\n",
      "Iteration 327, loss = 0.19550980\n",
      "Iteration 328, loss = 0.19528909\n",
      "Iteration 329, loss = 0.19492123\n",
      "Iteration 330, loss = 0.19589751\n",
      "Iteration 331, loss = 0.19572863\n",
      "Iteration 332, loss = 0.19429557\n",
      "Iteration 333, loss = 0.19456722\n",
      "Iteration 334, loss = 0.19443228\n",
      "Iteration 335, loss = 0.19389965\n",
      "Iteration 336, loss = 0.19432627\n",
      "Iteration 337, loss = 0.19390016\n",
      "Iteration 338, loss = 0.19374288\n",
      "Iteration 339, loss = 0.19297063\n",
      "Iteration 340, loss = 0.19315661\n",
      "Iteration 341, loss = 0.19253967\n",
      "Iteration 342, loss = 0.19297164\n",
      "Iteration 343, loss = 0.19309634\n",
      "Iteration 344, loss = 0.19263780\n",
      "Iteration 345, loss = 0.19221625\n",
      "Iteration 346, loss = 0.19226387\n",
      "Iteration 347, loss = 0.19254013\n",
      "Iteration 348, loss = 0.19185157\n",
      "Iteration 349, loss = 0.19190115\n",
      "Iteration 350, loss = 0.19156540\n",
      "Iteration 351, loss = 0.19125387\n",
      "Iteration 352, loss = 0.19119324\n",
      "Iteration 353, loss = 0.19099479\n",
      "Iteration 354, loss = 0.19129827\n",
      "Iteration 355, loss = 0.19033962\n",
      "Iteration 356, loss = 0.19053371\n",
      "Iteration 357, loss = 0.19054742\n",
      "Iteration 358, loss = 0.19027507\n",
      "Iteration 359, loss = 0.19018573\n",
      "Iteration 360, loss = 0.18974284\n",
      "Iteration 361, loss = 0.18958188\n",
      "Iteration 362, loss = 0.18917152\n",
      "Iteration 363, loss = 0.18972635\n",
      "Iteration 364, loss = 0.18947619\n",
      "Iteration 365, loss = 0.18922949\n",
      "Iteration 366, loss = 0.18922563\n",
      "Iteration 367, loss = 0.18878255\n",
      "Iteration 368, loss = 0.18882182\n",
      "Iteration 369, loss = 0.18855356\n",
      "Iteration 370, loss = 0.18830742\n",
      "Iteration 371, loss = 0.18844710\n",
      "Iteration 372, loss = 0.18865800\n",
      "Iteration 373, loss = 0.18847774\n",
      "Iteration 374, loss = 0.18781753\n",
      "Iteration 375, loss = 0.18780441\n",
      "Iteration 376, loss = 0.18773123\n",
      "Iteration 377, loss = 0.18750145\n",
      "Iteration 378, loss = 0.18775367\n",
      "Iteration 379, loss = 0.18706739\n",
      "Iteration 380, loss = 0.18762930\n",
      "Iteration 381, loss = 0.18750033\n",
      "Iteration 382, loss = 0.18706976\n",
      "Iteration 383, loss = 0.18687100\n",
      "Iteration 384, loss = 0.18720182\n",
      "Iteration 385, loss = 0.18692894\n",
      "Iteration 386, loss = 0.18703281\n",
      "Iteration 387, loss = 0.18660043\n",
      "Iteration 388, loss = 0.18671498\n",
      "Iteration 389, loss = 0.18673290\n",
      "Iteration 390, loss = 0.18676781\n",
      "Iteration 391, loss = 0.18597675\n",
      "Iteration 392, loss = 0.18630610\n",
      "Iteration 393, loss = 0.18666279\n",
      "Iteration 394, loss = 0.18577974\n",
      "Iteration 395, loss = 0.18591183\n",
      "Iteration 396, loss = 0.18687540\n",
      "Iteration 397, loss = 0.18615980\n",
      "Iteration 398, loss = 0.18610176\n",
      "Iteration 399, loss = 0.18594844\n",
      "Iteration 400, loss = 0.18624250\n",
      "Iteration 401, loss = 0.18600922\n",
      "Iteration 402, loss = 0.18596358\n",
      "Iteration 403, loss = 0.18545205\n",
      "Iteration 404, loss = 0.18545078\n",
      "Iteration 405, loss = 0.18512018\n",
      "Iteration 406, loss = 0.18517409\n",
      "Iteration 407, loss = 0.18529900\n",
      "Iteration 408, loss = 0.18491882\n",
      "Iteration 409, loss = 0.18634752\n",
      "Iteration 410, loss = 0.18491923\n",
      "Iteration 411, loss = 0.18436677\n",
      "Iteration 412, loss = 0.18471712\n",
      "Iteration 413, loss = 0.18414270\n",
      "Iteration 414, loss = 0.18455083\n",
      "Iteration 415, loss = 0.18404735\n",
      "Iteration 416, loss = 0.18433896\n",
      "Iteration 417, loss = 0.18434790\n",
      "Iteration 418, loss = 0.18403950\n",
      "Iteration 419, loss = 0.18377853\n",
      "Iteration 420, loss = 0.18387801\n",
      "Iteration 421, loss = 0.18373902\n",
      "Iteration 422, loss = 0.18367124\n",
      "Iteration 423, loss = 0.18309540\n",
      "Iteration 424, loss = 0.18355858\n",
      "Iteration 425, loss = 0.18506721\n",
      "Iteration 426, loss = 0.18341484\n",
      "Iteration 427, loss = 0.18297683\n",
      "Iteration 428, loss = 0.18274950\n",
      "Iteration 429, loss = 0.18247639\n",
      "Iteration 430, loss = 0.18243797\n",
      "Iteration 431, loss = 0.18257320\n",
      "Iteration 432, loss = 0.18180982\n",
      "Iteration 433, loss = 0.18187160\n",
      "Iteration 434, loss = 0.18191293\n",
      "Iteration 435, loss = 0.18172270\n",
      "Iteration 436, loss = 0.18165416\n",
      "Iteration 437, loss = 0.18130241\n",
      "Iteration 438, loss = 0.18135367\n",
      "Iteration 439, loss = 0.18101952\n",
      "Iteration 440, loss = 0.18083671\n",
      "Iteration 441, loss = 0.18037223\n",
      "Iteration 442, loss = 0.18077014\n",
      "Iteration 443, loss = 0.18060254\n",
      "Iteration 444, loss = 0.18087352\n",
      "Iteration 445, loss = 0.18079887\n",
      "Iteration 446, loss = 0.17977842\n",
      "Iteration 447, loss = 0.17969933\n",
      "Iteration 448, loss = 0.18000015\n",
      "Iteration 449, loss = 0.18006821\n",
      "Iteration 450, loss = 0.17983561\n",
      "Iteration 451, loss = 0.17921726\n",
      "Iteration 452, loss = 0.17907362\n",
      "Iteration 453, loss = 0.17914766\n",
      "Iteration 454, loss = 0.17893363\n",
      "Iteration 455, loss = 0.17851771\n",
      "Iteration 456, loss = 0.17907962\n",
      "Iteration 457, loss = 0.17906335\n",
      "Iteration 458, loss = 0.17832645\n",
      "Iteration 459, loss = 0.17802072\n",
      "Iteration 460, loss = 0.17822332\n",
      "Iteration 461, loss = 0.17761659\n",
      "Iteration 462, loss = 0.17771452\n",
      "Iteration 463, loss = 0.17778438\n",
      "Iteration 464, loss = 0.17722978\n",
      "Iteration 465, loss = 0.17747223\n",
      "Iteration 466, loss = 0.17777537\n",
      "Iteration 467, loss = 0.17712226\n",
      "Iteration 468, loss = 0.17746396\n",
      "Iteration 469, loss = 0.17752315\n",
      "Iteration 470, loss = 0.17687772\n",
      "Iteration 471, loss = 0.17670159\n",
      "Iteration 472, loss = 0.17688114\n",
      "Iteration 473, loss = 0.17687068\n",
      "Iteration 474, loss = 0.17629808\n",
      "Iteration 475, loss = 0.17675177\n",
      "Iteration 476, loss = 0.17593218\n",
      "Iteration 477, loss = 0.17620434\n",
      "Iteration 478, loss = 0.17625939\n",
      "Iteration 479, loss = 0.17618009\n",
      "Iteration 480, loss = 0.17668321\n",
      "Iteration 481, loss = 0.17574710\n",
      "Iteration 482, loss = 0.17606962\n",
      "Iteration 483, loss = 0.17515976\n",
      "Iteration 484, loss = 0.17523004\n",
      "Iteration 485, loss = 0.17522556\n",
      "Iteration 486, loss = 0.17508074\n",
      "Iteration 487, loss = 0.17533304\n",
      "Iteration 488, loss = 0.17542871\n",
      "Iteration 489, loss = 0.17505128\n",
      "Iteration 490, loss = 0.17484996\n",
      "Iteration 491, loss = 0.17484953\n",
      "Iteration 492, loss = 0.17471189\n",
      "Iteration 493, loss = 0.17524767\n",
      "Iteration 494, loss = 0.17468043\n",
      "Iteration 495, loss = 0.17462757\n",
      "Iteration 496, loss = 0.17452562\n",
      "Iteration 497, loss = 0.17473509\n",
      "Iteration 498, loss = 0.17467950\n",
      "Iteration 499, loss = 0.17406604\n",
      "Iteration 500, loss = 0.17407755\n",
      "Iteration 501, loss = 0.17390147\n",
      "Iteration 502, loss = 0.17392858\n",
      "Iteration 503, loss = 0.17358637\n",
      "Iteration 504, loss = 0.17379526\n",
      "Iteration 505, loss = 0.17345059\n",
      "Iteration 506, loss = 0.17391665\n",
      "Iteration 507, loss = 0.17359264\n",
      "Iteration 508, loss = 0.17342923\n",
      "Iteration 509, loss = 0.17384435\n",
      "Iteration 510, loss = 0.17374127\n",
      "Iteration 511, loss = 0.17425066\n",
      "Iteration 512, loss = 0.17424299\n",
      "Iteration 513, loss = 0.17278421\n",
      "Iteration 514, loss = 0.17413984\n",
      "Iteration 515, loss = 0.17297277\n",
      "Iteration 516, loss = 0.17365659\n",
      "Iteration 517, loss = 0.17327311\n",
      "Iteration 518, loss = 0.17311035\n",
      "Iteration 519, loss = 0.17261921\n",
      "Iteration 520, loss = 0.17292049\n",
      "Iteration 521, loss = 0.17330970\n",
      "Iteration 522, loss = 0.17232705\n",
      "Iteration 523, loss = 0.17245657\n",
      "Iteration 524, loss = 0.17230651\n",
      "Iteration 525, loss = 0.17266666\n",
      "Iteration 526, loss = 0.17359605\n",
      "Iteration 527, loss = 0.17315259\n",
      "Iteration 528, loss = 0.17234339\n",
      "Iteration 529, loss = 0.17289056\n",
      "Iteration 530, loss = 0.17278028\n",
      "Iteration 531, loss = 0.17211494\n",
      "Iteration 532, loss = 0.17173235\n",
      "Iteration 533, loss = 0.17177547\n",
      "Iteration 534, loss = 0.17156452\n",
      "Iteration 535, loss = 0.17136813\n",
      "Iteration 536, loss = 0.17168070\n",
      "Iteration 537, loss = 0.17158004\n",
      "Iteration 538, loss = 0.17154901\n",
      "Iteration 539, loss = 0.17142517\n",
      "Iteration 540, loss = 0.17149318\n",
      "Iteration 541, loss = 0.17137161\n",
      "Iteration 542, loss = 0.17199136\n",
      "Iteration 543, loss = 0.17162597\n",
      "Iteration 544, loss = 0.17131521\n",
      "Iteration 545, loss = 0.17098057\n",
      "Iteration 546, loss = 0.17120592\n",
      "Iteration 547, loss = 0.17079411\n",
      "Iteration 548, loss = 0.17099074\n",
      "Iteration 549, loss = 0.17130483\n",
      "Iteration 550, loss = 0.17074387\n",
      "Iteration 551, loss = 0.17181711\n",
      "Iteration 552, loss = 0.17099941\n",
      "Iteration 553, loss = 0.17115551\n",
      "Iteration 554, loss = 0.17036078\n",
      "Iteration 555, loss = 0.17077560\n",
      "Iteration 556, loss = 0.17019980\n",
      "Iteration 557, loss = 0.17057687\n",
      "Iteration 558, loss = 0.17096622\n",
      "Iteration 559, loss = 0.17032315\n",
      "Iteration 560, loss = 0.17077701\n",
      "Iteration 561, loss = 0.17053108\n",
      "Iteration 562, loss = 0.17063512\n",
      "Iteration 563, loss = 0.17046350\n",
      "Iteration 564, loss = 0.17010255\n",
      "Iteration 565, loss = 0.17036401\n",
      "Iteration 566, loss = 0.16998330\n",
      "Iteration 567, loss = 0.17015892\n",
      "Iteration 568, loss = 0.16989495\n",
      "Iteration 569, loss = 0.17008368\n",
      "Iteration 570, loss = 0.17193042\n",
      "Iteration 571, loss = 0.17071599\n",
      "Iteration 572, loss = 0.16943863\n",
      "Iteration 573, loss = 0.17003694\n",
      "Iteration 574, loss = 0.17050828\n",
      "Iteration 575, loss = 0.17001188\n",
      "Iteration 576, loss = 0.16998820\n",
      "Iteration 577, loss = 0.16993100\n",
      "Iteration 578, loss = 0.17041621\n",
      "Iteration 579, loss = 0.16930459\n",
      "Iteration 580, loss = 0.16896506\n",
      "Iteration 581, loss = 0.16943417\n",
      "Iteration 582, loss = 0.16917515\n",
      "Iteration 583, loss = 0.16954463\n",
      "Iteration 584, loss = 0.16900494\n",
      "Iteration 585, loss = 0.16970914\n",
      "Iteration 586, loss = 0.16965026\n",
      "Iteration 587, loss = 0.16972699\n",
      "Iteration 588, loss = 0.16891766\n",
      "Iteration 589, loss = 0.16918188\n",
      "Iteration 590, loss = 0.16911727\n",
      "Iteration 591, loss = 0.16896163\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76952508\n",
      "Iteration 2, loss = 0.74314555\n",
      "Iteration 3, loss = 0.72179580\n",
      "Iteration 4, loss = 0.70497364\n",
      "Iteration 5, loss = 0.69061076\n",
      "Iteration 6, loss = 0.67664131\n",
      "Iteration 7, loss = 0.66155399\n",
      "Iteration 8, loss = 0.64434437\n",
      "Iteration 9, loss = 0.62511786\n",
      "Iteration 10, loss = 0.60528394\n",
      "Iteration 11, loss = 0.58600284\n",
      "Iteration 12, loss = 0.56813179\n",
      "Iteration 13, loss = 0.55225342\n",
      "Iteration 14, loss = 0.53846943\n",
      "Iteration 15, loss = 0.52725284\n",
      "Iteration 16, loss = 0.51698717\n",
      "Iteration 17, loss = 0.50786964\n",
      "Iteration 18, loss = 0.50019695\n",
      "Iteration 19, loss = 0.49252076\n",
      "Iteration 20, loss = 0.48599768\n",
      "Iteration 21, loss = 0.47977753\n",
      "Iteration 22, loss = 0.47403232\n",
      "Iteration 23, loss = 0.46845610\n",
      "Iteration 24, loss = 0.46316920\n",
      "Iteration 25, loss = 0.45802629\n",
      "Iteration 26, loss = 0.45351571\n",
      "Iteration 27, loss = 0.44919173\n",
      "Iteration 28, loss = 0.44501997\n",
      "Iteration 29, loss = 0.44098387\n",
      "Iteration 30, loss = 0.43716717\n",
      "Iteration 31, loss = 0.43356129\n",
      "Iteration 32, loss = 0.42986811\n",
      "Iteration 33, loss = 0.42646295\n",
      "Iteration 34, loss = 0.42309723\n",
      "Iteration 35, loss = 0.41974426\n",
      "Iteration 36, loss = 0.41643724\n",
      "Iteration 37, loss = 0.41322729\n",
      "Iteration 38, loss = 0.41012625\n",
      "Iteration 39, loss = 0.40725903\n",
      "Iteration 40, loss = 0.40444854\n",
      "Iteration 41, loss = 0.40107572\n",
      "Iteration 42, loss = 0.39888045\n",
      "Iteration 43, loss = 0.39551267\n",
      "Iteration 44, loss = 0.39299749\n",
      "Iteration 45, loss = 0.39045857\n",
      "Iteration 46, loss = 0.38746715\n",
      "Iteration 47, loss = 0.38496738\n",
      "Iteration 48, loss = 0.38257697\n",
      "Iteration 49, loss = 0.38016495\n",
      "Iteration 50, loss = 0.37773066\n",
      "Iteration 51, loss = 0.37534700\n",
      "Iteration 52, loss = 0.37317412\n",
      "Iteration 53, loss = 0.37088151\n",
      "Iteration 54, loss = 0.36908372\n",
      "Iteration 55, loss = 0.36665221\n",
      "Iteration 56, loss = 0.36445036\n",
      "Iteration 57, loss = 0.36266786\n",
      "Iteration 58, loss = 0.36028074\n",
      "Iteration 59, loss = 0.35833924\n",
      "Iteration 60, loss = 0.35639327\n",
      "Iteration 61, loss = 0.35427260\n",
      "Iteration 62, loss = 0.35227070\n",
      "Iteration 63, loss = 0.35055123\n",
      "Iteration 64, loss = 0.34856383\n",
      "Iteration 65, loss = 0.34675462\n",
      "Iteration 66, loss = 0.34509115\n",
      "Iteration 67, loss = 0.34324220\n",
      "Iteration 68, loss = 0.34136862\n",
      "Iteration 69, loss = 0.33969548\n",
      "Iteration 70, loss = 0.33795735\n",
      "Iteration 71, loss = 0.33665132\n",
      "Iteration 72, loss = 0.33496885\n",
      "Iteration 73, loss = 0.33335013\n",
      "Iteration 74, loss = 0.33155543\n",
      "Iteration 75, loss = 0.33039075\n",
      "Iteration 76, loss = 0.32860257\n",
      "Iteration 77, loss = 0.32696514\n",
      "Iteration 78, loss = 0.32554945\n",
      "Iteration 79, loss = 0.32414134\n",
      "Iteration 80, loss = 0.32261012\n",
      "Iteration 81, loss = 0.32113154\n",
      "Iteration 82, loss = 0.31978898\n",
      "Iteration 83, loss = 0.31823531\n",
      "Iteration 84, loss = 0.31680200\n",
      "Iteration 85, loss = 0.31565245\n",
      "Iteration 86, loss = 0.31388430\n",
      "Iteration 87, loss = 0.31301886\n",
      "Iteration 88, loss = 0.31165537\n",
      "Iteration 89, loss = 0.31009003\n",
      "Iteration 90, loss = 0.30902424\n",
      "Iteration 91, loss = 0.30757625\n",
      "Iteration 92, loss = 0.30676815\n",
      "Iteration 93, loss = 0.30530576\n",
      "Iteration 94, loss = 0.30386658\n",
      "Iteration 95, loss = 0.30260607\n",
      "Iteration 96, loss = 0.30134928\n",
      "Iteration 97, loss = 0.30025740\n",
      "Iteration 98, loss = 0.29936143\n",
      "Iteration 99, loss = 0.29824727\n",
      "Iteration 100, loss = 0.29684426\n",
      "Iteration 101, loss = 0.29567808\n",
      "Iteration 102, loss = 0.29455512\n",
      "Iteration 103, loss = 0.29349261\n",
      "Iteration 104, loss = 0.29242843\n",
      "Iteration 105, loss = 0.29126908\n",
      "Iteration 106, loss = 0.29006609\n",
      "Iteration 107, loss = 0.28885858\n",
      "Iteration 108, loss = 0.28782940\n",
      "Iteration 109, loss = 0.28686194\n",
      "Iteration 110, loss = 0.28597916\n",
      "Iteration 111, loss = 0.28509938\n",
      "Iteration 112, loss = 0.28391625\n",
      "Iteration 113, loss = 0.28282363\n",
      "Iteration 114, loss = 0.28193528\n",
      "Iteration 115, loss = 0.28107943\n",
      "Iteration 116, loss = 0.28013210\n",
      "Iteration 117, loss = 0.27935690\n",
      "Iteration 118, loss = 0.27815511\n",
      "Iteration 119, loss = 0.27723247\n",
      "Iteration 120, loss = 0.27640884\n",
      "Iteration 121, loss = 0.27554697\n",
      "Iteration 122, loss = 0.27465741\n",
      "Iteration 123, loss = 0.27373242\n",
      "Iteration 124, loss = 0.27290072\n",
      "Iteration 125, loss = 0.27187672\n",
      "Iteration 126, loss = 0.27117967\n",
      "Iteration 127, loss = 0.27036556\n",
      "Iteration 128, loss = 0.26952234\n",
      "Iteration 129, loss = 0.26897943\n",
      "Iteration 130, loss = 0.26787513\n",
      "Iteration 131, loss = 0.26723573\n",
      "Iteration 132, loss = 0.26642745\n",
      "Iteration 133, loss = 0.26566522\n",
      "Iteration 134, loss = 0.26503427\n",
      "Iteration 135, loss = 0.26421320\n",
      "Iteration 136, loss = 0.26339017\n",
      "Iteration 137, loss = 0.26327127\n",
      "Iteration 138, loss = 0.26223369\n",
      "Iteration 139, loss = 0.26124534\n",
      "Iteration 140, loss = 0.26093237\n",
      "Iteration 141, loss = 0.26007912\n",
      "Iteration 142, loss = 0.25918628\n",
      "Iteration 143, loss = 0.25895025\n",
      "Iteration 144, loss = 0.25819327\n",
      "Iteration 145, loss = 0.25720316\n",
      "Iteration 146, loss = 0.25652870\n",
      "Iteration 147, loss = 0.25600078\n",
      "Iteration 148, loss = 0.25552607\n",
      "Iteration 149, loss = 0.25460222\n",
      "Iteration 150, loss = 0.25407940\n",
      "Iteration 151, loss = 0.25345493\n",
      "Iteration 152, loss = 0.25294699\n",
      "Iteration 153, loss = 0.25236932\n",
      "Iteration 154, loss = 0.25177412\n",
      "Iteration 155, loss = 0.25128776\n",
      "Iteration 156, loss = 0.25037196\n",
      "Iteration 157, loss = 0.25063048\n",
      "Iteration 158, loss = 0.24985764\n",
      "Iteration 159, loss = 0.24952754\n",
      "Iteration 160, loss = 0.24804490\n",
      "Iteration 161, loss = 0.24783652\n",
      "Iteration 162, loss = 0.24697046\n",
      "Iteration 163, loss = 0.24636170\n",
      "Iteration 164, loss = 0.24557847\n",
      "Iteration 165, loss = 0.24531376\n",
      "Iteration 166, loss = 0.24453505\n",
      "Iteration 167, loss = 0.24415720\n",
      "Iteration 168, loss = 0.24340814\n",
      "Iteration 169, loss = 0.24277000\n",
      "Iteration 170, loss = 0.24246915\n",
      "Iteration 171, loss = 0.24203597\n",
      "Iteration 172, loss = 0.24159549\n",
      "Iteration 173, loss = 0.24102969\n",
      "Iteration 174, loss = 0.24015975\n",
      "Iteration 175, loss = 0.23973921\n",
      "Iteration 176, loss = 0.23923114\n",
      "Iteration 177, loss = 0.23887621\n",
      "Iteration 178, loss = 0.23820418\n",
      "Iteration 179, loss = 0.23786861\n",
      "Iteration 180, loss = 0.23725214\n",
      "Iteration 181, loss = 0.23732906\n",
      "Iteration 182, loss = 0.23629985\n",
      "Iteration 183, loss = 0.23606013\n",
      "Iteration 184, loss = 0.23563355\n",
      "Iteration 185, loss = 0.23512971\n",
      "Iteration 186, loss = 0.23511730\n",
      "Iteration 187, loss = 0.23395080\n",
      "Iteration 188, loss = 0.23387856\n",
      "Iteration 189, loss = 0.23325660\n",
      "Iteration 190, loss = 0.23274098\n",
      "Iteration 191, loss = 0.23262958\n",
      "Iteration 192, loss = 0.23209629\n",
      "Iteration 193, loss = 0.23137346\n",
      "Iteration 194, loss = 0.23095482\n",
      "Iteration 195, loss = 0.23037218\n",
      "Iteration 196, loss = 0.23035589\n",
      "Iteration 197, loss = 0.22967780\n",
      "Iteration 198, loss = 0.22961991\n",
      "Iteration 199, loss = 0.22930497\n",
      "Iteration 200, loss = 0.22860823\n",
      "Iteration 201, loss = 0.22811829\n",
      "Iteration 202, loss = 0.22750612\n",
      "Iteration 203, loss = 0.22776179\n",
      "Iteration 204, loss = 0.22660745\n",
      "Iteration 205, loss = 0.22650374\n",
      "Iteration 206, loss = 0.22607651\n",
      "Iteration 207, loss = 0.22604534\n",
      "Iteration 208, loss = 0.22576938\n",
      "Iteration 209, loss = 0.22509810\n",
      "Iteration 210, loss = 0.22462327\n",
      "Iteration 211, loss = 0.22455012\n",
      "Iteration 212, loss = 0.22373035\n",
      "Iteration 213, loss = 0.22363127\n",
      "Iteration 214, loss = 0.22334596\n",
      "Iteration 215, loss = 0.22303512\n",
      "Iteration 216, loss = 0.22231962\n",
      "Iteration 217, loss = 0.22199603\n",
      "Iteration 218, loss = 0.22153242\n",
      "Iteration 219, loss = 0.22145579\n",
      "Iteration 220, loss = 0.22101920\n",
      "Iteration 221, loss = 0.22060865\n",
      "Iteration 222, loss = 0.22080542\n",
      "Iteration 223, loss = 0.21983768\n",
      "Iteration 224, loss = 0.21951102\n",
      "Iteration 225, loss = 0.21924619\n",
      "Iteration 226, loss = 0.21895944\n",
      "Iteration 227, loss = 0.21872218\n",
      "Iteration 228, loss = 0.21787560\n",
      "Iteration 229, loss = 0.21761679\n",
      "Iteration 230, loss = 0.21702671\n",
      "Iteration 231, loss = 0.21626438\n",
      "Iteration 232, loss = 0.21641464\n",
      "Iteration 233, loss = 0.21614566\n",
      "Iteration 234, loss = 0.21537966\n",
      "Iteration 235, loss = 0.21516342\n",
      "Iteration 236, loss = 0.21486803\n",
      "Iteration 237, loss = 0.21431348\n",
      "Iteration 238, loss = 0.21420125\n",
      "Iteration 239, loss = 0.21382526\n",
      "Iteration 240, loss = 0.21385395\n",
      "Iteration 241, loss = 0.21317940\n",
      "Iteration 242, loss = 0.21267131\n",
      "Iteration 243, loss = 0.21225791\n",
      "Iteration 244, loss = 0.21217594\n",
      "Iteration 245, loss = 0.21183303\n",
      "Iteration 246, loss = 0.21213573\n",
      "Iteration 247, loss = 0.21087592\n",
      "Iteration 248, loss = 0.21104270\n",
      "Iteration 249, loss = 0.21060548\n",
      "Iteration 250, loss = 0.21062066\n",
      "Iteration 251, loss = 0.20982761\n",
      "Iteration 252, loss = 0.20995031\n",
      "Iteration 253, loss = 0.21031060\n",
      "Iteration 254, loss = 0.20936649\n",
      "Iteration 255, loss = 0.20945086\n",
      "Iteration 256, loss = 0.20901525\n",
      "Iteration 257, loss = 0.20895461\n",
      "Iteration 258, loss = 0.20843565\n",
      "Iteration 259, loss = 0.20782858\n",
      "Iteration 260, loss = 0.20769797\n",
      "Iteration 261, loss = 0.20757194\n",
      "Iteration 262, loss = 0.20700557\n",
      "Iteration 263, loss = 0.20753611\n",
      "Iteration 264, loss = 0.20681060\n",
      "Iteration 265, loss = 0.20657050\n",
      "Iteration 266, loss = 0.20688720\n",
      "Iteration 267, loss = 0.20595568\n",
      "Iteration 268, loss = 0.20581520\n",
      "Iteration 269, loss = 0.20548078\n",
      "Iteration 270, loss = 0.20539393\n",
      "Iteration 271, loss = 0.20594202\n",
      "Iteration 272, loss = 0.20475751\n",
      "Iteration 273, loss = 0.20509156\n",
      "Iteration 274, loss = 0.20465430\n",
      "Iteration 275, loss = 0.20408337\n",
      "Iteration 276, loss = 0.20419107\n",
      "Iteration 277, loss = 0.20366537\n",
      "Iteration 278, loss = 0.20383881\n",
      "Iteration 279, loss = 0.20351356\n",
      "Iteration 280, loss = 0.20304005\n",
      "Iteration 281, loss = 0.20291095\n",
      "Iteration 282, loss = 0.20275777\n",
      "Iteration 283, loss = 0.20211746\n",
      "Iteration 284, loss = 0.20217698\n",
      "Iteration 285, loss = 0.20189812\n",
      "Iteration 286, loss = 0.20172226\n",
      "Iteration 287, loss = 0.20220608\n",
      "Iteration 288, loss = 0.20141379\n",
      "Iteration 289, loss = 0.20094037\n",
      "Iteration 290, loss = 0.20049144\n",
      "Iteration 291, loss = 0.20034056\n",
      "Iteration 292, loss = 0.20038011\n",
      "Iteration 293, loss = 0.19983117\n",
      "Iteration 294, loss = 0.19975940\n",
      "Iteration 295, loss = 0.19963377\n",
      "Iteration 296, loss = 0.19953474\n",
      "Iteration 297, loss = 0.19916253\n",
      "Iteration 298, loss = 0.19888567\n",
      "Iteration 299, loss = 0.19927973\n",
      "Iteration 300, loss = 0.19899341\n",
      "Iteration 301, loss = 0.19834287\n",
      "Iteration 302, loss = 0.19882008\n",
      "Iteration 303, loss = 0.19881215\n",
      "Iteration 304, loss = 0.19779091\n",
      "Iteration 305, loss = 0.19785890\n",
      "Iteration 306, loss = 0.19737355\n",
      "Iteration 307, loss = 0.19712606\n",
      "Iteration 308, loss = 0.19715185\n",
      "Iteration 309, loss = 0.19670470\n",
      "Iteration 310, loss = 0.19665833\n",
      "Iteration 311, loss = 0.19647086\n",
      "Iteration 312, loss = 0.19621598\n",
      "Iteration 313, loss = 0.19658842\n",
      "Iteration 314, loss = 0.19660221\n",
      "Iteration 315, loss = 0.19587927\n",
      "Iteration 316, loss = 0.19556131\n",
      "Iteration 317, loss = 0.19563078\n",
      "Iteration 318, loss = 0.19513489\n",
      "Iteration 319, loss = 0.19559098\n",
      "Iteration 320, loss = 0.19516918\n",
      "Iteration 321, loss = 0.19466944\n",
      "Iteration 322, loss = 0.19506741\n",
      "Iteration 323, loss = 0.19441081\n",
      "Iteration 324, loss = 0.19448745\n",
      "Iteration 325, loss = 0.19419519\n",
      "Iteration 326, loss = 0.19396271\n",
      "Iteration 327, loss = 0.19393279\n",
      "Iteration 328, loss = 0.19419414\n",
      "Iteration 329, loss = 0.19383596\n",
      "Iteration 330, loss = 0.19423481\n",
      "Iteration 331, loss = 0.19338857\n",
      "Iteration 332, loss = 0.19338853\n",
      "Iteration 333, loss = 0.19317488\n",
      "Iteration 334, loss = 0.19312148\n",
      "Iteration 335, loss = 0.19276299\n",
      "Iteration 336, loss = 0.19358137\n",
      "Iteration 337, loss = 0.19214333\n",
      "Iteration 338, loss = 0.19241742\n",
      "Iteration 339, loss = 0.19234187\n",
      "Iteration 340, loss = 0.19216478\n",
      "Iteration 341, loss = 0.19239857\n",
      "Iteration 342, loss = 0.19225988\n",
      "Iteration 343, loss = 0.19186943\n",
      "Iteration 344, loss = 0.19233380\n",
      "Iteration 345, loss = 0.19185458\n",
      "Iteration 346, loss = 0.19174988\n",
      "Iteration 347, loss = 0.19133056\n",
      "Iteration 348, loss = 0.19094139\n",
      "Iteration 349, loss = 0.19129447\n",
      "Iteration 350, loss = 0.19131545\n",
      "Iteration 351, loss = 0.19095845\n",
      "Iteration 352, loss = 0.19056983\n",
      "Iteration 353, loss = 0.19068516\n",
      "Iteration 354, loss = 0.19037965\n",
      "Iteration 355, loss = 0.19072447\n",
      "Iteration 356, loss = 0.19065483\n",
      "Iteration 357, loss = 0.18982147\n",
      "Iteration 358, loss = 0.19035587\n",
      "Iteration 359, loss = 0.19003333\n",
      "Iteration 360, loss = 0.18981618\n",
      "Iteration 361, loss = 0.18902279\n",
      "Iteration 362, loss = 0.18910319\n",
      "Iteration 363, loss = 0.18908191\n",
      "Iteration 364, loss = 0.18938369\n",
      "Iteration 365, loss = 0.18874509\n",
      "Iteration 366, loss = 0.18918355\n",
      "Iteration 367, loss = 0.18858718\n",
      "Iteration 368, loss = 0.18888649\n",
      "Iteration 369, loss = 0.18830128\n",
      "Iteration 370, loss = 0.18851323\n",
      "Iteration 371, loss = 0.18819216\n",
      "Iteration 372, loss = 0.18798311\n",
      "Iteration 373, loss = 0.18803835\n",
      "Iteration 374, loss = 0.18758923\n",
      "Iteration 375, loss = 0.18758539\n",
      "Iteration 376, loss = 0.18748140\n",
      "Iteration 377, loss = 0.18726206\n",
      "Iteration 378, loss = 0.18758618\n",
      "Iteration 379, loss = 0.18687615\n",
      "Iteration 380, loss = 0.18725400\n",
      "Iteration 381, loss = 0.18673563\n",
      "Iteration 382, loss = 0.18691164\n",
      "Iteration 383, loss = 0.18636348\n",
      "Iteration 384, loss = 0.18661898\n",
      "Iteration 385, loss = 0.18643566\n",
      "Iteration 386, loss = 0.18635359\n",
      "Iteration 387, loss = 0.18601044\n",
      "Iteration 388, loss = 0.18584145\n",
      "Iteration 389, loss = 0.18599279\n",
      "Iteration 390, loss = 0.18601888\n",
      "Iteration 391, loss = 0.18582891\n",
      "Iteration 392, loss = 0.18564314\n",
      "Iteration 393, loss = 0.18552311\n",
      "Iteration 394, loss = 0.18610202\n",
      "Iteration 395, loss = 0.18604277\n",
      "Iteration 396, loss = 0.18556935\n",
      "Iteration 397, loss = 0.18529995\n",
      "Iteration 398, loss = 0.18528764\n",
      "Iteration 399, loss = 0.18604539\n",
      "Iteration 400, loss = 0.18515693\n",
      "Iteration 401, loss = 0.18505155\n",
      "Iteration 402, loss = 0.18503669\n",
      "Iteration 403, loss = 0.18449855\n",
      "Iteration 404, loss = 0.18436190\n",
      "Iteration 405, loss = 0.18478404\n",
      "Iteration 406, loss = 0.18424649\n",
      "Iteration 407, loss = 0.18396644\n",
      "Iteration 408, loss = 0.18377028\n",
      "Iteration 409, loss = 0.18399198\n",
      "Iteration 410, loss = 0.18383480\n",
      "Iteration 411, loss = 0.18398444\n",
      "Iteration 412, loss = 0.18362546\n",
      "Iteration 413, loss = 0.18501458\n",
      "Iteration 414, loss = 0.18379164\n",
      "Iteration 415, loss = 0.18521203\n",
      "Iteration 416, loss = 0.18314847\n",
      "Iteration 417, loss = 0.18330618\n",
      "Iteration 418, loss = 0.18339138\n",
      "Iteration 419, loss = 0.18304245\n",
      "Iteration 420, loss = 0.18281751\n",
      "Iteration 421, loss = 0.18255584\n",
      "Iteration 422, loss = 0.18260650\n",
      "Iteration 423, loss = 0.18273684\n",
      "Iteration 424, loss = 0.18276568\n",
      "Iteration 425, loss = 0.18252358\n",
      "Iteration 426, loss = 0.18276985\n",
      "Iteration 427, loss = 0.18232518\n",
      "Iteration 428, loss = 0.18243980\n",
      "Iteration 429, loss = 0.18239331\n",
      "Iteration 430, loss = 0.18202539\n",
      "Iteration 431, loss = 0.18182392\n",
      "Iteration 432, loss = 0.18195967\n",
      "Iteration 433, loss = 0.18206897\n",
      "Iteration 434, loss = 0.18241815\n",
      "Iteration 435, loss = 0.18177873\n",
      "Iteration 436, loss = 0.18190669\n",
      "Iteration 437, loss = 0.18191291\n",
      "Iteration 438, loss = 0.18182643\n",
      "Iteration 439, loss = 0.18178916\n",
      "Iteration 440, loss = 0.18112163\n",
      "Iteration 441, loss = 0.18151317\n",
      "Iteration 442, loss = 0.18121752\n",
      "Iteration 443, loss = 0.18108891\n",
      "Iteration 444, loss = 0.18144169\n",
      "Iteration 445, loss = 0.18074339\n",
      "Iteration 446, loss = 0.18095516\n",
      "Iteration 447, loss = 0.18101412\n",
      "Iteration 448, loss = 0.18083226\n",
      "Iteration 449, loss = 0.18043823\n",
      "Iteration 450, loss = 0.18043160\n",
      "Iteration 451, loss = 0.18020265\n",
      "Iteration 452, loss = 0.18042324\n",
      "Iteration 453, loss = 0.18062275\n",
      "Iteration 454, loss = 0.18038709\n",
      "Iteration 455, loss = 0.18020988\n",
      "Iteration 456, loss = 0.18008344\n",
      "Iteration 457, loss = 0.18068878\n",
      "Iteration 458, loss = 0.17984636\n",
      "Iteration 459, loss = 0.18018665\n",
      "Iteration 460, loss = 0.17940290\n",
      "Iteration 461, loss = 0.17953756\n",
      "Iteration 462, loss = 0.17962544\n",
      "Iteration 463, loss = 0.17941273\n",
      "Iteration 464, loss = 0.17923397\n",
      "Iteration 465, loss = 0.17939904\n",
      "Iteration 466, loss = 0.17952107\n",
      "Iteration 467, loss = 0.17938272\n",
      "Iteration 468, loss = 0.17934308\n",
      "Iteration 469, loss = 0.17945182\n",
      "Iteration 470, loss = 0.17884505\n",
      "Iteration 471, loss = 0.17903715\n",
      "Iteration 472, loss = 0.17916468\n",
      "Iteration 473, loss = 0.17967274\n",
      "Iteration 474, loss = 0.17906439\n",
      "Iteration 475, loss = 0.17914045\n",
      "Iteration 476, loss = 0.17867328\n",
      "Iteration 477, loss = 0.17849730\n",
      "Iteration 478, loss = 0.17853508\n",
      "Iteration 479, loss = 0.17868967\n",
      "Iteration 480, loss = 0.17849114\n",
      "Iteration 481, loss = 0.17835516\n",
      "Iteration 482, loss = 0.17855475\n",
      "Iteration 483, loss = 0.17901539\n",
      "Iteration 484, loss = 0.17802553\n",
      "Iteration 485, loss = 0.17844525\n",
      "Iteration 486, loss = 0.17817825\n",
      "Iteration 487, loss = 0.17822904\n",
      "Iteration 488, loss = 0.17823018\n",
      "Iteration 489, loss = 0.17797629\n",
      "Iteration 490, loss = 0.17783612\n",
      "Iteration 491, loss = 0.17817414\n",
      "Iteration 492, loss = 0.17798083\n",
      "Iteration 493, loss = 0.17766775\n",
      "Iteration 494, loss = 0.17763428\n",
      "Iteration 495, loss = 0.17752397\n",
      "Iteration 496, loss = 0.17832165\n",
      "Iteration 497, loss = 0.17756912\n",
      "Iteration 498, loss = 0.17757062\n",
      "Iteration 499, loss = 0.17813387\n",
      "Iteration 500, loss = 0.17736163\n",
      "Iteration 501, loss = 0.17720406\n",
      "Iteration 502, loss = 0.17711102\n",
      "Iteration 503, loss = 0.17741617\n",
      "Iteration 504, loss = 0.17711061\n",
      "Iteration 505, loss = 0.17726458\n",
      "Iteration 506, loss = 0.17694729\n",
      "Iteration 507, loss = 0.17767321\n",
      "Iteration 508, loss = 0.17682288\n",
      "Iteration 509, loss = 0.17724205\n",
      "Iteration 510, loss = 0.17671640\n",
      "Iteration 511, loss = 0.17658369\n",
      "Iteration 512, loss = 0.17692198\n",
      "Iteration 513, loss = 0.17699520\n",
      "Iteration 514, loss = 0.17688012\n",
      "Iteration 515, loss = 0.17686017\n",
      "Iteration 516, loss = 0.17691745\n",
      "Iteration 517, loss = 0.17688375\n",
      "Iteration 518, loss = 0.17644201\n",
      "Iteration 519, loss = 0.17656041\n",
      "Iteration 520, loss = 0.17633305\n",
      "Iteration 521, loss = 0.17668598\n",
      "Iteration 522, loss = 0.17699608\n",
      "Iteration 523, loss = 0.17635797\n",
      "Iteration 524, loss = 0.17619817\n",
      "Iteration 525, loss = 0.17657901\n",
      "Iteration 526, loss = 0.17603813\n",
      "Iteration 527, loss = 0.17594047\n",
      "Iteration 528, loss = 0.17606611\n",
      "Iteration 529, loss = 0.17623850\n",
      "Iteration 530, loss = 0.17638998\n",
      "Iteration 531, loss = 0.17609962\n",
      "Iteration 532, loss = 0.17590624\n",
      "Iteration 533, loss = 0.17552078\n",
      "Iteration 534, loss = 0.17630544\n",
      "Iteration 535, loss = 0.17710532\n",
      "Iteration 536, loss = 0.17652333\n",
      "Iteration 537, loss = 0.17584535\n",
      "Iteration 538, loss = 0.17541476\n",
      "Iteration 539, loss = 0.17556389\n",
      "Iteration 540, loss = 0.17570531\n",
      "Iteration 541, loss = 0.17509533\n",
      "Iteration 542, loss = 0.17544419\n",
      "Iteration 543, loss = 0.17631675\n",
      "Iteration 544, loss = 0.17551547\n",
      "Iteration 545, loss = 0.17513832\n",
      "Iteration 546, loss = 0.17515739\n",
      "Iteration 547, loss = 0.17487178\n",
      "Iteration 548, loss = 0.17486664\n",
      "Iteration 549, loss = 0.17685752\n",
      "Iteration 550, loss = 0.17489757\n",
      "Iteration 551, loss = 0.17580341\n",
      "Iteration 552, loss = 0.17479111\n",
      "Iteration 553, loss = 0.17539560\n",
      "Iteration 554, loss = 0.17477424\n",
      "Iteration 555, loss = 0.17483504\n",
      "Iteration 556, loss = 0.17503476\n",
      "Iteration 557, loss = 0.17575951\n",
      "Iteration 558, loss = 0.17501789\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76849261\n",
      "Iteration 2, loss = 0.74307207\n",
      "Iteration 3, loss = 0.72160275\n",
      "Iteration 4, loss = 0.70496126\n",
      "Iteration 5, loss = 0.69123791\n",
      "Iteration 6, loss = 0.67773055\n",
      "Iteration 7, loss = 0.66337417\n",
      "Iteration 8, loss = 0.64776285\n",
      "Iteration 9, loss = 0.62913457\n",
      "Iteration 10, loss = 0.61026932\n",
      "Iteration 11, loss = 0.59082074\n",
      "Iteration 12, loss = 0.57276135\n",
      "Iteration 13, loss = 0.55660375\n",
      "Iteration 14, loss = 0.54284497\n",
      "Iteration 15, loss = 0.53066391\n",
      "Iteration 16, loss = 0.52032271\n",
      "Iteration 17, loss = 0.51064140\n",
      "Iteration 18, loss = 0.50246524\n",
      "Iteration 19, loss = 0.49490165\n",
      "Iteration 20, loss = 0.48802686\n",
      "Iteration 21, loss = 0.48189740\n",
      "Iteration 22, loss = 0.47592921\n",
      "Iteration 23, loss = 0.47027573\n",
      "Iteration 24, loss = 0.46483264\n",
      "Iteration 25, loss = 0.46004199\n",
      "Iteration 26, loss = 0.45516765\n",
      "Iteration 27, loss = 0.45065169\n",
      "Iteration 28, loss = 0.44641288\n",
      "Iteration 29, loss = 0.44234505\n",
      "Iteration 30, loss = 0.43825033\n",
      "Iteration 31, loss = 0.43452121\n",
      "Iteration 32, loss = 0.43060430\n",
      "Iteration 33, loss = 0.42700354\n",
      "Iteration 34, loss = 0.42351656\n",
      "Iteration 35, loss = 0.41994577\n",
      "Iteration 36, loss = 0.41664846\n",
      "Iteration 37, loss = 0.41353667\n",
      "Iteration 38, loss = 0.41017208\n",
      "Iteration 39, loss = 0.40720419\n",
      "Iteration 40, loss = 0.40402252\n",
      "Iteration 41, loss = 0.40118954\n",
      "Iteration 42, loss = 0.39833676\n",
      "Iteration 43, loss = 0.39560691\n",
      "Iteration 44, loss = 0.39284359\n",
      "Iteration 45, loss = 0.39046641\n",
      "Iteration 46, loss = 0.38755989\n",
      "Iteration 47, loss = 0.38505535\n",
      "Iteration 48, loss = 0.38238491\n",
      "Iteration 49, loss = 0.38013844\n",
      "Iteration 50, loss = 0.37777994\n",
      "Iteration 51, loss = 0.37527392\n",
      "Iteration 52, loss = 0.37298132\n",
      "Iteration 53, loss = 0.37087988\n",
      "Iteration 54, loss = 0.36855265\n",
      "Iteration 55, loss = 0.36646428\n",
      "Iteration 56, loss = 0.36443762\n",
      "Iteration 57, loss = 0.36243931\n",
      "Iteration 58, loss = 0.36059236\n",
      "Iteration 59, loss = 0.35805476\n",
      "Iteration 60, loss = 0.35623374\n",
      "Iteration 61, loss = 0.35432157\n",
      "Iteration 62, loss = 0.35214296\n",
      "Iteration 63, loss = 0.35035313\n",
      "Iteration 64, loss = 0.34882427\n",
      "Iteration 65, loss = 0.34677179\n",
      "Iteration 66, loss = 0.34493847\n",
      "Iteration 67, loss = 0.34295966\n",
      "Iteration 68, loss = 0.34130371\n",
      "Iteration 69, loss = 0.33955048\n",
      "Iteration 70, loss = 0.33754672\n",
      "Iteration 71, loss = 0.33586749\n",
      "Iteration 72, loss = 0.33390423\n",
      "Iteration 73, loss = 0.33212254\n",
      "Iteration 74, loss = 0.33037109\n",
      "Iteration 75, loss = 0.32869230\n",
      "Iteration 76, loss = 0.32706374\n",
      "Iteration 77, loss = 0.32522402\n",
      "Iteration 78, loss = 0.32355039\n",
      "Iteration 79, loss = 0.32205048\n",
      "Iteration 80, loss = 0.32074573\n",
      "Iteration 81, loss = 0.31915219\n",
      "Iteration 82, loss = 0.31782595\n",
      "Iteration 83, loss = 0.31570711\n",
      "Iteration 84, loss = 0.31456490\n",
      "Iteration 85, loss = 0.31324573\n",
      "Iteration 86, loss = 0.31147681\n",
      "Iteration 87, loss = 0.31055378\n",
      "Iteration 88, loss = 0.30890183\n",
      "Iteration 89, loss = 0.30732347\n",
      "Iteration 90, loss = 0.30620831\n",
      "Iteration 91, loss = 0.30475276\n",
      "Iteration 92, loss = 0.30352389\n",
      "Iteration 93, loss = 0.30252238\n",
      "Iteration 94, loss = 0.30100036\n",
      "Iteration 95, loss = 0.29993402\n",
      "Iteration 96, loss = 0.29860913\n",
      "Iteration 97, loss = 0.29747836\n",
      "Iteration 98, loss = 0.29613449\n",
      "Iteration 99, loss = 0.29482208\n",
      "Iteration 100, loss = 0.29365936\n",
      "Iteration 101, loss = 0.29272255\n",
      "Iteration 102, loss = 0.29150000\n",
      "Iteration 103, loss = 0.29038552\n",
      "Iteration 104, loss = 0.28932495\n",
      "Iteration 105, loss = 0.28862121\n",
      "Iteration 106, loss = 0.28722782\n",
      "Iteration 107, loss = 0.28614199\n",
      "Iteration 108, loss = 0.28510567\n",
      "Iteration 109, loss = 0.28396354\n",
      "Iteration 110, loss = 0.28305578\n",
      "Iteration 111, loss = 0.28258738\n",
      "Iteration 112, loss = 0.28186787\n",
      "Iteration 113, loss = 0.28043686\n",
      "Iteration 114, loss = 0.27923976\n",
      "Iteration 115, loss = 0.27845720\n",
      "Iteration 116, loss = 0.27729691\n",
      "Iteration 117, loss = 0.27657657\n",
      "Iteration 118, loss = 0.27540424\n",
      "Iteration 119, loss = 0.27474619\n",
      "Iteration 120, loss = 0.27364105\n",
      "Iteration 121, loss = 0.27287955\n",
      "Iteration 122, loss = 0.27206164\n",
      "Iteration 123, loss = 0.27153905\n",
      "Iteration 124, loss = 0.27080296\n",
      "Iteration 125, loss = 0.26995582\n",
      "Iteration 126, loss = 0.26901878\n",
      "Iteration 127, loss = 0.26847561\n",
      "Iteration 128, loss = 0.26739601\n",
      "Iteration 129, loss = 0.26700024\n",
      "Iteration 130, loss = 0.26599976\n",
      "Iteration 131, loss = 0.26510312\n",
      "Iteration 132, loss = 0.26441846\n",
      "Iteration 133, loss = 0.26366896\n",
      "Iteration 134, loss = 0.26288528\n",
      "Iteration 135, loss = 0.26233269\n",
      "Iteration 136, loss = 0.26162467\n",
      "Iteration 137, loss = 0.26228464\n",
      "Iteration 138, loss = 0.26108237\n",
      "Iteration 139, loss = 0.25954575\n",
      "Iteration 140, loss = 0.25901105\n",
      "Iteration 141, loss = 0.25811848\n",
      "Iteration 142, loss = 0.25748621\n",
      "Iteration 143, loss = 0.25694105\n",
      "Iteration 144, loss = 0.25609885\n",
      "Iteration 145, loss = 0.25553930\n",
      "Iteration 146, loss = 0.25493912\n",
      "Iteration 147, loss = 0.25467853\n",
      "Iteration 148, loss = 0.25366060\n",
      "Iteration 149, loss = 0.25314322\n",
      "Iteration 150, loss = 0.25231132\n",
      "Iteration 151, loss = 0.25179180\n",
      "Iteration 152, loss = 0.25092496\n",
      "Iteration 153, loss = 0.25066338\n",
      "Iteration 154, loss = 0.25001203\n",
      "Iteration 155, loss = 0.24954482\n",
      "Iteration 156, loss = 0.24896171\n",
      "Iteration 157, loss = 0.24809553\n",
      "Iteration 158, loss = 0.24788328\n",
      "Iteration 159, loss = 0.24706583\n",
      "Iteration 160, loss = 0.24686990\n",
      "Iteration 161, loss = 0.24568710\n",
      "Iteration 162, loss = 0.24512459\n",
      "Iteration 163, loss = 0.24466121\n",
      "Iteration 164, loss = 0.24413987\n",
      "Iteration 165, loss = 0.24358135\n",
      "Iteration 166, loss = 0.24272824\n",
      "Iteration 167, loss = 0.24238637\n",
      "Iteration 168, loss = 0.24176403\n",
      "Iteration 169, loss = 0.24163611\n",
      "Iteration 170, loss = 0.24097778\n",
      "Iteration 171, loss = 0.24068497\n",
      "Iteration 172, loss = 0.23946338\n",
      "Iteration 173, loss = 0.23938962\n",
      "Iteration 174, loss = 0.23836521\n",
      "Iteration 175, loss = 0.23797709\n",
      "Iteration 176, loss = 0.23782751\n",
      "Iteration 177, loss = 0.23686855\n",
      "Iteration 178, loss = 0.23681346\n",
      "Iteration 179, loss = 0.23590548\n",
      "Iteration 180, loss = 0.23618126\n",
      "Iteration 181, loss = 0.23538153\n",
      "Iteration 182, loss = 0.23449462\n",
      "Iteration 183, loss = 0.23424486\n",
      "Iteration 184, loss = 0.23418207\n",
      "Iteration 185, loss = 0.23350201\n",
      "Iteration 186, loss = 0.23271574\n",
      "Iteration 187, loss = 0.23237018\n",
      "Iteration 188, loss = 0.23210731\n",
      "Iteration 189, loss = 0.23147254\n",
      "Iteration 190, loss = 0.23167529\n",
      "Iteration 191, loss = 0.23086541\n",
      "Iteration 192, loss = 0.23029891\n",
      "Iteration 193, loss = 0.22972265\n",
      "Iteration 194, loss = 0.22986895\n",
      "Iteration 195, loss = 0.22947147\n",
      "Iteration 196, loss = 0.22894563\n",
      "Iteration 197, loss = 0.22849532\n",
      "Iteration 198, loss = 0.22805550\n",
      "Iteration 199, loss = 0.22728952\n",
      "Iteration 200, loss = 0.22735368\n",
      "Iteration 201, loss = 0.22670040\n",
      "Iteration 202, loss = 0.22646157\n",
      "Iteration 203, loss = 0.22628834\n",
      "Iteration 204, loss = 0.22577146\n",
      "Iteration 205, loss = 0.22496300\n",
      "Iteration 206, loss = 0.22507942\n",
      "Iteration 207, loss = 0.22582030\n",
      "Iteration 208, loss = 0.22415526\n",
      "Iteration 209, loss = 0.22366364\n",
      "Iteration 210, loss = 0.22328843\n",
      "Iteration 211, loss = 0.22269713\n",
      "Iteration 212, loss = 0.22259848\n",
      "Iteration 213, loss = 0.22267691\n",
      "Iteration 214, loss = 0.22228816\n",
      "Iteration 215, loss = 0.22082036\n",
      "Iteration 216, loss = 0.22086033\n",
      "Iteration 217, loss = 0.22053797\n",
      "Iteration 218, loss = 0.22037192\n",
      "Iteration 219, loss = 0.21960725\n",
      "Iteration 220, loss = 0.22000658\n",
      "Iteration 221, loss = 0.21881596\n",
      "Iteration 222, loss = 0.21839288\n",
      "Iteration 223, loss = 0.21831808\n",
      "Iteration 224, loss = 0.21760078\n",
      "Iteration 225, loss = 0.21752462\n",
      "Iteration 226, loss = 0.21722575\n",
      "Iteration 227, loss = 0.21661088\n",
      "Iteration 228, loss = 0.21682998\n",
      "Iteration 229, loss = 0.21605547\n",
      "Iteration 230, loss = 0.21609823\n",
      "Iteration 231, loss = 0.21655028\n",
      "Iteration 232, loss = 0.21541779\n",
      "Iteration 233, loss = 0.21507046\n",
      "Iteration 234, loss = 0.21499180\n",
      "Iteration 235, loss = 0.21458871\n",
      "Iteration 236, loss = 0.21418828\n",
      "Iteration 237, loss = 0.21431406\n",
      "Iteration 238, loss = 0.21402971\n",
      "Iteration 239, loss = 0.21376633\n",
      "Iteration 240, loss = 0.21327467\n",
      "Iteration 241, loss = 0.21272651\n",
      "Iteration 242, loss = 0.21322191\n",
      "Iteration 243, loss = 0.21276115\n",
      "Iteration 244, loss = 0.21238284\n",
      "Iteration 245, loss = 0.21219478\n",
      "Iteration 246, loss = 0.21212171\n",
      "Iteration 247, loss = 0.21165035\n",
      "Iteration 248, loss = 0.21110446\n",
      "Iteration 249, loss = 0.21106678\n",
      "Iteration 250, loss = 0.21080491\n",
      "Iteration 251, loss = 0.21103220\n",
      "Iteration 252, loss = 0.21088335\n",
      "Iteration 253, loss = 0.21002670\n",
      "Iteration 254, loss = 0.21011581\n",
      "Iteration 255, loss = 0.21007544\n",
      "Iteration 256, loss = 0.20963019\n",
      "Iteration 257, loss = 0.20982110\n",
      "Iteration 258, loss = 0.20897199\n",
      "Iteration 259, loss = 0.20904792\n",
      "Iteration 260, loss = 0.20880756\n",
      "Iteration 261, loss = 0.20842549\n",
      "Iteration 262, loss = 0.20825914\n",
      "Iteration 263, loss = 0.20837525\n",
      "Iteration 264, loss = 0.20772249\n",
      "Iteration 265, loss = 0.20771872\n",
      "Iteration 266, loss = 0.20773812\n",
      "Iteration 267, loss = 0.20764970\n",
      "Iteration 268, loss = 0.20759157\n",
      "Iteration 269, loss = 0.20749676\n",
      "Iteration 270, loss = 0.20738316\n",
      "Iteration 271, loss = 0.20657729\n",
      "Iteration 272, loss = 0.20677921\n",
      "Iteration 273, loss = 0.20625615\n",
      "Iteration 274, loss = 0.20641118\n",
      "Iteration 275, loss = 0.20590504\n",
      "Iteration 276, loss = 0.20624839\n",
      "Iteration 277, loss = 0.20598188\n",
      "Iteration 278, loss = 0.20556878\n",
      "Iteration 279, loss = 0.20533253\n",
      "Iteration 280, loss = 0.20541086\n",
      "Iteration 281, loss = 0.20560162\n",
      "Iteration 282, loss = 0.20491330\n",
      "Iteration 283, loss = 0.20507235\n",
      "Iteration 284, loss = 0.20473856\n",
      "Iteration 285, loss = 0.20435964\n",
      "Iteration 286, loss = 0.20440515\n",
      "Iteration 287, loss = 0.20482159\n",
      "Iteration 288, loss = 0.20427991\n",
      "Iteration 289, loss = 0.20396426\n",
      "Iteration 290, loss = 0.20380052\n",
      "Iteration 291, loss = 0.20346722\n",
      "Iteration 292, loss = 0.20357400\n",
      "Iteration 293, loss = 0.20370924\n",
      "Iteration 294, loss = 0.20318757\n",
      "Iteration 295, loss = 0.20332784\n",
      "Iteration 296, loss = 0.20266148\n",
      "Iteration 297, loss = 0.20313024\n",
      "Iteration 298, loss = 0.20277240\n",
      "Iteration 299, loss = 0.20246894\n",
      "Iteration 300, loss = 0.20232589\n",
      "Iteration 301, loss = 0.20200825\n",
      "Iteration 302, loss = 0.20200814\n",
      "Iteration 303, loss = 0.20209123\n",
      "Iteration 304, loss = 0.20172348\n",
      "Iteration 305, loss = 0.20145251\n",
      "Iteration 306, loss = 0.20163864\n",
      "Iteration 307, loss = 0.20163999\n",
      "Iteration 308, loss = 0.20096267\n",
      "Iteration 309, loss = 0.20116147\n",
      "Iteration 310, loss = 0.20145847\n",
      "Iteration 311, loss = 0.20070112\n",
      "Iteration 312, loss = 0.20076486\n",
      "Iteration 313, loss = 0.20055265\n",
      "Iteration 314, loss = 0.20134626\n",
      "Iteration 315, loss = 0.20072758\n",
      "Iteration 316, loss = 0.19998165\n",
      "Iteration 317, loss = 0.20060173\n",
      "Iteration 318, loss = 0.20047034\n",
      "Iteration 319, loss = 0.20080637\n",
      "Iteration 320, loss = 0.19946500\n",
      "Iteration 321, loss = 0.19941750\n",
      "Iteration 322, loss = 0.19948604\n",
      "Iteration 323, loss = 0.19937351\n",
      "Iteration 324, loss = 0.19944680\n",
      "Iteration 325, loss = 0.19933568\n",
      "Iteration 326, loss = 0.19854765\n",
      "Iteration 327, loss = 0.19871923\n",
      "Iteration 328, loss = 0.19885948\n",
      "Iteration 329, loss = 0.19850138\n",
      "Iteration 330, loss = 0.19849960\n",
      "Iteration 331, loss = 0.19820643\n",
      "Iteration 332, loss = 0.19921725\n",
      "Iteration 333, loss = 0.19761774\n",
      "Iteration 334, loss = 0.19789906\n",
      "Iteration 335, loss = 0.19771414\n",
      "Iteration 336, loss = 0.19810548\n",
      "Iteration 337, loss = 0.19755430\n",
      "Iteration 338, loss = 0.19807159\n",
      "Iteration 339, loss = 0.19791294\n",
      "Iteration 340, loss = 0.19732048\n",
      "Iteration 341, loss = 0.19699413\n",
      "Iteration 342, loss = 0.19748239\n",
      "Iteration 343, loss = 0.19662454\n",
      "Iteration 344, loss = 0.19632097\n",
      "Iteration 345, loss = 0.19686376\n",
      "Iteration 346, loss = 0.19649053\n",
      "Iteration 347, loss = 0.19624048\n",
      "Iteration 348, loss = 0.19597771\n",
      "Iteration 349, loss = 0.19608740\n",
      "Iteration 350, loss = 0.19581518\n",
      "Iteration 351, loss = 0.19678527\n",
      "Iteration 352, loss = 0.19600671\n",
      "Iteration 353, loss = 0.19560118\n",
      "Iteration 354, loss = 0.19574893\n",
      "Iteration 355, loss = 0.19599129\n",
      "Iteration 356, loss = 0.19586401\n",
      "Iteration 357, loss = 0.19550053\n",
      "Iteration 358, loss = 0.19538366\n",
      "Iteration 359, loss = 0.19559864\n",
      "Iteration 360, loss = 0.19548140\n",
      "Iteration 361, loss = 0.19478273\n",
      "Iteration 362, loss = 0.19516806\n",
      "Iteration 363, loss = 0.19455656\n",
      "Iteration 364, loss = 0.19447734\n",
      "Iteration 365, loss = 0.19442321\n",
      "Iteration 366, loss = 0.19479317\n",
      "Iteration 367, loss = 0.19396494\n",
      "Iteration 368, loss = 0.19448387\n",
      "Iteration 369, loss = 0.19387747\n",
      "Iteration 370, loss = 0.19460745\n",
      "Iteration 371, loss = 0.19408898\n",
      "Iteration 372, loss = 0.19411548\n",
      "Iteration 373, loss = 0.19402087\n",
      "Iteration 374, loss = 0.19423868\n",
      "Iteration 375, loss = 0.19384207\n",
      "Iteration 376, loss = 0.19361871\n",
      "Iteration 377, loss = 0.19381591\n",
      "Iteration 378, loss = 0.19336325\n",
      "Iteration 379, loss = 0.19345185\n",
      "Iteration 380, loss = 0.19380821\n",
      "Iteration 381, loss = 0.19377952\n",
      "Iteration 382, loss = 0.19301308\n",
      "Iteration 383, loss = 0.19312148\n",
      "Iteration 384, loss = 0.19284653\n",
      "Iteration 385, loss = 0.19294838\n",
      "Iteration 386, loss = 0.19291813\n",
      "Iteration 387, loss = 0.19335406\n",
      "Iteration 388, loss = 0.19280844\n",
      "Iteration 389, loss = 0.19283032\n",
      "Iteration 390, loss = 0.19248715\n",
      "Iteration 391, loss = 0.19232845\n",
      "Iteration 392, loss = 0.19256644\n",
      "Iteration 393, loss = 0.19235730\n",
      "Iteration 394, loss = 0.19228798\n",
      "Iteration 395, loss = 0.19276430\n",
      "Iteration 396, loss = 0.19241920\n",
      "Iteration 397, loss = 0.19203819\n",
      "Iteration 398, loss = 0.19224587\n",
      "Iteration 399, loss = 0.19224714\n",
      "Iteration 400, loss = 0.19267040\n",
      "Iteration 401, loss = 0.19274598\n",
      "Iteration 402, loss = 0.19188476\n",
      "Iteration 403, loss = 0.19169634\n",
      "Iteration 404, loss = 0.19186935\n",
      "Iteration 405, loss = 0.19202778\n",
      "Iteration 406, loss = 0.19158907\n",
      "Iteration 407, loss = 0.19196396\n",
      "Iteration 408, loss = 0.19161941\n",
      "Iteration 409, loss = 0.19159882\n",
      "Iteration 410, loss = 0.19189825\n",
      "Iteration 411, loss = 0.19156546\n",
      "Iteration 412, loss = 0.19119328\n",
      "Iteration 413, loss = 0.19118965\n",
      "Iteration 414, loss = 0.19098296\n",
      "Iteration 415, loss = 0.19125645\n",
      "Iteration 416, loss = 0.19056057\n",
      "Iteration 417, loss = 0.19148975\n",
      "Iteration 418, loss = 0.19086598\n",
      "Iteration 419, loss = 0.19136256\n",
      "Iteration 420, loss = 0.19056741\n",
      "Iteration 421, loss = 0.19062136\n",
      "Iteration 422, loss = 0.19069173\n",
      "Iteration 423, loss = 0.19043129\n",
      "Iteration 424, loss = 0.19021056\n",
      "Iteration 425, loss = 0.18999025\n",
      "Iteration 426, loss = 0.19031930\n",
      "Iteration 427, loss = 0.18975731\n",
      "Iteration 428, loss = 0.18969120\n",
      "Iteration 429, loss = 0.18962871\n",
      "Iteration 430, loss = 0.18953000\n",
      "Iteration 431, loss = 0.19025414\n",
      "Iteration 432, loss = 0.18981795\n",
      "Iteration 433, loss = 0.18928781\n",
      "Iteration 434, loss = 0.18920538\n",
      "Iteration 435, loss = 0.18966844\n",
      "Iteration 436, loss = 0.19027305\n",
      "Iteration 437, loss = 0.18874149\n",
      "Iteration 438, loss = 0.18923988\n",
      "Iteration 439, loss = 0.18894058\n",
      "Iteration 440, loss = 0.18855295\n",
      "Iteration 441, loss = 0.18833446\n",
      "Iteration 442, loss = 0.18831572\n",
      "Iteration 443, loss = 0.18854840\n",
      "Iteration 444, loss = 0.18883811\n",
      "Iteration 445, loss = 0.18860781\n",
      "Iteration 446, loss = 0.18815930\n",
      "Iteration 447, loss = 0.18813284\n",
      "Iteration 448, loss = 0.18893793\n",
      "Iteration 449, loss = 0.18845341\n",
      "Iteration 450, loss = 0.18842068\n",
      "Iteration 451, loss = 0.18778686\n",
      "Iteration 452, loss = 0.18803644\n",
      "Iteration 453, loss = 0.18768366\n",
      "Iteration 454, loss = 0.18824090\n",
      "Iteration 455, loss = 0.18763615\n",
      "Iteration 456, loss = 0.18781004\n",
      "Iteration 457, loss = 0.18754066\n",
      "Iteration 458, loss = 0.18772069\n",
      "Iteration 459, loss = 0.18704775\n",
      "Iteration 460, loss = 0.18688714\n",
      "Iteration 461, loss = 0.18716430\n",
      "Iteration 462, loss = 0.18704797\n",
      "Iteration 463, loss = 0.18706828\n",
      "Iteration 464, loss = 0.18714572\n",
      "Iteration 465, loss = 0.18697452\n",
      "Iteration 466, loss = 0.18655064\n",
      "Iteration 467, loss = 0.18631286\n",
      "Iteration 468, loss = 0.18631667\n",
      "Iteration 469, loss = 0.18627628\n",
      "Iteration 470, loss = 0.18632870\n",
      "Iteration 471, loss = 0.18626513\n",
      "Iteration 472, loss = 0.18605446\n",
      "Iteration 473, loss = 0.18596097\n",
      "Iteration 474, loss = 0.18588176\n",
      "Iteration 475, loss = 0.18599136\n",
      "Iteration 476, loss = 0.18619276\n",
      "Iteration 477, loss = 0.18590970\n",
      "Iteration 478, loss = 0.18622898\n",
      "Iteration 479, loss = 0.18786512\n",
      "Iteration 480, loss = 0.18568957\n",
      "Iteration 481, loss = 0.18631922\n",
      "Iteration 482, loss = 0.18584018\n",
      "Iteration 483, loss = 0.18592192\n",
      "Iteration 484, loss = 0.18548186\n",
      "Iteration 485, loss = 0.18523630\n",
      "Iteration 486, loss = 0.18529604\n",
      "Iteration 487, loss = 0.18508652\n",
      "Iteration 488, loss = 0.18521163\n",
      "Iteration 489, loss = 0.18548877\n",
      "Iteration 490, loss = 0.18517246\n",
      "Iteration 491, loss = 0.18499601\n",
      "Iteration 492, loss = 0.18578652\n",
      "Iteration 493, loss = 0.18510995\n",
      "Iteration 494, loss = 0.18530041\n",
      "Iteration 495, loss = 0.18526264\n",
      "Iteration 496, loss = 0.18470503\n",
      "Iteration 497, loss = 0.18484581\n",
      "Iteration 498, loss = 0.18452366\n",
      "Iteration 499, loss = 0.18434570\n",
      "Iteration 500, loss = 0.18458563\n",
      "Iteration 501, loss = 0.18472621\n",
      "Iteration 502, loss = 0.18431949\n",
      "Iteration 503, loss = 0.18416087\n",
      "Iteration 504, loss = 0.18452282\n",
      "Iteration 505, loss = 0.18418653\n",
      "Iteration 506, loss = 0.18410852\n",
      "Iteration 507, loss = 0.18437422\n",
      "Iteration 508, loss = 0.18393367\n",
      "Iteration 509, loss = 0.18421917\n",
      "Iteration 510, loss = 0.18408155\n",
      "Iteration 511, loss = 0.18542327\n",
      "Iteration 512, loss = 0.18432806\n",
      "Iteration 513, loss = 0.18390657\n",
      "Iteration 514, loss = 0.18379647\n",
      "Iteration 515, loss = 0.18392283\n",
      "Iteration 516, loss = 0.18392067\n",
      "Iteration 517, loss = 0.18353946\n",
      "Iteration 518, loss = 0.18415886\n",
      "Iteration 519, loss = 0.18489092\n",
      "Iteration 520, loss = 0.18300003\n",
      "Iteration 521, loss = 0.18452671\n",
      "Iteration 522, loss = 0.18357092\n",
      "Iteration 523, loss = 0.18317461\n",
      "Iteration 524, loss = 0.18327096\n",
      "Iteration 525, loss = 0.18319735\n",
      "Iteration 526, loss = 0.18330220\n",
      "Iteration 527, loss = 0.18344695\n",
      "Iteration 528, loss = 0.18438701\n",
      "Iteration 529, loss = 0.18649592\n",
      "Iteration 530, loss = 0.18439469\n",
      "Iteration 531, loss = 0.18368684\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76843420\n",
      "Iteration 2, loss = 0.74268827\n",
      "Iteration 3, loss = 0.72093215\n",
      "Iteration 4, loss = 0.70367506\n",
      "Iteration 5, loss = 0.68932663\n",
      "Iteration 6, loss = 0.67526576\n",
      "Iteration 7, loss = 0.66063672\n",
      "Iteration 8, loss = 0.64420295\n",
      "Iteration 9, loss = 0.62525108\n",
      "Iteration 10, loss = 0.60611703\n",
      "Iteration 11, loss = 0.58645320\n",
      "Iteration 12, loss = 0.56838207\n",
      "Iteration 13, loss = 0.55262166\n",
      "Iteration 14, loss = 0.53925023\n",
      "Iteration 15, loss = 0.52768676\n",
      "Iteration 16, loss = 0.51714533\n",
      "Iteration 17, loss = 0.50770375\n",
      "Iteration 18, loss = 0.49946048\n",
      "Iteration 19, loss = 0.49215158\n",
      "Iteration 20, loss = 0.48561169\n",
      "Iteration 21, loss = 0.47929343\n",
      "Iteration 22, loss = 0.47380996\n",
      "Iteration 23, loss = 0.46806142\n",
      "Iteration 24, loss = 0.46284218\n",
      "Iteration 25, loss = 0.45815197\n",
      "Iteration 26, loss = 0.45352354\n",
      "Iteration 27, loss = 0.44933892\n",
      "Iteration 28, loss = 0.44520466\n",
      "Iteration 29, loss = 0.44094403\n",
      "Iteration 30, loss = 0.43718806\n",
      "Iteration 31, loss = 0.43316818\n",
      "Iteration 32, loss = 0.42953343\n",
      "Iteration 33, loss = 0.42590191\n",
      "Iteration 34, loss = 0.42221528\n",
      "Iteration 35, loss = 0.41887640\n",
      "Iteration 36, loss = 0.41559788\n",
      "Iteration 37, loss = 0.41229266\n",
      "Iteration 38, loss = 0.40911195\n",
      "Iteration 39, loss = 0.40628390\n",
      "Iteration 40, loss = 0.40324134\n",
      "Iteration 41, loss = 0.40020595\n",
      "Iteration 42, loss = 0.39739812\n",
      "Iteration 43, loss = 0.39459791\n",
      "Iteration 44, loss = 0.39195839\n",
      "Iteration 45, loss = 0.38948408\n",
      "Iteration 46, loss = 0.38671613\n",
      "Iteration 47, loss = 0.38427414\n",
      "Iteration 48, loss = 0.38200988\n",
      "Iteration 49, loss = 0.37930157\n",
      "Iteration 50, loss = 0.37701821\n",
      "Iteration 51, loss = 0.37450211\n",
      "Iteration 52, loss = 0.37192309\n",
      "Iteration 53, loss = 0.37028971\n",
      "Iteration 54, loss = 0.36745811\n",
      "Iteration 55, loss = 0.36534567\n",
      "Iteration 56, loss = 0.36342167\n",
      "Iteration 57, loss = 0.36153326\n",
      "Iteration 58, loss = 0.35898933\n",
      "Iteration 59, loss = 0.35689136\n",
      "Iteration 60, loss = 0.35498540\n",
      "Iteration 61, loss = 0.35311466\n",
      "Iteration 62, loss = 0.35111475\n",
      "Iteration 63, loss = 0.34910697\n",
      "Iteration 64, loss = 0.34798827\n",
      "Iteration 65, loss = 0.34533770\n",
      "Iteration 66, loss = 0.34459582\n",
      "Iteration 67, loss = 0.34225941\n",
      "Iteration 68, loss = 0.34036886\n",
      "Iteration 69, loss = 0.33961315\n",
      "Iteration 70, loss = 0.33740660\n",
      "Iteration 71, loss = 0.33560177\n",
      "Iteration 72, loss = 0.33396309\n",
      "Iteration 73, loss = 0.33229699\n",
      "Iteration 74, loss = 0.33071798\n",
      "Iteration 75, loss = 0.32906051\n",
      "Iteration 76, loss = 0.32767434\n",
      "Iteration 77, loss = 0.32594374\n",
      "Iteration 78, loss = 0.32434174\n",
      "Iteration 79, loss = 0.32249411\n",
      "Iteration 80, loss = 0.32090639\n",
      "Iteration 81, loss = 0.31928968\n",
      "Iteration 82, loss = 0.31759750\n",
      "Iteration 83, loss = 0.31620263\n",
      "Iteration 84, loss = 0.31466288\n",
      "Iteration 85, loss = 0.31331730\n",
      "Iteration 86, loss = 0.31189002\n",
      "Iteration 87, loss = 0.31031158\n",
      "Iteration 88, loss = 0.30887905\n",
      "Iteration 89, loss = 0.30769286\n",
      "Iteration 90, loss = 0.30646276\n",
      "Iteration 91, loss = 0.30527210\n",
      "Iteration 92, loss = 0.30393697\n",
      "Iteration 93, loss = 0.30253625\n",
      "Iteration 94, loss = 0.30115947\n",
      "Iteration 95, loss = 0.29982606\n",
      "Iteration 96, loss = 0.29874174\n",
      "Iteration 97, loss = 0.29754265\n",
      "Iteration 98, loss = 0.29620513\n",
      "Iteration 99, loss = 0.29512646\n",
      "Iteration 100, loss = 0.29389861\n",
      "Iteration 101, loss = 0.29277896\n",
      "Iteration 102, loss = 0.29170816\n",
      "Iteration 103, loss = 0.29066499\n",
      "Iteration 104, loss = 0.28943589\n",
      "Iteration 105, loss = 0.28861339\n",
      "Iteration 106, loss = 0.28740471\n",
      "Iteration 107, loss = 0.28638011\n",
      "Iteration 108, loss = 0.28549347\n",
      "Iteration 109, loss = 0.28445466\n",
      "Iteration 110, loss = 0.28358945\n",
      "Iteration 111, loss = 0.28243365\n",
      "Iteration 112, loss = 0.28137914\n",
      "Iteration 113, loss = 0.28096809\n",
      "Iteration 114, loss = 0.27966664\n",
      "Iteration 115, loss = 0.27872260\n",
      "Iteration 116, loss = 0.27786932\n",
      "Iteration 117, loss = 0.27694447\n",
      "Iteration 118, loss = 0.27595524\n",
      "Iteration 119, loss = 0.27506056\n",
      "Iteration 120, loss = 0.27425951\n",
      "Iteration 121, loss = 0.27353346\n",
      "Iteration 122, loss = 0.27233753\n",
      "Iteration 123, loss = 0.27195214\n",
      "Iteration 124, loss = 0.27090193\n",
      "Iteration 125, loss = 0.26983370\n",
      "Iteration 126, loss = 0.26930207\n",
      "Iteration 127, loss = 0.26931144\n",
      "Iteration 128, loss = 0.26780115\n",
      "Iteration 129, loss = 0.26683177\n",
      "Iteration 130, loss = 0.26634113\n",
      "Iteration 131, loss = 0.26534171\n",
      "Iteration 132, loss = 0.26460957\n",
      "Iteration 133, loss = 0.26392611\n",
      "Iteration 134, loss = 0.26298858\n",
      "Iteration 135, loss = 0.26235514\n",
      "Iteration 136, loss = 0.26158084\n",
      "Iteration 137, loss = 0.26071003\n",
      "Iteration 138, loss = 0.26014296\n",
      "Iteration 139, loss = 0.25951331\n",
      "Iteration 140, loss = 0.25867695\n",
      "Iteration 141, loss = 0.25784835\n",
      "Iteration 142, loss = 0.25707450\n",
      "Iteration 143, loss = 0.25670240\n",
      "Iteration 144, loss = 0.25613103\n",
      "Iteration 145, loss = 0.25516866\n",
      "Iteration 146, loss = 0.25464649\n",
      "Iteration 147, loss = 0.25394338\n",
      "Iteration 148, loss = 0.25333217\n",
      "Iteration 149, loss = 0.25338583\n",
      "Iteration 150, loss = 0.25227662\n",
      "Iteration 151, loss = 0.25167250\n",
      "Iteration 152, loss = 0.25107644\n",
      "Iteration 153, loss = 0.25119044\n",
      "Iteration 154, loss = 0.24973682\n",
      "Iteration 155, loss = 0.24906191\n",
      "Iteration 156, loss = 0.24865691\n",
      "Iteration 157, loss = 0.24790010\n",
      "Iteration 158, loss = 0.24721928\n",
      "Iteration 159, loss = 0.24712822\n",
      "Iteration 160, loss = 0.24643585\n",
      "Iteration 161, loss = 0.24568543\n",
      "Iteration 162, loss = 0.24547405\n",
      "Iteration 163, loss = 0.24516521\n",
      "Iteration 164, loss = 0.24474519\n",
      "Iteration 165, loss = 0.24341538\n",
      "Iteration 166, loss = 0.24351860\n",
      "Iteration 167, loss = 0.24235890\n",
      "Iteration 168, loss = 0.24202713\n",
      "Iteration 169, loss = 0.24163865\n",
      "Iteration 170, loss = 0.24112364\n",
      "Iteration 171, loss = 0.24051434\n",
      "Iteration 172, loss = 0.24045230\n",
      "Iteration 173, loss = 0.23952437\n",
      "Iteration 174, loss = 0.23909408\n",
      "Iteration 175, loss = 0.23869037\n",
      "Iteration 176, loss = 0.23879339\n",
      "Iteration 177, loss = 0.23769152\n",
      "Iteration 178, loss = 0.23764409\n",
      "Iteration 179, loss = 0.23687425\n",
      "Iteration 180, loss = 0.23647942\n",
      "Iteration 181, loss = 0.23614232\n",
      "Iteration 182, loss = 0.23599513\n",
      "Iteration 183, loss = 0.23561077\n",
      "Iteration 184, loss = 0.23468517\n",
      "Iteration 185, loss = 0.23418935\n",
      "Iteration 186, loss = 0.23376363\n",
      "Iteration 187, loss = 0.23338229\n",
      "Iteration 188, loss = 0.23279954\n",
      "Iteration 189, loss = 0.23259930\n",
      "Iteration 190, loss = 0.23208411\n",
      "Iteration 191, loss = 0.23205815\n",
      "Iteration 192, loss = 0.23139697\n",
      "Iteration 193, loss = 0.23112394\n",
      "Iteration 194, loss = 0.23056660\n",
      "Iteration 195, loss = 0.23004517\n",
      "Iteration 196, loss = 0.22965268\n",
      "Iteration 197, loss = 0.22983370\n",
      "Iteration 198, loss = 0.22944695\n",
      "Iteration 199, loss = 0.22875171\n",
      "Iteration 200, loss = 0.22841142\n",
      "Iteration 201, loss = 0.22781040\n",
      "Iteration 202, loss = 0.22764145\n",
      "Iteration 203, loss = 0.22721279\n",
      "Iteration 204, loss = 0.22670212\n",
      "Iteration 205, loss = 0.22721138\n",
      "Iteration 206, loss = 0.22624612\n",
      "Iteration 207, loss = 0.22617243\n",
      "Iteration 208, loss = 0.22517704\n",
      "Iteration 209, loss = 0.22530801\n",
      "Iteration 210, loss = 0.22513117\n",
      "Iteration 211, loss = 0.22487239\n",
      "Iteration 212, loss = 0.22420452\n",
      "Iteration 213, loss = 0.22500430\n",
      "Iteration 214, loss = 0.22370459\n",
      "Iteration 215, loss = 0.22359566\n",
      "Iteration 216, loss = 0.22339460\n",
      "Iteration 217, loss = 0.22277710\n",
      "Iteration 218, loss = 0.22277825\n",
      "Iteration 219, loss = 0.22262000\n",
      "Iteration 220, loss = 0.22197037\n",
      "Iteration 221, loss = 0.22126272\n",
      "Iteration 222, loss = 0.22107702\n",
      "Iteration 223, loss = 0.22082119\n",
      "Iteration 224, loss = 0.22040708\n",
      "Iteration 225, loss = 0.21961785\n",
      "Iteration 226, loss = 0.21967419\n",
      "Iteration 227, loss = 0.21991020\n",
      "Iteration 228, loss = 0.21913880\n",
      "Iteration 229, loss = 0.21951054\n",
      "Iteration 230, loss = 0.21878540\n",
      "Iteration 231, loss = 0.21813505\n",
      "Iteration 232, loss = 0.21849061\n",
      "Iteration 233, loss = 0.21721813\n",
      "Iteration 234, loss = 0.21733149\n",
      "Iteration 235, loss = 0.21660110\n",
      "Iteration 236, loss = 0.21680437\n",
      "Iteration 237, loss = 0.21623878\n",
      "Iteration 238, loss = 0.21595498\n",
      "Iteration 239, loss = 0.21556000\n",
      "Iteration 240, loss = 0.21525635\n",
      "Iteration 241, loss = 0.21512577\n",
      "Iteration 242, loss = 0.21667831\n",
      "Iteration 243, loss = 0.21464757\n",
      "Iteration 244, loss = 0.21410715\n",
      "Iteration 245, loss = 0.21346271\n",
      "Iteration 246, loss = 0.21332290\n",
      "Iteration 247, loss = 0.21359791\n",
      "Iteration 248, loss = 0.21280019\n",
      "Iteration 249, loss = 0.21287301\n",
      "Iteration 250, loss = 0.21199563\n",
      "Iteration 251, loss = 0.21230272\n",
      "Iteration 252, loss = 0.21176184\n",
      "Iteration 253, loss = 0.21114507\n",
      "Iteration 254, loss = 0.21100108\n",
      "Iteration 255, loss = 0.21091382\n",
      "Iteration 256, loss = 0.21045479\n",
      "Iteration 257, loss = 0.20997854\n",
      "Iteration 258, loss = 0.20980596\n",
      "Iteration 259, loss = 0.20967910\n",
      "Iteration 260, loss = 0.20953585\n",
      "Iteration 261, loss = 0.20904007\n",
      "Iteration 262, loss = 0.20892690\n",
      "Iteration 263, loss = 0.20850199\n",
      "Iteration 264, loss = 0.20828140\n",
      "Iteration 265, loss = 0.20953727\n",
      "Iteration 266, loss = 0.20940053\n",
      "Iteration 267, loss = 0.20815275\n",
      "Iteration 268, loss = 0.20736859\n",
      "Iteration 269, loss = 0.20674522\n",
      "Iteration 270, loss = 0.20626328\n",
      "Iteration 271, loss = 0.20672846\n",
      "Iteration 272, loss = 0.20657744\n",
      "Iteration 273, loss = 0.20590423\n",
      "Iteration 274, loss = 0.20622809\n",
      "Iteration 275, loss = 0.20580081\n",
      "Iteration 276, loss = 0.20522775\n",
      "Iteration 277, loss = 0.20488752\n",
      "Iteration 278, loss = 0.20475329\n",
      "Iteration 279, loss = 0.20430048\n",
      "Iteration 280, loss = 0.20450980\n",
      "Iteration 281, loss = 0.20386890\n",
      "Iteration 282, loss = 0.20344630\n",
      "Iteration 283, loss = 0.20335895\n",
      "Iteration 284, loss = 0.20314477\n",
      "Iteration 285, loss = 0.20287641\n",
      "Iteration 286, loss = 0.20283134\n",
      "Iteration 287, loss = 0.20242703\n",
      "Iteration 288, loss = 0.20233458\n",
      "Iteration 289, loss = 0.20185896\n",
      "Iteration 290, loss = 0.20160121\n",
      "Iteration 291, loss = 0.20132500\n",
      "Iteration 292, loss = 0.20219663\n",
      "Iteration 293, loss = 0.20098795\n",
      "Iteration 294, loss = 0.20063552\n",
      "Iteration 295, loss = 0.20147697\n",
      "Iteration 296, loss = 0.20051675\n",
      "Iteration 297, loss = 0.20025549\n",
      "Iteration 298, loss = 0.19963206\n",
      "Iteration 299, loss = 0.19954414\n",
      "Iteration 300, loss = 0.19934643\n",
      "Iteration 301, loss = 0.19923666\n",
      "Iteration 302, loss = 0.19889091\n",
      "Iteration 303, loss = 0.19863808\n",
      "Iteration 304, loss = 0.19922772\n",
      "Iteration 305, loss = 0.19996390\n",
      "Iteration 306, loss = 0.19769313\n",
      "Iteration 307, loss = 0.19841546\n",
      "Iteration 308, loss = 0.19798972\n",
      "Iteration 309, loss = 0.19740914\n",
      "Iteration 310, loss = 0.19779817\n",
      "Iteration 311, loss = 0.19716228\n",
      "Iteration 312, loss = 0.19741860\n",
      "Iteration 313, loss = 0.19680681\n",
      "Iteration 314, loss = 0.19661767\n",
      "Iteration 315, loss = 0.19654263\n",
      "Iteration 316, loss = 0.19609771\n",
      "Iteration 317, loss = 0.19609528\n",
      "Iteration 318, loss = 0.19589407\n",
      "Iteration 319, loss = 0.19554368\n",
      "Iteration 320, loss = 0.19517081\n",
      "Iteration 321, loss = 0.19542300\n",
      "Iteration 322, loss = 0.19481333\n",
      "Iteration 323, loss = 0.19474478\n",
      "Iteration 324, loss = 0.19463756\n",
      "Iteration 325, loss = 0.19452855\n",
      "Iteration 326, loss = 0.19437666\n",
      "Iteration 327, loss = 0.19375194\n",
      "Iteration 328, loss = 0.19352934\n",
      "Iteration 329, loss = 0.19330769\n",
      "Iteration 330, loss = 0.19328269\n",
      "Iteration 331, loss = 0.19241116\n",
      "Iteration 332, loss = 0.19243615\n",
      "Iteration 333, loss = 0.19200099\n",
      "Iteration 334, loss = 0.19205842\n",
      "Iteration 335, loss = 0.19133883\n",
      "Iteration 336, loss = 0.19151036\n",
      "Iteration 337, loss = 0.19103690\n",
      "Iteration 338, loss = 0.19084735\n",
      "Iteration 339, loss = 0.19094941\n",
      "Iteration 340, loss = 0.19075720\n",
      "Iteration 341, loss = 0.19090265\n",
      "Iteration 342, loss = 0.18996471\n",
      "Iteration 343, loss = 0.19024383\n",
      "Iteration 344, loss = 0.18941072\n",
      "Iteration 345, loss = 0.18931661\n",
      "Iteration 346, loss = 0.18977507\n",
      "Iteration 347, loss = 0.18929209\n",
      "Iteration 348, loss = 0.18892795\n",
      "Iteration 349, loss = 0.18865520\n",
      "Iteration 350, loss = 0.18880577\n",
      "Iteration 351, loss = 0.18900397\n",
      "Iteration 352, loss = 0.18884850\n",
      "Iteration 353, loss = 0.18799801\n",
      "Iteration 354, loss = 0.18760029\n",
      "Iteration 355, loss = 0.18751436\n",
      "Iteration 356, loss = 0.18739873\n",
      "Iteration 357, loss = 0.18736448\n",
      "Iteration 358, loss = 0.18755674\n",
      "Iteration 359, loss = 0.18700412\n",
      "Iteration 360, loss = 0.18690588\n",
      "Iteration 361, loss = 0.18664489\n",
      "Iteration 362, loss = 0.18653409\n",
      "Iteration 363, loss = 0.18647806\n",
      "Iteration 364, loss = 0.18665232\n",
      "Iteration 365, loss = 0.18589366\n",
      "Iteration 366, loss = 0.18577003\n",
      "Iteration 367, loss = 0.18561657\n",
      "Iteration 368, loss = 0.18611968\n",
      "Iteration 369, loss = 0.18619740\n",
      "Iteration 370, loss = 0.18584472\n",
      "Iteration 371, loss = 0.18535819\n",
      "Iteration 372, loss = 0.18499024\n",
      "Iteration 373, loss = 0.18483125\n",
      "Iteration 374, loss = 0.18520441\n",
      "Iteration 375, loss = 0.18621877\n",
      "Iteration 376, loss = 0.18506595\n",
      "Iteration 377, loss = 0.18508613\n",
      "Iteration 378, loss = 0.18483030\n",
      "Iteration 379, loss = 0.18459108\n",
      "Iteration 380, loss = 0.18411507\n",
      "Iteration 381, loss = 0.18460763\n",
      "Iteration 382, loss = 0.18411056\n",
      "Iteration 383, loss = 0.18356795\n",
      "Iteration 384, loss = 0.18337262\n",
      "Iteration 385, loss = 0.18397053\n",
      "Iteration 386, loss = 0.18433855\n",
      "Iteration 387, loss = 0.18351310\n",
      "Iteration 388, loss = 0.18373964\n",
      "Iteration 389, loss = 0.18311351\n",
      "Iteration 390, loss = 0.18313375\n",
      "Iteration 391, loss = 0.18256047\n",
      "Iteration 392, loss = 0.18274653\n",
      "Iteration 393, loss = 0.18283699\n",
      "Iteration 394, loss = 0.18238209\n",
      "Iteration 395, loss = 0.18275450\n",
      "Iteration 396, loss = 0.18274539\n",
      "Iteration 397, loss = 0.18319665\n",
      "Iteration 398, loss = 0.18170685\n",
      "Iteration 399, loss = 0.18195041\n",
      "Iteration 400, loss = 0.18217266\n",
      "Iteration 401, loss = 0.18186776\n",
      "Iteration 402, loss = 0.18116779\n",
      "Iteration 403, loss = 0.18162354\n",
      "Iteration 404, loss = 0.18112815\n",
      "Iteration 405, loss = 0.18123816\n",
      "Iteration 406, loss = 0.18085121\n",
      "Iteration 407, loss = 0.18147137\n",
      "Iteration 408, loss = 0.18154716\n",
      "Iteration 409, loss = 0.18085761\n",
      "Iteration 410, loss = 0.18075065\n",
      "Iteration 411, loss = 0.18022617\n",
      "Iteration 412, loss = 0.18020779\n",
      "Iteration 413, loss = 0.18040880\n",
      "Iteration 414, loss = 0.18010151\n",
      "Iteration 415, loss = 0.18002795\n",
      "Iteration 416, loss = 0.17968805\n",
      "Iteration 417, loss = 0.17972369\n",
      "Iteration 418, loss = 0.18019315\n",
      "Iteration 419, loss = 0.18014220\n",
      "Iteration 420, loss = 0.17938898\n",
      "Iteration 421, loss = 0.17957529\n",
      "Iteration 422, loss = 0.18000314\n",
      "Iteration 423, loss = 0.17981363\n",
      "Iteration 424, loss = 0.17985057\n",
      "Iteration 425, loss = 0.17953987\n",
      "Iteration 426, loss = 0.17954283\n",
      "Iteration 427, loss = 0.17947885\n",
      "Iteration 428, loss = 0.17990174\n",
      "Iteration 429, loss = 0.17932438\n",
      "Iteration 430, loss = 0.17906397\n",
      "Iteration 431, loss = 0.17844989\n",
      "Iteration 432, loss = 0.17903729\n",
      "Iteration 433, loss = 0.17920865\n",
      "Iteration 434, loss = 0.17821761\n",
      "Iteration 435, loss = 0.17894605\n",
      "Iteration 436, loss = 0.17878159\n",
      "Iteration 437, loss = 0.17994526\n",
      "Iteration 438, loss = 0.17932263\n",
      "Iteration 439, loss = 0.17812713\n",
      "Iteration 440, loss = 0.17860418\n",
      "Iteration 441, loss = 0.17817489\n",
      "Iteration 442, loss = 0.17851295\n",
      "Iteration 443, loss = 0.17815830\n",
      "Iteration 444, loss = 0.17805118\n",
      "Iteration 445, loss = 0.17789511\n",
      "Iteration 446, loss = 0.17787635\n",
      "Iteration 447, loss = 0.17799042\n",
      "Iteration 448, loss = 0.17850953\n",
      "Iteration 449, loss = 0.17850743\n",
      "Iteration 450, loss = 0.17769763\n",
      "Iteration 451, loss = 0.17746340\n",
      "Iteration 452, loss = 0.17729446\n",
      "Iteration 453, loss = 0.17734905\n",
      "Iteration 454, loss = 0.17772420\n",
      "Iteration 455, loss = 0.17747613\n",
      "Iteration 456, loss = 0.17752959\n",
      "Iteration 457, loss = 0.17765357\n",
      "Iteration 458, loss = 0.17724384\n",
      "Iteration 459, loss = 0.17695733\n",
      "Iteration 460, loss = 0.17700240\n",
      "Iteration 461, loss = 0.17784060\n",
      "Iteration 462, loss = 0.17688905\n",
      "Iteration 463, loss = 0.17686529\n",
      "Iteration 464, loss = 0.17687926\n",
      "Iteration 465, loss = 0.17636351\n",
      "Iteration 466, loss = 0.17601635\n",
      "Iteration 467, loss = 0.17671416\n",
      "Iteration 468, loss = 0.17733557\n",
      "Iteration 469, loss = 0.17804851\n",
      "Iteration 470, loss = 0.17624640\n",
      "Iteration 471, loss = 0.17658427\n",
      "Iteration 472, loss = 0.17636188\n",
      "Iteration 473, loss = 0.17634371\n",
      "Iteration 474, loss = 0.17613312\n",
      "Iteration 475, loss = 0.17653067\n",
      "Iteration 476, loss = 0.17581777\n",
      "Iteration 477, loss = 0.17610643\n",
      "Iteration 478, loss = 0.17628898\n",
      "Iteration 479, loss = 0.17601951\n",
      "Iteration 480, loss = 0.17592723\n",
      "Iteration 481, loss = 0.17705441\n",
      "Iteration 482, loss = 0.17583299\n",
      "Iteration 483, loss = 0.17614462\n",
      "Iteration 484, loss = 0.17572120\n",
      "Iteration 485, loss = 0.17550338\n",
      "Iteration 486, loss = 0.17564267\n",
      "Iteration 487, loss = 0.17522747\n",
      "Iteration 488, loss = 0.17530638\n",
      "Iteration 489, loss = 0.17540153\n",
      "Iteration 490, loss = 0.17550683\n",
      "Iteration 491, loss = 0.17584369\n",
      "Iteration 492, loss = 0.17525290\n",
      "Iteration 493, loss = 0.17550002\n",
      "Iteration 494, loss = 0.17494250\n",
      "Iteration 495, loss = 0.17465521\n",
      "Iteration 496, loss = 0.17609195\n",
      "Iteration 497, loss = 0.17564011\n",
      "Iteration 498, loss = 0.17518728\n",
      "Iteration 499, loss = 0.17476648\n",
      "Iteration 500, loss = 0.17498716\n",
      "Iteration 501, loss = 0.17530186\n",
      "Iteration 502, loss = 0.17490719\n",
      "Iteration 503, loss = 0.17483758\n",
      "Iteration 504, loss = 0.17440312\n",
      "Iteration 505, loss = 0.17471670\n",
      "Iteration 506, loss = 0.17468048\n",
      "Iteration 507, loss = 0.17510612\n",
      "Iteration 508, loss = 0.17469142\n",
      "Iteration 509, loss = 0.17432745\n",
      "Iteration 510, loss = 0.17459842\n",
      "Iteration 511, loss = 0.17429845\n",
      "Iteration 512, loss = 0.17484068\n",
      "Iteration 513, loss = 0.17465479\n",
      "Iteration 514, loss = 0.17420946\n",
      "Iteration 515, loss = 0.17410222\n",
      "Iteration 516, loss = 0.17425319\n",
      "Iteration 517, loss = 0.17426921\n",
      "Iteration 518, loss = 0.17463967\n",
      "Iteration 519, loss = 0.17563008\n",
      "Iteration 520, loss = 0.17671443\n",
      "Iteration 521, loss = 0.17460097\n",
      "Iteration 522, loss = 0.17503297\n",
      "Iteration 523, loss = 0.17401878\n",
      "Iteration 524, loss = 0.17422280\n",
      "Iteration 525, loss = 0.17428450\n",
      "Iteration 526, loss = 0.17401493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76797669\n",
      "Iteration 2, loss = 0.74224794\n",
      "Iteration 3, loss = 0.71992048\n",
      "Iteration 4, loss = 0.70275828\n",
      "Iteration 5, loss = 0.68831317\n",
      "Iteration 6, loss = 0.67429859\n",
      "Iteration 7, loss = 0.65932817\n",
      "Iteration 8, loss = 0.64228306\n",
      "Iteration 9, loss = 0.62301753\n",
      "Iteration 10, loss = 0.60324315\n",
      "Iteration 11, loss = 0.58383928\n",
      "Iteration 12, loss = 0.56610261\n",
      "Iteration 13, loss = 0.55031570\n",
      "Iteration 14, loss = 0.53683171\n",
      "Iteration 15, loss = 0.52533987\n",
      "Iteration 16, loss = 0.51501766\n",
      "Iteration 17, loss = 0.50585897\n",
      "Iteration 18, loss = 0.49797598\n",
      "Iteration 19, loss = 0.49056892\n",
      "Iteration 20, loss = 0.48391349\n",
      "Iteration 21, loss = 0.47809975\n",
      "Iteration 22, loss = 0.47227641\n",
      "Iteration 23, loss = 0.46691806\n",
      "Iteration 24, loss = 0.46177661\n",
      "Iteration 25, loss = 0.45704284\n",
      "Iteration 26, loss = 0.45220534\n",
      "Iteration 27, loss = 0.44769505\n",
      "Iteration 28, loss = 0.44358881\n",
      "Iteration 29, loss = 0.43958211\n",
      "Iteration 30, loss = 0.43543847\n",
      "Iteration 31, loss = 0.43151568\n",
      "Iteration 32, loss = 0.42789242\n",
      "Iteration 33, loss = 0.42421125\n",
      "Iteration 34, loss = 0.42103748\n",
      "Iteration 35, loss = 0.41730428\n",
      "Iteration 36, loss = 0.41379668\n",
      "Iteration 37, loss = 0.41051458\n",
      "Iteration 38, loss = 0.40729356\n",
      "Iteration 39, loss = 0.40418690\n",
      "Iteration 40, loss = 0.40092953\n",
      "Iteration 41, loss = 0.39849421\n",
      "Iteration 42, loss = 0.39493358\n",
      "Iteration 43, loss = 0.39220224\n",
      "Iteration 44, loss = 0.38951823\n",
      "Iteration 45, loss = 0.38675021\n",
      "Iteration 46, loss = 0.38400279\n",
      "Iteration 47, loss = 0.38204802\n",
      "Iteration 48, loss = 0.37908747\n",
      "Iteration 49, loss = 0.37669679\n",
      "Iteration 50, loss = 0.37427148\n",
      "Iteration 51, loss = 0.37197174\n",
      "Iteration 52, loss = 0.36953172\n",
      "Iteration 53, loss = 0.36723348\n",
      "Iteration 54, loss = 0.36517091\n",
      "Iteration 55, loss = 0.36303123\n",
      "Iteration 56, loss = 0.36101054\n",
      "Iteration 57, loss = 0.35856301\n",
      "Iteration 58, loss = 0.35664624\n",
      "Iteration 59, loss = 0.35448525\n",
      "Iteration 60, loss = 0.35248788\n",
      "Iteration 61, loss = 0.35020777\n",
      "Iteration 62, loss = 0.34817106\n",
      "Iteration 63, loss = 0.34624818\n",
      "Iteration 64, loss = 0.34402751\n",
      "Iteration 65, loss = 0.34204983\n",
      "Iteration 66, loss = 0.33994069\n",
      "Iteration 67, loss = 0.33828315\n",
      "Iteration 68, loss = 0.33609656\n",
      "Iteration 69, loss = 0.33430076\n",
      "Iteration 70, loss = 0.33252500\n",
      "Iteration 71, loss = 0.33091284\n",
      "Iteration 72, loss = 0.32906567\n",
      "Iteration 73, loss = 0.32710561\n",
      "Iteration 74, loss = 0.32520822\n",
      "Iteration 75, loss = 0.32334587\n",
      "Iteration 76, loss = 0.32174312\n",
      "Iteration 77, loss = 0.31985578\n",
      "Iteration 78, loss = 0.31790634\n",
      "Iteration 79, loss = 0.31650598\n",
      "Iteration 80, loss = 0.31459869\n",
      "Iteration 81, loss = 0.31331605\n",
      "Iteration 82, loss = 0.31148703\n",
      "Iteration 83, loss = 0.31017765\n",
      "Iteration 84, loss = 0.30841012\n",
      "Iteration 85, loss = 0.30690409\n",
      "Iteration 86, loss = 0.30544426\n",
      "Iteration 87, loss = 0.30404167\n",
      "Iteration 88, loss = 0.30246053\n",
      "Iteration 89, loss = 0.30143577\n",
      "Iteration 90, loss = 0.29973768\n",
      "Iteration 91, loss = 0.29843531\n",
      "Iteration 92, loss = 0.29685750\n",
      "Iteration 93, loss = 0.29558434\n",
      "Iteration 94, loss = 0.29408548\n",
      "Iteration 95, loss = 0.29325667\n",
      "Iteration 96, loss = 0.29158499\n",
      "Iteration 97, loss = 0.29024937\n",
      "Iteration 98, loss = 0.28926123\n",
      "Iteration 99, loss = 0.28786446\n",
      "Iteration 100, loss = 0.28679287\n",
      "Iteration 101, loss = 0.28516402\n",
      "Iteration 102, loss = 0.28398284\n",
      "Iteration 103, loss = 0.28288486\n",
      "Iteration 104, loss = 0.28174644\n",
      "Iteration 105, loss = 0.28053545\n",
      "Iteration 106, loss = 0.27941643\n",
      "Iteration 107, loss = 0.27855756\n",
      "Iteration 108, loss = 0.27721041\n",
      "Iteration 109, loss = 0.27648523\n",
      "Iteration 110, loss = 0.27523565\n",
      "Iteration 111, loss = 0.27437738\n",
      "Iteration 112, loss = 0.27301909\n",
      "Iteration 113, loss = 0.27221693\n",
      "Iteration 114, loss = 0.27103952\n",
      "Iteration 115, loss = 0.27002106\n",
      "Iteration 116, loss = 0.26909552\n",
      "Iteration 117, loss = 0.26817150\n",
      "Iteration 118, loss = 0.26713971\n",
      "Iteration 119, loss = 0.26626440\n",
      "Iteration 120, loss = 0.26517565\n",
      "Iteration 121, loss = 0.26475735\n",
      "Iteration 122, loss = 0.26400330\n",
      "Iteration 123, loss = 0.26301436\n",
      "Iteration 124, loss = 0.26209282\n",
      "Iteration 125, loss = 0.26109172\n",
      "Iteration 126, loss = 0.26047329\n",
      "Iteration 127, loss = 0.25940369\n",
      "Iteration 128, loss = 0.25860734\n",
      "Iteration 129, loss = 0.25725623\n",
      "Iteration 130, loss = 0.25625413\n",
      "Iteration 131, loss = 0.25565650\n",
      "Iteration 132, loss = 0.25506254\n",
      "Iteration 133, loss = 0.25425459\n",
      "Iteration 134, loss = 0.25314751\n",
      "Iteration 135, loss = 0.25275072\n",
      "Iteration 136, loss = 0.25172773\n",
      "Iteration 137, loss = 0.25160090\n",
      "Iteration 138, loss = 0.25009853\n",
      "Iteration 139, loss = 0.24973748\n",
      "Iteration 140, loss = 0.24890350\n",
      "Iteration 141, loss = 0.24797453\n",
      "Iteration 142, loss = 0.24675718\n",
      "Iteration 143, loss = 0.24645374\n",
      "Iteration 144, loss = 0.24576718\n",
      "Iteration 145, loss = 0.24481695\n",
      "Iteration 146, loss = 0.24415214\n",
      "Iteration 147, loss = 0.24337748\n",
      "Iteration 148, loss = 0.24289910\n",
      "Iteration 149, loss = 0.24213486\n",
      "Iteration 150, loss = 0.24155711\n",
      "Iteration 151, loss = 0.24074504\n",
      "Iteration 152, loss = 0.23993866\n",
      "Iteration 153, loss = 0.23937840\n",
      "Iteration 154, loss = 0.23881832\n",
      "Iteration 155, loss = 0.23876714\n",
      "Iteration 156, loss = 0.23746528\n",
      "Iteration 157, loss = 0.23703424\n",
      "Iteration 158, loss = 0.23649560\n",
      "Iteration 159, loss = 0.23567529\n",
      "Iteration 160, loss = 0.23523794\n",
      "Iteration 161, loss = 0.23433713\n",
      "Iteration 162, loss = 0.23386831\n",
      "Iteration 163, loss = 0.23364349\n",
      "Iteration 164, loss = 0.23293971\n",
      "Iteration 165, loss = 0.23253602\n",
      "Iteration 166, loss = 0.23205392\n",
      "Iteration 167, loss = 0.23106712\n",
      "Iteration 168, loss = 0.23064757\n",
      "Iteration 169, loss = 0.23008332\n",
      "Iteration 170, loss = 0.22925207\n",
      "Iteration 171, loss = 0.22884856\n",
      "Iteration 172, loss = 0.22797237\n",
      "Iteration 173, loss = 0.22770092\n",
      "Iteration 174, loss = 0.22715131\n",
      "Iteration 175, loss = 0.22626722\n",
      "Iteration 176, loss = 0.22591576\n",
      "Iteration 177, loss = 0.22516803\n",
      "Iteration 178, loss = 0.22462675\n",
      "Iteration 179, loss = 0.22391413\n",
      "Iteration 180, loss = 0.22377039\n",
      "Iteration 181, loss = 0.22339394\n",
      "Iteration 182, loss = 0.22306163\n",
      "Iteration 183, loss = 0.22228695\n",
      "Iteration 184, loss = 0.22160903\n",
      "Iteration 185, loss = 0.22113803\n",
      "Iteration 186, loss = 0.22061378\n",
      "Iteration 187, loss = 0.22035197\n",
      "Iteration 188, loss = 0.21989918\n",
      "Iteration 189, loss = 0.21920461\n",
      "Iteration 190, loss = 0.21874854\n",
      "Iteration 191, loss = 0.21884725\n",
      "Iteration 192, loss = 0.21791010\n",
      "Iteration 193, loss = 0.21761821\n",
      "Iteration 194, loss = 0.21737588\n",
      "Iteration 195, loss = 0.21675768\n",
      "Iteration 196, loss = 0.21670077\n",
      "Iteration 197, loss = 0.21627088\n",
      "Iteration 198, loss = 0.21557216\n",
      "Iteration 199, loss = 0.21544106\n",
      "Iteration 200, loss = 0.21497368\n",
      "Iteration 201, loss = 0.21491007\n",
      "Iteration 202, loss = 0.21427264\n",
      "Iteration 203, loss = 0.21437072\n",
      "Iteration 204, loss = 0.21406087\n",
      "Iteration 205, loss = 0.21318682\n",
      "Iteration 206, loss = 0.21337379\n",
      "Iteration 207, loss = 0.21262964\n",
      "Iteration 208, loss = 0.21241000\n",
      "Iteration 209, loss = 0.21196216\n",
      "Iteration 210, loss = 0.21167690\n",
      "Iteration 211, loss = 0.21138640\n",
      "Iteration 212, loss = 0.21074060\n",
      "Iteration 213, loss = 0.21068257\n",
      "Iteration 214, loss = 0.21037723\n",
      "Iteration 215, loss = 0.21097036\n",
      "Iteration 216, loss = 0.20988718\n",
      "Iteration 217, loss = 0.20962373\n",
      "Iteration 218, loss = 0.20910905\n",
      "Iteration 219, loss = 0.20929565\n",
      "Iteration 220, loss = 0.20848131\n",
      "Iteration 221, loss = 0.20827608\n",
      "Iteration 222, loss = 0.20809250\n",
      "Iteration 223, loss = 0.20781738\n",
      "Iteration 224, loss = 0.20744131\n",
      "Iteration 225, loss = 0.20723232\n",
      "Iteration 226, loss = 0.20715470\n",
      "Iteration 227, loss = 0.20670750\n",
      "Iteration 228, loss = 0.20658940\n",
      "Iteration 229, loss = 0.20606186\n",
      "Iteration 230, loss = 0.20617207\n",
      "Iteration 231, loss = 0.20605161\n",
      "Iteration 232, loss = 0.20561078\n",
      "Iteration 233, loss = 0.20503094\n",
      "Iteration 234, loss = 0.20503174\n",
      "Iteration 235, loss = 0.20512133\n",
      "Iteration 236, loss = 0.20462802\n",
      "Iteration 237, loss = 0.20429778\n",
      "Iteration 238, loss = 0.20411654\n",
      "Iteration 239, loss = 0.20414716\n",
      "Iteration 240, loss = 0.20359342\n",
      "Iteration 241, loss = 0.20341277\n",
      "Iteration 242, loss = 0.20362866\n",
      "Iteration 243, loss = 0.20281353\n",
      "Iteration 244, loss = 0.20273201\n",
      "Iteration 245, loss = 0.20254059\n",
      "Iteration 246, loss = 0.20250636\n",
      "Iteration 247, loss = 0.20219732\n",
      "Iteration 248, loss = 0.20212707\n",
      "Iteration 249, loss = 0.20171587\n",
      "Iteration 250, loss = 0.20148501\n",
      "Iteration 251, loss = 0.20129190\n",
      "Iteration 252, loss = 0.20086951\n",
      "Iteration 253, loss = 0.20125052\n",
      "Iteration 254, loss = 0.20136469\n",
      "Iteration 255, loss = 0.20102032\n",
      "Iteration 256, loss = 0.20058689\n",
      "Iteration 257, loss = 0.20050275\n",
      "Iteration 258, loss = 0.20034691\n",
      "Iteration 259, loss = 0.19990215\n",
      "Iteration 260, loss = 0.19968695\n",
      "Iteration 261, loss = 0.19944869\n",
      "Iteration 262, loss = 0.19933137\n",
      "Iteration 263, loss = 0.19965636\n",
      "Iteration 264, loss = 0.20098287\n",
      "Iteration 265, loss = 0.19878064\n",
      "Iteration 266, loss = 0.19885027\n",
      "Iteration 267, loss = 0.19850992\n",
      "Iteration 268, loss = 0.19845029\n",
      "Iteration 269, loss = 0.19816880\n",
      "Iteration 270, loss = 0.19892689\n",
      "Iteration 271, loss = 0.19775586\n",
      "Iteration 272, loss = 0.19809301\n",
      "Iteration 273, loss = 0.19782512\n",
      "Iteration 274, loss = 0.19763325\n",
      "Iteration 275, loss = 0.19721698\n",
      "Iteration 276, loss = 0.19735506\n",
      "Iteration 277, loss = 0.19704417\n",
      "Iteration 278, loss = 0.19731543\n",
      "Iteration 279, loss = 0.19701963\n",
      "Iteration 280, loss = 0.19680367\n",
      "Iteration 281, loss = 0.19654071\n",
      "Iteration 282, loss = 0.19626725\n",
      "Iteration 283, loss = 0.19646695\n",
      "Iteration 284, loss = 0.19583971\n",
      "Iteration 285, loss = 0.19616714\n",
      "Iteration 286, loss = 0.19572937\n",
      "Iteration 287, loss = 0.19568787\n",
      "Iteration 288, loss = 0.19551129\n",
      "Iteration 289, loss = 0.19554538\n",
      "Iteration 290, loss = 0.19536884\n",
      "Iteration 291, loss = 0.19569528\n",
      "Iteration 292, loss = 0.19546116\n",
      "Iteration 293, loss = 0.19504394\n",
      "Iteration 294, loss = 0.19539999\n",
      "Iteration 295, loss = 0.19482754\n",
      "Iteration 296, loss = 0.19488397\n",
      "Iteration 297, loss = 0.19461448\n",
      "Iteration 298, loss = 0.19406713\n",
      "Iteration 299, loss = 0.19410329\n",
      "Iteration 300, loss = 0.19439068\n",
      "Iteration 301, loss = 0.19412612\n",
      "Iteration 302, loss = 0.19479479\n",
      "Iteration 303, loss = 0.19377221\n",
      "Iteration 304, loss = 0.19366659\n",
      "Iteration 305, loss = 0.19444636\n",
      "Iteration 306, loss = 0.19380461\n",
      "Iteration 307, loss = 0.19294597\n",
      "Iteration 308, loss = 0.19334841\n",
      "Iteration 309, loss = 0.19305977\n",
      "Iteration 310, loss = 0.19255680\n",
      "Iteration 311, loss = 0.19291727\n",
      "Iteration 312, loss = 0.19261151\n",
      "Iteration 313, loss = 0.19347510\n",
      "Iteration 314, loss = 0.19259033\n",
      "Iteration 315, loss = 0.19240810\n",
      "Iteration 316, loss = 0.19224806\n",
      "Iteration 317, loss = 0.19214018\n",
      "Iteration 318, loss = 0.19183206\n",
      "Iteration 319, loss = 0.19299231\n",
      "Iteration 320, loss = 0.19332412\n",
      "Iteration 321, loss = 0.19205221\n",
      "Iteration 322, loss = 0.19184313\n",
      "Iteration 323, loss = 0.19175631\n",
      "Iteration 324, loss = 0.19140684\n",
      "Iteration 325, loss = 0.19138199\n",
      "Iteration 326, loss = 0.19113344\n",
      "Iteration 327, loss = 0.19141175\n",
      "Iteration 328, loss = 0.19105752\n",
      "Iteration 329, loss = 0.19132379\n",
      "Iteration 330, loss = 0.19124880\n",
      "Iteration 331, loss = 0.19086357\n",
      "Iteration 332, loss = 0.19095447\n",
      "Iteration 333, loss = 0.19101866\n",
      "Iteration 334, loss = 0.19100475\n",
      "Iteration 335, loss = 0.19039470\n",
      "Iteration 336, loss = 0.19038794\n",
      "Iteration 337, loss = 0.19021120\n",
      "Iteration 338, loss = 0.18992785\n",
      "Iteration 339, loss = 0.19003329\n",
      "Iteration 340, loss = 0.19004152\n",
      "Iteration 341, loss = 0.18998569\n",
      "Iteration 342, loss = 0.18982255\n",
      "Iteration 343, loss = 0.19006047\n",
      "Iteration 344, loss = 0.18965089\n",
      "Iteration 345, loss = 0.18947766\n",
      "Iteration 346, loss = 0.18939648\n",
      "Iteration 347, loss = 0.18956380\n",
      "Iteration 348, loss = 0.18963873\n",
      "Iteration 349, loss = 0.18927123\n",
      "Iteration 350, loss = 0.18906630\n",
      "Iteration 351, loss = 0.18902010\n",
      "Iteration 352, loss = 0.18916691\n",
      "Iteration 353, loss = 0.18885007\n",
      "Iteration 354, loss = 0.18876909\n",
      "Iteration 355, loss = 0.18911172\n",
      "Iteration 356, loss = 0.18875267\n",
      "Iteration 357, loss = 0.18903060\n",
      "Iteration 358, loss = 0.18833321\n",
      "Iteration 359, loss = 0.18883810\n",
      "Iteration 360, loss = 0.18862773\n",
      "Iteration 361, loss = 0.18873329\n",
      "Iteration 362, loss = 0.18827085\n",
      "Iteration 363, loss = 0.18838553\n",
      "Iteration 364, loss = 0.18824929\n",
      "Iteration 365, loss = 0.18803465\n",
      "Iteration 366, loss = 0.18791121\n",
      "Iteration 367, loss = 0.18803683\n",
      "Iteration 368, loss = 0.18810438\n",
      "Iteration 369, loss = 0.18821045\n",
      "Iteration 370, loss = 0.18777735\n",
      "Iteration 371, loss = 0.18780881\n",
      "Iteration 372, loss = 0.18761517\n",
      "Iteration 373, loss = 0.18739906\n",
      "Iteration 374, loss = 0.18761325\n",
      "Iteration 375, loss = 0.18719940\n",
      "Iteration 376, loss = 0.18734606\n",
      "Iteration 377, loss = 0.18779569\n",
      "Iteration 378, loss = 0.18751681\n",
      "Iteration 379, loss = 0.18730855\n",
      "Iteration 380, loss = 0.18691533\n",
      "Iteration 381, loss = 0.18710931\n",
      "Iteration 382, loss = 0.18717819\n",
      "Iteration 383, loss = 0.18675811\n",
      "Iteration 384, loss = 0.18672468\n",
      "Iteration 385, loss = 0.18700040\n",
      "Iteration 386, loss = 0.18709585\n",
      "Iteration 387, loss = 0.18684766\n",
      "Iteration 388, loss = 0.18706209\n",
      "Iteration 389, loss = 0.18651004\n",
      "Iteration 390, loss = 0.18694028\n",
      "Iteration 391, loss = 0.18647907\n",
      "Iteration 392, loss = 0.18666291\n",
      "Iteration 393, loss = 0.18662743\n",
      "Iteration 394, loss = 0.18651549\n",
      "Iteration 395, loss = 0.18657539\n",
      "Iteration 396, loss = 0.18619134\n",
      "Iteration 397, loss = 0.18640286\n",
      "Iteration 398, loss = 0.18595439\n",
      "Iteration 399, loss = 0.18792600\n",
      "Iteration 400, loss = 0.18559005\n",
      "Iteration 401, loss = 0.18607852\n",
      "Iteration 402, loss = 0.18563459\n",
      "Iteration 403, loss = 0.18594113\n",
      "Iteration 404, loss = 0.18578996\n",
      "Iteration 405, loss = 0.18604102\n",
      "Iteration 406, loss = 0.18631462\n",
      "Iteration 407, loss = 0.18555723\n",
      "Iteration 408, loss = 0.18613006\n",
      "Iteration 409, loss = 0.18542050\n",
      "Iteration 410, loss = 0.18574184\n",
      "Iteration 411, loss = 0.18548156\n",
      "Iteration 412, loss = 0.18580887\n",
      "Iteration 413, loss = 0.18511198\n",
      "Iteration 414, loss = 0.18541428\n",
      "Iteration 415, loss = 0.18525569\n",
      "Iteration 416, loss = 0.18593659\n",
      "Iteration 417, loss = 0.18516307\n",
      "Iteration 418, loss = 0.18506465\n",
      "Iteration 419, loss = 0.18526042\n",
      "Iteration 420, loss = 0.18507886\n",
      "Iteration 421, loss = 0.18490376\n",
      "Iteration 422, loss = 0.18529905\n",
      "Iteration 423, loss = 0.18480327\n",
      "Iteration 424, loss = 0.18513199\n",
      "Iteration 425, loss = 0.18572283\n",
      "Iteration 426, loss = 0.18491295\n",
      "Iteration 427, loss = 0.18492139\n",
      "Iteration 428, loss = 0.18557711\n",
      "Iteration 429, loss = 0.18467436\n",
      "Iteration 430, loss = 0.18576977\n",
      "Iteration 431, loss = 0.18495550\n",
      "Iteration 432, loss = 0.18456822\n",
      "Iteration 433, loss = 0.18442972\n",
      "Iteration 434, loss = 0.18460980\n",
      "Iteration 435, loss = 0.18485916\n",
      "Iteration 436, loss = 0.18468756\n",
      "Iteration 437, loss = 0.18459395\n",
      "Iteration 438, loss = 0.18434480\n",
      "Iteration 439, loss = 0.18426473\n",
      "Iteration 440, loss = 0.18399073\n",
      "Iteration 441, loss = 0.18403980\n",
      "Iteration 442, loss = 0.18405298\n",
      "Iteration 443, loss = 0.18467413\n",
      "Iteration 444, loss = 0.18419358\n",
      "Iteration 445, loss = 0.18438335\n",
      "Iteration 446, loss = 0.18413940\n",
      "Iteration 447, loss = 0.18410232\n",
      "Iteration 448, loss = 0.18382431\n",
      "Iteration 449, loss = 0.18423348\n",
      "Iteration 450, loss = 0.18353223\n",
      "Iteration 451, loss = 0.18379161\n",
      "Iteration 452, loss = 0.18352696\n",
      "Iteration 453, loss = 0.18389961\n",
      "Iteration 454, loss = 0.18509865\n",
      "Iteration 455, loss = 0.18343402\n",
      "Iteration 456, loss = 0.18401985\n",
      "Iteration 457, loss = 0.18378292\n",
      "Iteration 458, loss = 0.18352047\n",
      "Iteration 459, loss = 0.18303407\n",
      "Iteration 460, loss = 0.18327150\n",
      "Iteration 461, loss = 0.18338443\n",
      "Iteration 462, loss = 0.18304644\n",
      "Iteration 463, loss = 0.18294755\n",
      "Iteration 464, loss = 0.18304384\n",
      "Iteration 465, loss = 0.18326188\n",
      "Iteration 466, loss = 0.18399282\n",
      "Iteration 467, loss = 0.18284590\n",
      "Iteration 468, loss = 0.18331803\n",
      "Iteration 469, loss = 0.18307411\n",
      "Iteration 470, loss = 0.18370436\n",
      "Iteration 471, loss = 0.18237960\n",
      "Iteration 472, loss = 0.18234296\n",
      "Iteration 473, loss = 0.18277836\n",
      "Iteration 474, loss = 0.18245052\n",
      "Iteration 475, loss = 0.18239004\n",
      "Iteration 476, loss = 0.18224833\n",
      "Iteration 477, loss = 0.18224247\n",
      "Iteration 478, loss = 0.18217812\n",
      "Iteration 479, loss = 0.18301494\n",
      "Iteration 480, loss = 0.18233613\n",
      "Iteration 481, loss = 0.18204610\n",
      "Iteration 482, loss = 0.18201602\n",
      "Iteration 483, loss = 0.18199289\n",
      "Iteration 484, loss = 0.18179943\n",
      "Iteration 485, loss = 0.18191186\n",
      "Iteration 486, loss = 0.18220547\n",
      "Iteration 487, loss = 0.18156428\n",
      "Iteration 488, loss = 0.18198591\n",
      "Iteration 489, loss = 0.18168363\n",
      "Iteration 490, loss = 0.18200014\n",
      "Iteration 491, loss = 0.18150377\n",
      "Iteration 492, loss = 0.18138471\n",
      "Iteration 493, loss = 0.18180458\n",
      "Iteration 494, loss = 0.18141224\n",
      "Iteration 495, loss = 0.18142558\n",
      "Iteration 496, loss = 0.18098470\n",
      "Iteration 497, loss = 0.18140757\n",
      "Iteration 498, loss = 0.18049014\n",
      "Iteration 499, loss = 0.18059686\n",
      "Iteration 500, loss = 0.18088124\n",
      "Iteration 501, loss = 0.18087735\n",
      "Iteration 502, loss = 0.18046033\n",
      "Iteration 503, loss = 0.18021186\n",
      "Iteration 504, loss = 0.18025130\n",
      "Iteration 505, loss = 0.18032536\n",
      "Iteration 506, loss = 0.18020568\n",
      "Iteration 507, loss = 0.18006767\n",
      "Iteration 508, loss = 0.17996626\n",
      "Iteration 509, loss = 0.18000384\n",
      "Iteration 510, loss = 0.17956039\n",
      "Iteration 511, loss = 0.17960014\n",
      "Iteration 512, loss = 0.17948626\n",
      "Iteration 513, loss = 0.17961508\n",
      "Iteration 514, loss = 0.17941757\n",
      "Iteration 515, loss = 0.17998565\n",
      "Iteration 516, loss = 0.17978463\n",
      "Iteration 517, loss = 0.17942880\n",
      "Iteration 518, loss = 0.17891740\n",
      "Iteration 519, loss = 0.17910309\n",
      "Iteration 520, loss = 0.17858117\n",
      "Iteration 521, loss = 0.17852223\n",
      "Iteration 522, loss = 0.17942770\n",
      "Iteration 523, loss = 0.17796816\n",
      "Iteration 524, loss = 0.17778413\n",
      "Iteration 525, loss = 0.17758843\n",
      "Iteration 526, loss = 0.17778254\n",
      "Iteration 527, loss = 0.17743549\n",
      "Iteration 528, loss = 0.17736182\n",
      "Iteration 529, loss = 0.17754803\n",
      "Iteration 530, loss = 0.17770136\n",
      "Iteration 531, loss = 0.17746396\n",
      "Iteration 532, loss = 0.17795483\n",
      "Iteration 533, loss = 0.17734574\n",
      "Iteration 534, loss = 0.17731670\n",
      "Iteration 535, loss = 0.17694446\n",
      "Iteration 536, loss = 0.17678729\n",
      "Iteration 537, loss = 0.17629150\n",
      "Iteration 538, loss = 0.17694340\n",
      "Iteration 539, loss = 0.17663849\n",
      "Iteration 540, loss = 0.17657786\n",
      "Iteration 541, loss = 0.17670956\n",
      "Iteration 542, loss = 0.17628207\n",
      "Iteration 543, loss = 0.17640709\n",
      "Iteration 544, loss = 0.17617860\n",
      "Iteration 545, loss = 0.17647635\n",
      "Iteration 546, loss = 0.17600481\n",
      "Iteration 547, loss = 0.17616105\n",
      "Iteration 548, loss = 0.17638555\n",
      "Iteration 549, loss = 0.17599085\n",
      "Iteration 550, loss = 0.17609102\n",
      "Iteration 551, loss = 0.17576605\n",
      "Iteration 552, loss = 0.17577972\n",
      "Iteration 553, loss = 0.17554524\n",
      "Iteration 554, loss = 0.17569998\n",
      "Iteration 555, loss = 0.17629427\n",
      "Iteration 556, loss = 0.17566277\n",
      "Iteration 557, loss = 0.17549129\n",
      "Iteration 558, loss = 0.17595655\n",
      "Iteration 559, loss = 0.17543076\n",
      "Iteration 560, loss = 0.17530124\n",
      "Iteration 561, loss = 0.17583108\n",
      "Iteration 562, loss = 0.17498240\n",
      "Iteration 563, loss = 0.17535180\n",
      "Iteration 564, loss = 0.17552187\n",
      "Iteration 565, loss = 0.17510372\n",
      "Iteration 566, loss = 0.17499631\n",
      "Iteration 567, loss = 0.17521859\n",
      "Iteration 568, loss = 0.17583569\n",
      "Iteration 569, loss = 0.17520660\n",
      "Iteration 570, loss = 0.17542478\n",
      "Iteration 571, loss = 0.17466203\n",
      "Iteration 572, loss = 0.17491169\n",
      "Iteration 573, loss = 0.17527910\n",
      "Iteration 574, loss = 0.17480964\n",
      "Iteration 575, loss = 0.17480777\n",
      "Iteration 576, loss = 0.17512310\n",
      "Iteration 577, loss = 0.17464678\n",
      "Iteration 578, loss = 0.17455320\n",
      "Iteration 579, loss = 0.17508173\n",
      "Iteration 580, loss = 0.17418597\n",
      "Iteration 581, loss = 0.17507217\n",
      "Iteration 582, loss = 0.17521379\n",
      "Iteration 583, loss = 0.17425482\n",
      "Iteration 584, loss = 0.17450758\n",
      "Iteration 585, loss = 0.17408194\n",
      "Iteration 586, loss = 0.17386182\n",
      "Iteration 587, loss = 0.17375066\n",
      "Iteration 588, loss = 0.17385254\n",
      "Iteration 589, loss = 0.17387422\n",
      "Iteration 590, loss = 0.17393044\n",
      "Iteration 591, loss = 0.17395246\n",
      "Iteration 592, loss = 0.17384041\n",
      "Iteration 593, loss = 0.17451274\n",
      "Iteration 594, loss = 0.17427268\n",
      "Iteration 595, loss = 0.17498533\n",
      "Iteration 596, loss = 0.17332374\n",
      "Iteration 597, loss = 0.17357799\n",
      "Iteration 598, loss = 0.17324658\n",
      "Iteration 599, loss = 0.17342967\n",
      "Iteration 600, loss = 0.17391531\n",
      "Iteration 601, loss = 0.17347217\n",
      "Iteration 602, loss = 0.17328832\n",
      "Iteration 603, loss = 0.17330085\n",
      "Iteration 604, loss = 0.17308921\n",
      "Iteration 605, loss = 0.17354031\n",
      "Iteration 606, loss = 0.17308233\n",
      "Iteration 607, loss = 0.17318077\n",
      "Iteration 608, loss = 0.17371964\n",
      "Iteration 609, loss = 0.17317758\n",
      "Iteration 610, loss = 0.17311032\n",
      "Iteration 611, loss = 0.17335923\n",
      "Iteration 612, loss = 0.17311585\n",
      "Iteration 613, loss = 0.17292484\n",
      "Iteration 614, loss = 0.17267471\n",
      "Iteration 615, loss = 0.17302478\n",
      "Iteration 616, loss = 0.17284290\n",
      "Iteration 617, loss = 0.17277411\n",
      "Iteration 618, loss = 0.17305360\n",
      "Iteration 619, loss = 0.17330234\n",
      "Iteration 620, loss = 0.17274622\n",
      "Iteration 621, loss = 0.17300872\n",
      "Iteration 622, loss = 0.17273914\n",
      "Iteration 623, loss = 0.17266582\n",
      "Iteration 624, loss = 0.17302416\n",
      "Iteration 625, loss = 0.17238203\n",
      "Iteration 626, loss = 0.17248686\n",
      "Iteration 627, loss = 0.17259751\n",
      "Iteration 628, loss = 0.17287509\n",
      "Iteration 629, loss = 0.17292753\n",
      "Iteration 630, loss = 0.17261882\n",
      "Iteration 631, loss = 0.17258970\n",
      "Iteration 632, loss = 0.17311012\n",
      "Iteration 633, loss = 0.17215321\n",
      "Iteration 634, loss = 0.17246062\n",
      "Iteration 635, loss = 0.17221075\n",
      "Iteration 636, loss = 0.17298484\n",
      "Iteration 637, loss = 0.17204554\n",
      "Iteration 638, loss = 0.17247663\n",
      "Iteration 639, loss = 0.17203911\n",
      "Iteration 640, loss = 0.17220174\n",
      "Iteration 641, loss = 0.17235565\n",
      "Iteration 642, loss = 0.17243893\n",
      "Iteration 643, loss = 0.17329942\n",
      "Iteration 644, loss = 0.17200223\n",
      "Iteration 645, loss = 0.17233226\n",
      "Iteration 646, loss = 0.17248975\n",
      "Iteration 647, loss = 0.17167830\n",
      "Iteration 648, loss = 0.17278573\n",
      "Iteration 649, loss = 0.17181133\n",
      "Iteration 650, loss = 0.17210517\n",
      "Iteration 651, loss = 0.17236302\n",
      "Iteration 652, loss = 0.17179492\n",
      "Iteration 653, loss = 0.17182470\n",
      "Iteration 654, loss = 0.17165790\n",
      "Iteration 655, loss = 0.17179327\n",
      "Iteration 656, loss = 0.17171958\n",
      "Iteration 657, loss = 0.17159586\n",
      "Iteration 658, loss = 0.17186733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76826755\n",
      "Iteration 2, loss = 0.74224741\n",
      "Iteration 3, loss = 0.72032743\n",
      "Iteration 4, loss = 0.70328546\n",
      "Iteration 5, loss = 0.68913336\n",
      "Iteration 6, loss = 0.67561097\n",
      "Iteration 7, loss = 0.66155451\n",
      "Iteration 8, loss = 0.64556490\n",
      "Iteration 9, loss = 0.62723250\n",
      "Iteration 10, loss = 0.60771507\n",
      "Iteration 11, loss = 0.58808797\n",
      "Iteration 12, loss = 0.57034944\n",
      "Iteration 13, loss = 0.55459651\n",
      "Iteration 14, loss = 0.54137736\n",
      "Iteration 15, loss = 0.52958125\n",
      "Iteration 16, loss = 0.51955867\n",
      "Iteration 17, loss = 0.51009746\n",
      "Iteration 18, loss = 0.50204246\n",
      "Iteration 19, loss = 0.49461253\n",
      "Iteration 20, loss = 0.48804659\n",
      "Iteration 21, loss = 0.48168058\n",
      "Iteration 22, loss = 0.47581742\n",
      "Iteration 23, loss = 0.47028537\n",
      "Iteration 24, loss = 0.46500779\n",
      "Iteration 25, loss = 0.46012456\n",
      "Iteration 26, loss = 0.45550699\n",
      "Iteration 27, loss = 0.45125676\n",
      "Iteration 28, loss = 0.44693548\n",
      "Iteration 29, loss = 0.44287481\n",
      "Iteration 30, loss = 0.43869762\n",
      "Iteration 31, loss = 0.43496013\n",
      "Iteration 32, loss = 0.43114041\n",
      "Iteration 33, loss = 0.42769006\n",
      "Iteration 34, loss = 0.42389579\n",
      "Iteration 35, loss = 0.42054038\n",
      "Iteration 36, loss = 0.41702327\n",
      "Iteration 37, loss = 0.41369424\n",
      "Iteration 38, loss = 0.41065470\n",
      "Iteration 39, loss = 0.40768142\n",
      "Iteration 40, loss = 0.40447617\n",
      "Iteration 41, loss = 0.40174389\n",
      "Iteration 42, loss = 0.39883659\n",
      "Iteration 43, loss = 0.39604784\n",
      "Iteration 44, loss = 0.39342498\n",
      "Iteration 45, loss = 0.39081728\n",
      "Iteration 46, loss = 0.38820242\n",
      "Iteration 47, loss = 0.38582061\n",
      "Iteration 48, loss = 0.38318596\n",
      "Iteration 49, loss = 0.38080399\n",
      "Iteration 50, loss = 0.37825376\n",
      "Iteration 51, loss = 0.37593476\n",
      "Iteration 52, loss = 0.37367668\n",
      "Iteration 53, loss = 0.37147351\n",
      "Iteration 54, loss = 0.36934747\n",
      "Iteration 55, loss = 0.36731907\n",
      "Iteration 56, loss = 0.36523000\n",
      "Iteration 57, loss = 0.36304068\n",
      "Iteration 58, loss = 0.36088902\n",
      "Iteration 59, loss = 0.35884626\n",
      "Iteration 60, loss = 0.35680456\n",
      "Iteration 61, loss = 0.35468414\n",
      "Iteration 62, loss = 0.35257119\n",
      "Iteration 63, loss = 0.35087742\n",
      "Iteration 64, loss = 0.34872623\n",
      "Iteration 65, loss = 0.34692867\n",
      "Iteration 66, loss = 0.34512063\n",
      "Iteration 67, loss = 0.34301406\n",
      "Iteration 68, loss = 0.34094880\n",
      "Iteration 69, loss = 0.33911445\n",
      "Iteration 70, loss = 0.33733675\n",
      "Iteration 71, loss = 0.33557317\n",
      "Iteration 72, loss = 0.33357488\n",
      "Iteration 73, loss = 0.33188421\n",
      "Iteration 74, loss = 0.33005189\n",
      "Iteration 75, loss = 0.32873737\n",
      "Iteration 76, loss = 0.32687352\n",
      "Iteration 77, loss = 0.32540577\n",
      "Iteration 78, loss = 0.32347355\n",
      "Iteration 79, loss = 0.32207350\n",
      "Iteration 80, loss = 0.32030486\n",
      "Iteration 81, loss = 0.31904338\n",
      "Iteration 82, loss = 0.31752859\n",
      "Iteration 83, loss = 0.31603198\n",
      "Iteration 84, loss = 0.31432475\n",
      "Iteration 85, loss = 0.31313835\n",
      "Iteration 86, loss = 0.31153942\n",
      "Iteration 87, loss = 0.31031172\n",
      "Iteration 88, loss = 0.30876811\n",
      "Iteration 89, loss = 0.30725614\n",
      "Iteration 90, loss = 0.30586989\n",
      "Iteration 91, loss = 0.30455714\n",
      "Iteration 92, loss = 0.30336917\n",
      "Iteration 93, loss = 0.30206161\n",
      "Iteration 94, loss = 0.30071684\n",
      "Iteration 95, loss = 0.29948576\n",
      "Iteration 96, loss = 0.29851844\n",
      "Iteration 97, loss = 0.29707497\n",
      "Iteration 98, loss = 0.29588620\n",
      "Iteration 99, loss = 0.29482982\n",
      "Iteration 100, loss = 0.29348976\n",
      "Iteration 101, loss = 0.29246102\n",
      "Iteration 102, loss = 0.29116417\n",
      "Iteration 103, loss = 0.29019951\n",
      "Iteration 104, loss = 0.28907739\n",
      "Iteration 105, loss = 0.28818633\n",
      "Iteration 106, loss = 0.28700042\n",
      "Iteration 107, loss = 0.28611131\n",
      "Iteration 108, loss = 0.28496720\n",
      "Iteration 109, loss = 0.28412672\n",
      "Iteration 110, loss = 0.28320078\n",
      "Iteration 111, loss = 0.28219520\n",
      "Iteration 112, loss = 0.28105833\n",
      "Iteration 113, loss = 0.28013160\n",
      "Iteration 114, loss = 0.27904705\n",
      "Iteration 115, loss = 0.27809102\n",
      "Iteration 116, loss = 0.27737119\n",
      "Iteration 117, loss = 0.27602129\n",
      "Iteration 118, loss = 0.27529298\n",
      "Iteration 119, loss = 0.27445087\n",
      "Iteration 120, loss = 0.27396705\n",
      "Iteration 121, loss = 0.27315544\n",
      "Iteration 122, loss = 0.27158926\n",
      "Iteration 123, loss = 0.27084580\n",
      "Iteration 124, loss = 0.26979695\n",
      "Iteration 125, loss = 0.26980262\n",
      "Iteration 126, loss = 0.26883515\n",
      "Iteration 127, loss = 0.26779320\n",
      "Iteration 128, loss = 0.26669161\n",
      "Iteration 129, loss = 0.26573919\n",
      "Iteration 130, loss = 0.26534526\n",
      "Iteration 131, loss = 0.26430865\n",
      "Iteration 132, loss = 0.26367626\n",
      "Iteration 133, loss = 0.26289418\n",
      "Iteration 134, loss = 0.26235605\n",
      "Iteration 135, loss = 0.26143076\n",
      "Iteration 136, loss = 0.26072318\n",
      "Iteration 137, loss = 0.25989788\n",
      "Iteration 138, loss = 0.25934306\n",
      "Iteration 139, loss = 0.25867669\n",
      "Iteration 140, loss = 0.25824043\n",
      "Iteration 141, loss = 0.25740222\n",
      "Iteration 142, loss = 0.25703384\n",
      "Iteration 143, loss = 0.25596655\n",
      "Iteration 144, loss = 0.25536817\n",
      "Iteration 145, loss = 0.25493467\n",
      "Iteration 146, loss = 0.25416799\n",
      "Iteration 147, loss = 0.25355587\n",
      "Iteration 148, loss = 0.25306262\n",
      "Iteration 149, loss = 0.25306626\n",
      "Iteration 150, loss = 0.25175249\n",
      "Iteration 151, loss = 0.25126801\n",
      "Iteration 152, loss = 0.25067300\n",
      "Iteration 153, loss = 0.24990158\n",
      "Iteration 154, loss = 0.24948230\n",
      "Iteration 155, loss = 0.24892830\n",
      "Iteration 156, loss = 0.24811103\n",
      "Iteration 157, loss = 0.24818740\n",
      "Iteration 158, loss = 0.24709726\n",
      "Iteration 159, loss = 0.24685248\n",
      "Iteration 160, loss = 0.24614282\n",
      "Iteration 161, loss = 0.24531481\n",
      "Iteration 162, loss = 0.24503002\n",
      "Iteration 163, loss = 0.24512486\n",
      "Iteration 164, loss = 0.24353657\n",
      "Iteration 165, loss = 0.24366663\n",
      "Iteration 166, loss = 0.24287214\n",
      "Iteration 167, loss = 0.24199440\n",
      "Iteration 168, loss = 0.24150986\n",
      "Iteration 169, loss = 0.24140957\n",
      "Iteration 170, loss = 0.24064437\n",
      "Iteration 171, loss = 0.24062555\n",
      "Iteration 172, loss = 0.23967456\n",
      "Iteration 173, loss = 0.23844960\n",
      "Iteration 174, loss = 0.23810511\n",
      "Iteration 175, loss = 0.23748263\n",
      "Iteration 176, loss = 0.23683698\n",
      "Iteration 177, loss = 0.23679535\n",
      "Iteration 178, loss = 0.23628385\n",
      "Iteration 179, loss = 0.23562209\n",
      "Iteration 180, loss = 0.23526897\n",
      "Iteration 181, loss = 0.23428529\n",
      "Iteration 182, loss = 0.23392206\n",
      "Iteration 183, loss = 0.23399106\n",
      "Iteration 184, loss = 0.23333089\n",
      "Iteration 185, loss = 0.23248716\n",
      "Iteration 186, loss = 0.23206313\n",
      "Iteration 187, loss = 0.23160865\n",
      "Iteration 188, loss = 0.23140145\n",
      "Iteration 189, loss = 0.23053469\n",
      "Iteration 190, loss = 0.23026352\n",
      "Iteration 191, loss = 0.22960543\n",
      "Iteration 192, loss = 0.22923435\n",
      "Iteration 193, loss = 0.22836757\n",
      "Iteration 194, loss = 0.22831946\n",
      "Iteration 195, loss = 0.22779866\n",
      "Iteration 196, loss = 0.22772616\n",
      "Iteration 197, loss = 0.22708333\n",
      "Iteration 198, loss = 0.22668141\n",
      "Iteration 199, loss = 0.22634957\n",
      "Iteration 200, loss = 0.22599853\n",
      "Iteration 201, loss = 0.22542282\n",
      "Iteration 202, loss = 0.22524168\n",
      "Iteration 203, loss = 0.22458972\n",
      "Iteration 204, loss = 0.22427180\n",
      "Iteration 205, loss = 0.22387814\n",
      "Iteration 206, loss = 0.22397465\n",
      "Iteration 207, loss = 0.22328519\n",
      "Iteration 208, loss = 0.22336046\n",
      "Iteration 209, loss = 0.22260121\n",
      "Iteration 210, loss = 0.22203844\n",
      "Iteration 211, loss = 0.22182761\n",
      "Iteration 212, loss = 0.22143569\n",
      "Iteration 213, loss = 0.22143723\n",
      "Iteration 214, loss = 0.22194923\n",
      "Iteration 215, loss = 0.22041885\n",
      "Iteration 216, loss = 0.21999356\n",
      "Iteration 217, loss = 0.22005208\n",
      "Iteration 218, loss = 0.21937626\n",
      "Iteration 219, loss = 0.21946997\n",
      "Iteration 220, loss = 0.21907964\n",
      "Iteration 221, loss = 0.21841959\n",
      "Iteration 222, loss = 0.21851454\n",
      "Iteration 223, loss = 0.21789509\n",
      "Iteration 224, loss = 0.21813229\n",
      "Iteration 225, loss = 0.21699129\n",
      "Iteration 226, loss = 0.21706465\n",
      "Iteration 227, loss = 0.21646635\n",
      "Iteration 228, loss = 0.21714228\n",
      "Iteration 229, loss = 0.21600017\n",
      "Iteration 230, loss = 0.21613735\n",
      "Iteration 231, loss = 0.21560902\n",
      "Iteration 232, loss = 0.21531588\n",
      "Iteration 233, loss = 0.21500564\n",
      "Iteration 234, loss = 0.21518707\n",
      "Iteration 235, loss = 0.21442434\n",
      "Iteration 236, loss = 0.21503597\n",
      "Iteration 237, loss = 0.21428761\n",
      "Iteration 238, loss = 0.21387800\n",
      "Iteration 239, loss = 0.21380602\n",
      "Iteration 240, loss = 0.21335994\n",
      "Iteration 241, loss = 0.21296724\n",
      "Iteration 242, loss = 0.21293867\n",
      "Iteration 243, loss = 0.21288230\n",
      "Iteration 244, loss = 0.21259194\n",
      "Iteration 245, loss = 0.21213475\n",
      "Iteration 246, loss = 0.21202260\n",
      "Iteration 247, loss = 0.21201026\n",
      "Iteration 248, loss = 0.21170337\n",
      "Iteration 249, loss = 0.21166205\n",
      "Iteration 250, loss = 0.21136296\n",
      "Iteration 251, loss = 0.21113682\n",
      "Iteration 252, loss = 0.21099538\n",
      "Iteration 253, loss = 0.21051468\n",
      "Iteration 254, loss = 0.21092626\n",
      "Iteration 255, loss = 0.21085091\n",
      "Iteration 256, loss = 0.21012103\n",
      "Iteration 257, loss = 0.20980396\n",
      "Iteration 258, loss = 0.21095451\n",
      "Iteration 259, loss = 0.20980326\n",
      "Iteration 260, loss = 0.20924636\n",
      "Iteration 261, loss = 0.20955561\n",
      "Iteration 262, loss = 0.20870329\n",
      "Iteration 263, loss = 0.20933661\n",
      "Iteration 264, loss = 0.20910493\n",
      "Iteration 265, loss = 0.20856889\n",
      "Iteration 266, loss = 0.20788848\n",
      "Iteration 267, loss = 0.20798168\n",
      "Iteration 268, loss = 0.20781315\n",
      "Iteration 269, loss = 0.20765795\n",
      "Iteration 270, loss = 0.20725866\n",
      "Iteration 271, loss = 0.20763216\n",
      "Iteration 272, loss = 0.20806600\n",
      "Iteration 273, loss = 0.20676385\n",
      "Iteration 274, loss = 0.20647755\n",
      "Iteration 275, loss = 0.20647794\n",
      "Iteration 276, loss = 0.20626423\n",
      "Iteration 277, loss = 0.20627614\n",
      "Iteration 278, loss = 0.20613719\n",
      "Iteration 279, loss = 0.20554019\n",
      "Iteration 280, loss = 0.20543014\n",
      "Iteration 281, loss = 0.20547629\n",
      "Iteration 282, loss = 0.20530502\n",
      "Iteration 283, loss = 0.20533385\n",
      "Iteration 284, loss = 0.20487654\n",
      "Iteration 285, loss = 0.20559050\n",
      "Iteration 286, loss = 0.20596112\n",
      "Iteration 287, loss = 0.20476301\n",
      "Iteration 288, loss = 0.20454408\n",
      "Iteration 289, loss = 0.20441940\n",
      "Iteration 290, loss = 0.20345109\n",
      "Iteration 291, loss = 0.20357755\n",
      "Iteration 292, loss = 0.20328937\n",
      "Iteration 293, loss = 0.20327809\n",
      "Iteration 294, loss = 0.20272445\n",
      "Iteration 295, loss = 0.20277523\n",
      "Iteration 296, loss = 0.20223871\n",
      "Iteration 297, loss = 0.20236591\n",
      "Iteration 298, loss = 0.20216245\n",
      "Iteration 299, loss = 0.20208831\n",
      "Iteration 300, loss = 0.20184891\n",
      "Iteration 301, loss = 0.20165314\n",
      "Iteration 302, loss = 0.20162952\n",
      "Iteration 303, loss = 0.20133134\n",
      "Iteration 304, loss = 0.20123991\n",
      "Iteration 305, loss = 0.20099918\n",
      "Iteration 306, loss = 0.20071229\n",
      "Iteration 307, loss = 0.20062216\n",
      "Iteration 308, loss = 0.20089340\n",
      "Iteration 309, loss = 0.20053575\n",
      "Iteration 310, loss = 0.20105825\n",
      "Iteration 311, loss = 0.20031908\n",
      "Iteration 312, loss = 0.20026421\n",
      "Iteration 313, loss = 0.19979004\n",
      "Iteration 314, loss = 0.19974310\n",
      "Iteration 315, loss = 0.19946188\n",
      "Iteration 316, loss = 0.19936388\n",
      "Iteration 317, loss = 0.19978186\n",
      "Iteration 318, loss = 0.19970964\n",
      "Iteration 319, loss = 0.19907632\n",
      "Iteration 320, loss = 0.19908779\n",
      "Iteration 321, loss = 0.19886267\n",
      "Iteration 322, loss = 0.19830044\n",
      "Iteration 323, loss = 0.19902398\n",
      "Iteration 324, loss = 0.19821191\n",
      "Iteration 325, loss = 0.19789482\n",
      "Iteration 326, loss = 0.19763980\n",
      "Iteration 327, loss = 0.19732649\n",
      "Iteration 328, loss = 0.19699073\n",
      "Iteration 329, loss = 0.19711311\n",
      "Iteration 330, loss = 0.19743706\n",
      "Iteration 331, loss = 0.19689299\n",
      "Iteration 332, loss = 0.19649342\n",
      "Iteration 333, loss = 0.19665271\n",
      "Iteration 334, loss = 0.19622982\n",
      "Iteration 335, loss = 0.19629297\n",
      "Iteration 336, loss = 0.19653062\n",
      "Iteration 337, loss = 0.19629065\n",
      "Iteration 338, loss = 0.19590262\n",
      "Iteration 339, loss = 0.19554712\n",
      "Iteration 340, loss = 0.19590641\n",
      "Iteration 341, loss = 0.19564904\n",
      "Iteration 342, loss = 0.19529195\n",
      "Iteration 343, loss = 0.19550228\n",
      "Iteration 344, loss = 0.19495876\n",
      "Iteration 345, loss = 0.19508320\n",
      "Iteration 346, loss = 0.19496506\n",
      "Iteration 347, loss = 0.19516727\n",
      "Iteration 348, loss = 0.19480911\n",
      "Iteration 349, loss = 0.19492991\n",
      "Iteration 350, loss = 0.19504575\n",
      "Iteration 351, loss = 0.19429444\n",
      "Iteration 352, loss = 0.19497723\n",
      "Iteration 353, loss = 0.19491396\n",
      "Iteration 354, loss = 0.19401229\n",
      "Iteration 355, loss = 0.19470668\n",
      "Iteration 356, loss = 0.19394550\n",
      "Iteration 357, loss = 0.19390048\n",
      "Iteration 358, loss = 0.19380100\n",
      "Iteration 359, loss = 0.19574676\n",
      "Iteration 360, loss = 0.19373063\n",
      "Iteration 361, loss = 0.19463367\n",
      "Iteration 362, loss = 0.19302349\n",
      "Iteration 363, loss = 0.19322403\n",
      "Iteration 364, loss = 0.19308250\n",
      "Iteration 365, loss = 0.19333004\n",
      "Iteration 366, loss = 0.19306089\n",
      "Iteration 367, loss = 0.19304384\n",
      "Iteration 368, loss = 0.19315449\n",
      "Iteration 369, loss = 0.19311761\n",
      "Iteration 370, loss = 0.19270022\n",
      "Iteration 371, loss = 0.19262806\n",
      "Iteration 372, loss = 0.19252961\n",
      "Iteration 373, loss = 0.19227068\n",
      "Iteration 374, loss = 0.19229994\n",
      "Iteration 375, loss = 0.19237228\n",
      "Iteration 376, loss = 0.19241897\n",
      "Iteration 377, loss = 0.19228566\n",
      "Iteration 378, loss = 0.19207593\n",
      "Iteration 379, loss = 0.19193418\n",
      "Iteration 380, loss = 0.19230689\n",
      "Iteration 381, loss = 0.19182640\n",
      "Iteration 382, loss = 0.19165486\n",
      "Iteration 383, loss = 0.19173152\n",
      "Iteration 384, loss = 0.19178555\n",
      "Iteration 385, loss = 0.19156223\n",
      "Iteration 386, loss = 0.19116192\n",
      "Iteration 387, loss = 0.19125401\n",
      "Iteration 388, loss = 0.19109718\n",
      "Iteration 389, loss = 0.19160739\n",
      "Iteration 390, loss = 0.19104314\n",
      "Iteration 391, loss = 0.19125493\n",
      "Iteration 392, loss = 0.19087736\n",
      "Iteration 393, loss = 0.19075563\n",
      "Iteration 394, loss = 0.19068287\n",
      "Iteration 395, loss = 0.19100032\n",
      "Iteration 396, loss = 0.19031109\n",
      "Iteration 397, loss = 0.19052388\n",
      "Iteration 398, loss = 0.19062985\n",
      "Iteration 399, loss = 0.19030092\n",
      "Iteration 400, loss = 0.19054927\n",
      "Iteration 401, loss = 0.19002687\n",
      "Iteration 402, loss = 0.19013504\n",
      "Iteration 403, loss = 0.18974459\n",
      "Iteration 404, loss = 0.18995586\n",
      "Iteration 405, loss = 0.18978285\n",
      "Iteration 406, loss = 0.18946424\n",
      "Iteration 407, loss = 0.18970402\n",
      "Iteration 408, loss = 0.18957740\n",
      "Iteration 409, loss = 0.19046418\n",
      "Iteration 410, loss = 0.18945420\n",
      "Iteration 411, loss = 0.18946101\n",
      "Iteration 412, loss = 0.18918468\n",
      "Iteration 413, loss = 0.18947972\n",
      "Iteration 414, loss = 0.18859811\n",
      "Iteration 415, loss = 0.18854664\n",
      "Iteration 416, loss = 0.18842486\n",
      "Iteration 417, loss = 0.18862373\n",
      "Iteration 418, loss = 0.18909011\n",
      "Iteration 419, loss = 0.18833716\n",
      "Iteration 420, loss = 0.18799764\n",
      "Iteration 421, loss = 0.18787349\n",
      "Iteration 422, loss = 0.18816899\n",
      "Iteration 423, loss = 0.18852278\n",
      "Iteration 424, loss = 0.18785599\n",
      "Iteration 425, loss = 0.18851134\n",
      "Iteration 426, loss = 0.18787000\n",
      "Iteration 427, loss = 0.18756249\n",
      "Iteration 428, loss = 0.18774643\n",
      "Iteration 429, loss = 0.18737793\n",
      "Iteration 430, loss = 0.18770554\n",
      "Iteration 431, loss = 0.18744044\n",
      "Iteration 432, loss = 0.18715812\n",
      "Iteration 433, loss = 0.18703282\n",
      "Iteration 434, loss = 0.18670156\n",
      "Iteration 435, loss = 0.18669456\n",
      "Iteration 436, loss = 0.18657935\n",
      "Iteration 437, loss = 0.18672559\n",
      "Iteration 438, loss = 0.18694745\n",
      "Iteration 439, loss = 0.18701430\n",
      "Iteration 440, loss = 0.18704421\n",
      "Iteration 441, loss = 0.18667585\n",
      "Iteration 442, loss = 0.18651888\n",
      "Iteration 443, loss = 0.18684958\n",
      "Iteration 444, loss = 0.18595242\n",
      "Iteration 445, loss = 0.18602240\n",
      "Iteration 446, loss = 0.18599940\n",
      "Iteration 447, loss = 0.18591245\n",
      "Iteration 448, loss = 0.18617504\n",
      "Iteration 449, loss = 0.18547794\n",
      "Iteration 450, loss = 0.18553677\n",
      "Iteration 451, loss = 0.18551365\n",
      "Iteration 452, loss = 0.18527326\n",
      "Iteration 453, loss = 0.18532879\n",
      "Iteration 454, loss = 0.18527058\n",
      "Iteration 455, loss = 0.18497569\n",
      "Iteration 456, loss = 0.18542448\n",
      "Iteration 457, loss = 0.18487549\n",
      "Iteration 458, loss = 0.18468221\n",
      "Iteration 459, loss = 0.18452667\n",
      "Iteration 460, loss = 0.18515065\n",
      "Iteration 461, loss = 0.18411541\n",
      "Iteration 462, loss = 0.18430078\n",
      "Iteration 463, loss = 0.18401199\n",
      "Iteration 464, loss = 0.18392680\n",
      "Iteration 465, loss = 0.18410932\n",
      "Iteration 466, loss = 0.18397557\n",
      "Iteration 467, loss = 0.18358403\n",
      "Iteration 468, loss = 0.18360830\n",
      "Iteration 469, loss = 0.18381457\n",
      "Iteration 470, loss = 0.18373076\n",
      "Iteration 471, loss = 0.18356554\n",
      "Iteration 472, loss = 0.18352760\n",
      "Iteration 473, loss = 0.18332057\n",
      "Iteration 474, loss = 0.18316266\n",
      "Iteration 475, loss = 0.18305253\n",
      "Iteration 476, loss = 0.18325856\n",
      "Iteration 477, loss = 0.18299358\n",
      "Iteration 478, loss = 0.18274879\n",
      "Iteration 479, loss = 0.18317949\n",
      "Iteration 480, loss = 0.18249997\n",
      "Iteration 481, loss = 0.18267780\n",
      "Iteration 482, loss = 0.18282748\n",
      "Iteration 483, loss = 0.18255336\n",
      "Iteration 484, loss = 0.18255466\n",
      "Iteration 485, loss = 0.18227136\n",
      "Iteration 486, loss = 0.18232601\n",
      "Iteration 487, loss = 0.18301825\n",
      "Iteration 488, loss = 0.18220917\n",
      "Iteration 489, loss = 0.18246130\n",
      "Iteration 490, loss = 0.18232171\n",
      "Iteration 491, loss = 0.18260237\n",
      "Iteration 492, loss = 0.18186681\n",
      "Iteration 493, loss = 0.18253441\n",
      "Iteration 494, loss = 0.18225978\n",
      "Iteration 495, loss = 0.18167017\n",
      "Iteration 496, loss = 0.18239529\n",
      "Iteration 497, loss = 0.18235969\n",
      "Iteration 498, loss = 0.18218375\n",
      "Iteration 499, loss = 0.18221217\n",
      "Iteration 500, loss = 0.18152601\n",
      "Iteration 501, loss = 0.18224787\n",
      "Iteration 502, loss = 0.18153999\n",
      "Iteration 503, loss = 0.18144904\n",
      "Iteration 504, loss = 0.18162027\n",
      "Iteration 505, loss = 0.18154774\n",
      "Iteration 506, loss = 0.18183375\n",
      "Iteration 507, loss = 0.18136462\n",
      "Iteration 508, loss = 0.18227462\n",
      "Iteration 509, loss = 0.18135763\n",
      "Iteration 510, loss = 0.18089324\n",
      "Iteration 511, loss = 0.18094130\n",
      "Iteration 512, loss = 0.18102217\n",
      "Iteration 513, loss = 0.18094912\n",
      "Iteration 514, loss = 0.18100390\n",
      "Iteration 515, loss = 0.18129111\n",
      "Iteration 516, loss = 0.18081769\n",
      "Iteration 517, loss = 0.18107745\n",
      "Iteration 518, loss = 0.18064660\n",
      "Iteration 519, loss = 0.18082657\n",
      "Iteration 520, loss = 0.18145091\n",
      "Iteration 521, loss = 0.18062978\n",
      "Iteration 522, loss = 0.18085456\n",
      "Iteration 523, loss = 0.18145614\n",
      "Iteration 524, loss = 0.18036497\n",
      "Iteration 525, loss = 0.18119778\n",
      "Iteration 526, loss = 0.18023247\n",
      "Iteration 527, loss = 0.18016335\n",
      "Iteration 528, loss = 0.18053819\n",
      "Iteration 529, loss = 0.18084905\n",
      "Iteration 530, loss = 0.18045933\n",
      "Iteration 531, loss = 0.18007940\n",
      "Iteration 532, loss = 0.18009654\n",
      "Iteration 533, loss = 0.18057624\n",
      "Iteration 534, loss = 0.18020418\n",
      "Iteration 535, loss = 0.17983167\n",
      "Iteration 536, loss = 0.17992195\n",
      "Iteration 537, loss = 0.18103152\n",
      "Iteration 538, loss = 0.17962532\n",
      "Iteration 539, loss = 0.17985869\n",
      "Iteration 540, loss = 0.17977792\n",
      "Iteration 541, loss = 0.17985319\n",
      "Iteration 542, loss = 0.18014708\n",
      "Iteration 543, loss = 0.17989664\n",
      "Iteration 544, loss = 0.18026484\n",
      "Iteration 545, loss = 0.17961208\n",
      "Iteration 546, loss = 0.17986893\n",
      "Iteration 547, loss = 0.17949771\n",
      "Iteration 548, loss = 0.17943546\n",
      "Iteration 549, loss = 0.17950932\n",
      "Iteration 550, loss = 0.18002190\n",
      "Iteration 551, loss = 0.17973219\n",
      "Iteration 552, loss = 0.18005384\n",
      "Iteration 553, loss = 0.17970034\n",
      "Iteration 554, loss = 0.17932056\n",
      "Iteration 555, loss = 0.17967572\n",
      "Iteration 556, loss = 0.17952308\n",
      "Iteration 557, loss = 0.17971268\n",
      "Iteration 558, loss = 0.17957701\n",
      "Iteration 559, loss = 0.17902282\n",
      "Iteration 560, loss = 0.17941095\n",
      "Iteration 561, loss = 0.17911625\n",
      "Iteration 562, loss = 0.17970756\n",
      "Iteration 563, loss = 0.17997538\n",
      "Iteration 564, loss = 0.17931073\n",
      "Iteration 565, loss = 0.17890674\n",
      "Iteration 566, loss = 0.17926855\n",
      "Iteration 567, loss = 0.17914884\n",
      "Iteration 568, loss = 0.17872796\n",
      "Iteration 569, loss = 0.17938191\n",
      "Iteration 570, loss = 0.17978128\n",
      "Iteration 571, loss = 0.17904374\n",
      "Iteration 572, loss = 0.17884723\n",
      "Iteration 573, loss = 0.17854555\n",
      "Iteration 574, loss = 0.17885948\n",
      "Iteration 575, loss = 0.17963495\n",
      "Iteration 576, loss = 0.17919271\n",
      "Iteration 577, loss = 0.17897881\n",
      "Iteration 578, loss = 0.17855626\n",
      "Iteration 579, loss = 0.17909268\n",
      "Iteration 580, loss = 0.17918713\n",
      "Iteration 581, loss = 0.17928947\n",
      "Iteration 582, loss = 0.17880744\n",
      "Iteration 583, loss = 0.17879407\n",
      "Iteration 584, loss = 0.17890549\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76872785\n",
      "Iteration 2, loss = 0.74257964\n",
      "Iteration 3, loss = 0.72108458\n",
      "Iteration 4, loss = 0.70411379\n",
      "Iteration 5, loss = 0.69003823\n",
      "Iteration 6, loss = 0.67665727\n",
      "Iteration 7, loss = 0.66269995\n",
      "Iteration 8, loss = 0.64693769\n",
      "Iteration 9, loss = 0.62903477\n",
      "Iteration 10, loss = 0.61032345\n",
      "Iteration 11, loss = 0.59166578\n",
      "Iteration 12, loss = 0.57326477\n",
      "Iteration 13, loss = 0.55677505\n",
      "Iteration 14, loss = 0.54241212\n",
      "Iteration 15, loss = 0.52957792\n",
      "Iteration 16, loss = 0.51925979\n",
      "Iteration 17, loss = 0.51013559\n",
      "Iteration 18, loss = 0.50214493\n",
      "Iteration 19, loss = 0.49503189\n",
      "Iteration 20, loss = 0.48853393\n",
      "Iteration 21, loss = 0.48207355\n",
      "Iteration 22, loss = 0.47593066\n",
      "Iteration 23, loss = 0.47018715\n",
      "Iteration 24, loss = 0.46512090\n",
      "Iteration 25, loss = 0.46026622\n",
      "Iteration 26, loss = 0.45558096\n",
      "Iteration 27, loss = 0.45140292\n",
      "Iteration 28, loss = 0.44713262\n",
      "Iteration 29, loss = 0.44320112\n",
      "Iteration 30, loss = 0.43940825\n",
      "Iteration 31, loss = 0.43569580\n",
      "Iteration 32, loss = 0.43210202\n",
      "Iteration 33, loss = 0.42866480\n",
      "Iteration 34, loss = 0.42530570\n",
      "Iteration 35, loss = 0.42203652\n",
      "Iteration 36, loss = 0.41864947\n",
      "Iteration 37, loss = 0.41555655\n",
      "Iteration 38, loss = 0.41254480\n",
      "Iteration 39, loss = 0.40935893\n",
      "Iteration 40, loss = 0.40613067\n",
      "Iteration 41, loss = 0.40343770\n",
      "Iteration 42, loss = 0.40050473\n",
      "Iteration 43, loss = 0.39775080\n",
      "Iteration 44, loss = 0.39525457\n",
      "Iteration 45, loss = 0.39235003\n",
      "Iteration 46, loss = 0.38981911\n",
      "Iteration 47, loss = 0.38735574\n",
      "Iteration 48, loss = 0.38494989\n",
      "Iteration 49, loss = 0.38260756\n",
      "Iteration 50, loss = 0.38003657\n",
      "Iteration 51, loss = 0.37789032\n",
      "Iteration 52, loss = 0.37546388\n",
      "Iteration 53, loss = 0.37334401\n",
      "Iteration 54, loss = 0.37089060\n",
      "Iteration 55, loss = 0.36893225\n",
      "Iteration 56, loss = 0.36649523\n",
      "Iteration 57, loss = 0.36421908\n",
      "Iteration 58, loss = 0.36200183\n",
      "Iteration 59, loss = 0.36000654\n",
      "Iteration 60, loss = 0.35812079\n",
      "Iteration 61, loss = 0.35593509\n",
      "Iteration 62, loss = 0.35397289\n",
      "Iteration 63, loss = 0.35227008\n",
      "Iteration 64, loss = 0.35046195\n",
      "Iteration 65, loss = 0.34846605\n",
      "Iteration 66, loss = 0.34667941\n",
      "Iteration 67, loss = 0.34492565\n",
      "Iteration 68, loss = 0.34303761\n",
      "Iteration 69, loss = 0.34097466\n",
      "Iteration 70, loss = 0.33908887\n",
      "Iteration 71, loss = 0.33742035\n",
      "Iteration 72, loss = 0.33607846\n",
      "Iteration 73, loss = 0.33399836\n",
      "Iteration 74, loss = 0.33272225\n",
      "Iteration 75, loss = 0.33060762\n",
      "Iteration 76, loss = 0.32939379\n",
      "Iteration 77, loss = 0.32715008\n",
      "Iteration 78, loss = 0.32613149\n",
      "Iteration 79, loss = 0.32421868\n",
      "Iteration 80, loss = 0.32265354\n",
      "Iteration 81, loss = 0.32137326\n",
      "Iteration 82, loss = 0.31980117\n",
      "Iteration 83, loss = 0.31821078\n",
      "Iteration 84, loss = 0.31711792\n",
      "Iteration 85, loss = 0.31549155\n",
      "Iteration 86, loss = 0.31406507\n",
      "Iteration 87, loss = 0.31269348\n",
      "Iteration 88, loss = 0.31135608\n",
      "Iteration 89, loss = 0.30985065\n",
      "Iteration 90, loss = 0.30867495\n",
      "Iteration 91, loss = 0.30758666\n",
      "Iteration 92, loss = 0.30603598\n",
      "Iteration 93, loss = 0.30498901\n",
      "Iteration 94, loss = 0.30376526\n",
      "Iteration 95, loss = 0.30273343\n",
      "Iteration 96, loss = 0.30226406\n",
      "Iteration 97, loss = 0.29991873\n",
      "Iteration 98, loss = 0.29886869\n",
      "Iteration 99, loss = 0.29770858\n",
      "Iteration 100, loss = 0.29660360\n",
      "Iteration 101, loss = 0.29555714\n",
      "Iteration 102, loss = 0.29452308\n",
      "Iteration 103, loss = 0.29327693\n",
      "Iteration 104, loss = 0.29229448\n",
      "Iteration 105, loss = 0.29148354\n",
      "Iteration 106, loss = 0.28972070\n",
      "Iteration 107, loss = 0.28921933\n",
      "Iteration 108, loss = 0.28785989\n",
      "Iteration 109, loss = 0.28684644\n",
      "Iteration 110, loss = 0.28578860\n",
      "Iteration 111, loss = 0.28462355\n",
      "Iteration 112, loss = 0.28414745\n",
      "Iteration 113, loss = 0.28307372\n",
      "Iteration 114, loss = 0.28230477\n",
      "Iteration 115, loss = 0.28119839\n",
      "Iteration 116, loss = 0.28011365\n",
      "Iteration 117, loss = 0.27910831\n",
      "Iteration 118, loss = 0.27826776\n",
      "Iteration 119, loss = 0.27739503\n",
      "Iteration 120, loss = 0.27699546\n",
      "Iteration 121, loss = 0.27585436\n",
      "Iteration 122, loss = 0.27491344\n",
      "Iteration 123, loss = 0.27445606\n",
      "Iteration 124, loss = 0.27367196\n",
      "Iteration 125, loss = 0.27229294\n",
      "Iteration 126, loss = 0.27164205\n",
      "Iteration 127, loss = 0.27080490\n",
      "Iteration 128, loss = 0.27008218\n",
      "Iteration 129, loss = 0.26915075\n",
      "Iteration 130, loss = 0.26849124\n",
      "Iteration 131, loss = 0.26764519\n",
      "Iteration 132, loss = 0.26686753\n",
      "Iteration 133, loss = 0.26644790\n",
      "Iteration 134, loss = 0.26547321\n",
      "Iteration 135, loss = 0.26487696\n",
      "Iteration 136, loss = 0.26407051\n",
      "Iteration 137, loss = 0.26361725\n",
      "Iteration 138, loss = 0.26268509\n",
      "Iteration 139, loss = 0.26208111\n",
      "Iteration 140, loss = 0.26190241\n",
      "Iteration 141, loss = 0.26102691\n",
      "Iteration 142, loss = 0.26011695\n",
      "Iteration 143, loss = 0.25973252\n",
      "Iteration 144, loss = 0.25879480\n",
      "Iteration 145, loss = 0.25810172\n",
      "Iteration 146, loss = 0.25707315\n",
      "Iteration 147, loss = 0.25639677\n",
      "Iteration 148, loss = 0.25546206\n",
      "Iteration 149, loss = 0.25496796\n",
      "Iteration 150, loss = 0.25430247\n",
      "Iteration 151, loss = 0.25374057\n",
      "Iteration 152, loss = 0.25290985\n",
      "Iteration 153, loss = 0.25233459\n",
      "Iteration 154, loss = 0.25183798\n",
      "Iteration 155, loss = 0.25120021\n",
      "Iteration 156, loss = 0.25117542\n",
      "Iteration 157, loss = 0.25011046\n",
      "Iteration 158, loss = 0.24975925\n",
      "Iteration 159, loss = 0.24909923\n",
      "Iteration 160, loss = 0.24853864\n",
      "Iteration 161, loss = 0.24830684\n",
      "Iteration 162, loss = 0.24791183\n",
      "Iteration 163, loss = 0.24698516\n",
      "Iteration 164, loss = 0.24648059\n",
      "Iteration 165, loss = 0.24619678\n",
      "Iteration 166, loss = 0.24553250\n",
      "Iteration 167, loss = 0.24509721\n",
      "Iteration 168, loss = 0.24422423\n",
      "Iteration 169, loss = 0.24411803\n",
      "Iteration 170, loss = 0.24380512\n",
      "Iteration 171, loss = 0.24327381\n",
      "Iteration 172, loss = 0.24278695\n",
      "Iteration 173, loss = 0.24190613\n",
      "Iteration 174, loss = 0.24192805\n",
      "Iteration 175, loss = 0.24105951\n",
      "Iteration 176, loss = 0.24069154\n",
      "Iteration 177, loss = 0.23990082\n",
      "Iteration 178, loss = 0.23993061\n",
      "Iteration 179, loss = 0.23919981\n",
      "Iteration 180, loss = 0.23912787\n",
      "Iteration 181, loss = 0.23834076\n",
      "Iteration 182, loss = 0.23843831\n",
      "Iteration 183, loss = 0.23738393\n",
      "Iteration 184, loss = 0.23755152\n",
      "Iteration 185, loss = 0.23705062\n",
      "Iteration 186, loss = 0.23675019\n",
      "Iteration 187, loss = 0.23585959\n",
      "Iteration 188, loss = 0.23626490\n",
      "Iteration 189, loss = 0.23543905\n",
      "Iteration 190, loss = 0.23516183\n",
      "Iteration 191, loss = 0.23471821\n",
      "Iteration 192, loss = 0.23422381\n",
      "Iteration 193, loss = 0.23371312\n",
      "Iteration 194, loss = 0.23309848\n",
      "Iteration 195, loss = 0.23286088\n",
      "Iteration 196, loss = 0.23276761\n",
      "Iteration 197, loss = 0.23213880\n",
      "Iteration 198, loss = 0.23182270\n",
      "Iteration 199, loss = 0.23240938\n",
      "Iteration 200, loss = 0.23122090\n",
      "Iteration 201, loss = 0.23066551\n",
      "Iteration 202, loss = 0.23041424\n",
      "Iteration 203, loss = 0.22973596\n",
      "Iteration 204, loss = 0.22950897\n",
      "Iteration 205, loss = 0.22900400\n",
      "Iteration 206, loss = 0.22887216\n",
      "Iteration 207, loss = 0.22813380\n",
      "Iteration 208, loss = 0.22816651\n",
      "Iteration 209, loss = 0.22755623\n",
      "Iteration 210, loss = 0.22737272\n",
      "Iteration 211, loss = 0.22689111\n",
      "Iteration 212, loss = 0.22696538\n",
      "Iteration 213, loss = 0.22625755\n",
      "Iteration 214, loss = 0.22584879\n",
      "Iteration 215, loss = 0.22536569\n",
      "Iteration 216, loss = 0.22533219\n",
      "Iteration 217, loss = 0.22464260\n",
      "Iteration 218, loss = 0.22479790\n",
      "Iteration 219, loss = 0.22441624\n",
      "Iteration 220, loss = 0.22466206\n",
      "Iteration 221, loss = 0.22375922\n",
      "Iteration 222, loss = 0.22395465\n",
      "Iteration 223, loss = 0.22328510\n",
      "Iteration 224, loss = 0.22255864\n",
      "Iteration 225, loss = 0.22258087\n",
      "Iteration 226, loss = 0.22247087\n",
      "Iteration 227, loss = 0.22162561\n",
      "Iteration 228, loss = 0.22143160\n",
      "Iteration 229, loss = 0.22137858\n",
      "Iteration 230, loss = 0.22092207\n",
      "Iteration 231, loss = 0.22114949\n",
      "Iteration 232, loss = 0.22032327\n",
      "Iteration 233, loss = 0.22015085\n",
      "Iteration 234, loss = 0.22013440\n",
      "Iteration 235, loss = 0.21989568\n",
      "Iteration 236, loss = 0.21957023\n",
      "Iteration 237, loss = 0.21920984\n",
      "Iteration 238, loss = 0.21916934\n",
      "Iteration 239, loss = 0.21888265\n",
      "Iteration 240, loss = 0.21807771\n",
      "Iteration 241, loss = 0.21813288\n",
      "Iteration 242, loss = 0.21826957\n",
      "Iteration 243, loss = 0.21738348\n",
      "Iteration 244, loss = 0.21817631\n",
      "Iteration 245, loss = 0.21762353\n",
      "Iteration 246, loss = 0.21694406\n",
      "Iteration 247, loss = 0.21670427\n",
      "Iteration 248, loss = 0.21649974\n",
      "Iteration 249, loss = 0.21651921\n",
      "Iteration 250, loss = 0.21617615\n",
      "Iteration 251, loss = 0.21598325\n",
      "Iteration 252, loss = 0.21600228\n",
      "Iteration 253, loss = 0.21516678\n",
      "Iteration 254, loss = 0.21562125\n",
      "Iteration 255, loss = 0.21502782\n",
      "Iteration 256, loss = 0.21473075\n",
      "Iteration 257, loss = 0.21444680\n",
      "Iteration 258, loss = 0.21477218\n",
      "Iteration 259, loss = 0.21425603\n",
      "Iteration 260, loss = 0.21393283\n",
      "Iteration 261, loss = 0.21335519\n",
      "Iteration 262, loss = 0.21376971\n",
      "Iteration 263, loss = 0.21304623\n",
      "Iteration 264, loss = 0.21332888\n",
      "Iteration 265, loss = 0.21302083\n",
      "Iteration 266, loss = 0.21256439\n",
      "Iteration 267, loss = 0.21234932\n",
      "Iteration 268, loss = 0.21202819\n",
      "Iteration 269, loss = 0.21198625\n",
      "Iteration 270, loss = 0.21167390\n",
      "Iteration 271, loss = 0.21155596\n",
      "Iteration 272, loss = 0.21115581\n",
      "Iteration 273, loss = 0.21121705\n",
      "Iteration 274, loss = 0.21067469\n",
      "Iteration 275, loss = 0.21034524\n",
      "Iteration 276, loss = 0.21047004\n",
      "Iteration 277, loss = 0.21009312\n",
      "Iteration 278, loss = 0.20998712\n",
      "Iteration 279, loss = 0.20954915\n",
      "Iteration 280, loss = 0.20947738\n",
      "Iteration 281, loss = 0.20959842\n",
      "Iteration 282, loss = 0.20905205\n",
      "Iteration 283, loss = 0.20899929\n",
      "Iteration 284, loss = 0.20892164\n",
      "Iteration 285, loss = 0.20860984\n",
      "Iteration 286, loss = 0.20837601\n",
      "Iteration 287, loss = 0.20804542\n",
      "Iteration 288, loss = 0.20780018\n",
      "Iteration 289, loss = 0.20765423\n",
      "Iteration 290, loss = 0.20787662\n",
      "Iteration 291, loss = 0.20750415\n",
      "Iteration 292, loss = 0.20759352\n",
      "Iteration 293, loss = 0.20747611\n",
      "Iteration 294, loss = 0.20696707\n",
      "Iteration 295, loss = 0.20654771\n",
      "Iteration 296, loss = 0.20660694\n",
      "Iteration 297, loss = 0.20627811\n",
      "Iteration 298, loss = 0.20624534\n",
      "Iteration 299, loss = 0.20589220\n",
      "Iteration 300, loss = 0.20558042\n",
      "Iteration 301, loss = 0.20587150\n",
      "Iteration 302, loss = 0.20572348\n",
      "Iteration 303, loss = 0.20514829\n",
      "Iteration 304, loss = 0.20516264\n",
      "Iteration 305, loss = 0.20549508\n",
      "Iteration 306, loss = 0.20537004\n",
      "Iteration 307, loss = 0.20462423\n",
      "Iteration 308, loss = 0.20416453\n",
      "Iteration 309, loss = 0.20396259\n",
      "Iteration 310, loss = 0.20373571\n",
      "Iteration 311, loss = 0.20348021\n",
      "Iteration 312, loss = 0.20351740\n",
      "Iteration 313, loss = 0.20307341\n",
      "Iteration 314, loss = 0.20334733\n",
      "Iteration 315, loss = 0.20246090\n",
      "Iteration 316, loss = 0.20232602\n",
      "Iteration 317, loss = 0.20224829\n",
      "Iteration 318, loss = 0.20197605\n",
      "Iteration 319, loss = 0.20184570\n",
      "Iteration 320, loss = 0.20147441\n",
      "Iteration 321, loss = 0.20160745\n",
      "Iteration 322, loss = 0.20122602\n",
      "Iteration 323, loss = 0.20159512\n",
      "Iteration 324, loss = 0.20096337\n",
      "Iteration 325, loss = 0.20086201\n",
      "Iteration 326, loss = 0.20059523\n",
      "Iteration 327, loss = 0.20078521\n",
      "Iteration 328, loss = 0.20060645\n",
      "Iteration 329, loss = 0.20007868\n",
      "Iteration 330, loss = 0.19994295\n",
      "Iteration 331, loss = 0.19983646\n",
      "Iteration 332, loss = 0.19947127\n",
      "Iteration 333, loss = 0.19936644\n",
      "Iteration 334, loss = 0.19936587\n",
      "Iteration 335, loss = 0.19920222\n",
      "Iteration 336, loss = 0.19903675\n",
      "Iteration 337, loss = 0.19924992\n",
      "Iteration 338, loss = 0.19884331\n",
      "Iteration 339, loss = 0.19891438\n",
      "Iteration 340, loss = 0.19839657\n",
      "Iteration 341, loss = 0.19812944\n",
      "Iteration 342, loss = 0.19818760\n",
      "Iteration 343, loss = 0.19781756\n",
      "Iteration 344, loss = 0.19792703\n",
      "Iteration 345, loss = 0.19760840\n",
      "Iteration 346, loss = 0.19755141\n",
      "Iteration 347, loss = 0.19751730\n",
      "Iteration 348, loss = 0.19724025\n",
      "Iteration 349, loss = 0.19709355\n",
      "Iteration 350, loss = 0.19681352\n",
      "Iteration 351, loss = 0.19675837\n",
      "Iteration 352, loss = 0.19657205\n",
      "Iteration 353, loss = 0.19646510\n",
      "Iteration 354, loss = 0.19635549\n",
      "Iteration 355, loss = 0.19632443\n",
      "Iteration 356, loss = 0.19594055\n",
      "Iteration 357, loss = 0.19588802\n",
      "Iteration 358, loss = 0.19637274\n",
      "Iteration 359, loss = 0.19624611\n",
      "Iteration 360, loss = 0.19555338\n",
      "Iteration 361, loss = 0.19571496\n",
      "Iteration 362, loss = 0.19512150\n",
      "Iteration 363, loss = 0.19550197\n",
      "Iteration 364, loss = 0.19480916\n",
      "Iteration 365, loss = 0.19528163\n",
      "Iteration 366, loss = 0.19469079\n",
      "Iteration 367, loss = 0.19433015\n",
      "Iteration 368, loss = 0.19484440\n",
      "Iteration 369, loss = 0.19373733\n",
      "Iteration 370, loss = 0.19425434\n",
      "Iteration 371, loss = 0.19373986\n",
      "Iteration 372, loss = 0.19459622\n",
      "Iteration 373, loss = 0.19341342\n",
      "Iteration 374, loss = 0.19342868\n",
      "Iteration 375, loss = 0.19317334\n",
      "Iteration 376, loss = 0.19355216\n",
      "Iteration 377, loss = 0.19291357\n",
      "Iteration 378, loss = 0.19284478\n",
      "Iteration 379, loss = 0.19307794\n",
      "Iteration 380, loss = 0.19281256\n",
      "Iteration 381, loss = 0.19310709\n",
      "Iteration 382, loss = 0.19238942\n",
      "Iteration 383, loss = 0.19246025\n",
      "Iteration 384, loss = 0.19263142\n",
      "Iteration 385, loss = 0.19238782\n",
      "Iteration 386, loss = 0.19220436\n",
      "Iteration 387, loss = 0.19207467\n",
      "Iteration 388, loss = 0.19198010\n",
      "Iteration 389, loss = 0.19148942\n",
      "Iteration 390, loss = 0.19174905\n",
      "Iteration 391, loss = 0.19156859\n",
      "Iteration 392, loss = 0.19155575\n",
      "Iteration 393, loss = 0.19120597\n",
      "Iteration 394, loss = 0.19120697\n",
      "Iteration 395, loss = 0.19140907\n",
      "Iteration 396, loss = 0.19139693\n",
      "Iteration 397, loss = 0.19094170\n",
      "Iteration 398, loss = 0.19095105\n",
      "Iteration 399, loss = 0.19121215\n",
      "Iteration 400, loss = 0.19074124\n",
      "Iteration 401, loss = 0.19041396\n",
      "Iteration 402, loss = 0.19042513\n",
      "Iteration 403, loss = 0.19024309\n",
      "Iteration 404, loss = 0.19037192\n",
      "Iteration 405, loss = 0.19026105\n",
      "Iteration 406, loss = 0.18995468\n",
      "Iteration 407, loss = 0.19035246\n",
      "Iteration 408, loss = 0.19052102\n",
      "Iteration 409, loss = 0.18998835\n",
      "Iteration 410, loss = 0.18975538\n",
      "Iteration 411, loss = 0.18971034\n",
      "Iteration 412, loss = 0.18934536\n",
      "Iteration 413, loss = 0.18938276\n",
      "Iteration 414, loss = 0.18947169\n",
      "Iteration 415, loss = 0.18894879\n",
      "Iteration 416, loss = 0.18920009\n",
      "Iteration 417, loss = 0.18888136\n",
      "Iteration 418, loss = 0.18942370\n",
      "Iteration 419, loss = 0.18885509\n",
      "Iteration 420, loss = 0.18870549\n",
      "Iteration 421, loss = 0.18853531\n",
      "Iteration 422, loss = 0.18873606\n",
      "Iteration 423, loss = 0.18820485\n",
      "Iteration 424, loss = 0.18854032\n",
      "Iteration 425, loss = 0.18827881\n",
      "Iteration 426, loss = 0.18778455\n",
      "Iteration 427, loss = 0.18853656\n",
      "Iteration 428, loss = 0.18769568\n",
      "Iteration 429, loss = 0.18755033\n",
      "Iteration 430, loss = 0.18777247\n",
      "Iteration 431, loss = 0.18736124\n",
      "Iteration 432, loss = 0.18748022\n",
      "Iteration 433, loss = 0.18715219\n",
      "Iteration 434, loss = 0.18745719\n",
      "Iteration 435, loss = 0.18771233\n",
      "Iteration 436, loss = 0.18729345\n",
      "Iteration 437, loss = 0.18781132\n",
      "Iteration 438, loss = 0.18680459\n",
      "Iteration 439, loss = 0.18698867\n",
      "Iteration 440, loss = 0.18671731\n",
      "Iteration 441, loss = 0.18630437\n",
      "Iteration 442, loss = 0.18680438\n",
      "Iteration 443, loss = 0.18637709\n",
      "Iteration 444, loss = 0.18707409\n",
      "Iteration 445, loss = 0.18675069\n",
      "Iteration 446, loss = 0.18658894\n",
      "Iteration 447, loss = 0.18596634\n",
      "Iteration 448, loss = 0.18583629\n",
      "Iteration 449, loss = 0.18565978\n",
      "Iteration 450, loss = 0.18560032\n",
      "Iteration 451, loss = 0.18602614\n",
      "Iteration 452, loss = 0.18577179\n",
      "Iteration 453, loss = 0.18514579\n",
      "Iteration 454, loss = 0.18521119\n",
      "Iteration 455, loss = 0.18485532\n",
      "Iteration 456, loss = 0.18492608\n",
      "Iteration 457, loss = 0.18479528\n",
      "Iteration 458, loss = 0.18443570\n",
      "Iteration 459, loss = 0.18457628\n",
      "Iteration 460, loss = 0.18470270\n",
      "Iteration 461, loss = 0.18446933\n",
      "Iteration 462, loss = 0.18484813\n",
      "Iteration 463, loss = 0.18439584\n",
      "Iteration 464, loss = 0.18407501\n",
      "Iteration 465, loss = 0.18345983\n",
      "Iteration 466, loss = 0.18401059\n",
      "Iteration 467, loss = 0.18431898\n",
      "Iteration 468, loss = 0.18416979\n",
      "Iteration 469, loss = 0.18395702\n",
      "Iteration 470, loss = 0.18341719\n",
      "Iteration 471, loss = 0.18349964\n",
      "Iteration 472, loss = 0.18341179\n",
      "Iteration 473, loss = 0.18310140\n",
      "Iteration 474, loss = 0.18314905\n",
      "Iteration 475, loss = 0.18270689\n",
      "Iteration 476, loss = 0.18261837\n",
      "Iteration 477, loss = 0.18287632\n",
      "Iteration 478, loss = 0.18292775\n",
      "Iteration 479, loss = 0.18258162\n",
      "Iteration 480, loss = 0.18256328\n",
      "Iteration 481, loss = 0.18234925\n",
      "Iteration 482, loss = 0.18222863\n",
      "Iteration 483, loss = 0.18231057\n",
      "Iteration 484, loss = 0.18243305\n",
      "Iteration 485, loss = 0.18256640\n",
      "Iteration 486, loss = 0.18212084\n",
      "Iteration 487, loss = 0.18290134\n",
      "Iteration 488, loss = 0.18257571\n",
      "Iteration 489, loss = 0.18297642\n",
      "Iteration 490, loss = 0.18149062\n",
      "Iteration 491, loss = 0.18155968\n",
      "Iteration 492, loss = 0.18156251\n",
      "Iteration 493, loss = 0.18152161\n",
      "Iteration 494, loss = 0.18097108\n",
      "Iteration 495, loss = 0.18119351\n",
      "Iteration 496, loss = 0.18087960\n",
      "Iteration 497, loss = 0.18143386\n",
      "Iteration 498, loss = 0.18119496\n",
      "Iteration 499, loss = 0.18076759\n",
      "Iteration 500, loss = 0.18088401\n",
      "Iteration 501, loss = 0.18081748\n",
      "Iteration 502, loss = 0.18045114\n",
      "Iteration 503, loss = 0.18038303\n",
      "Iteration 504, loss = 0.18025908\n",
      "Iteration 505, loss = 0.18076666\n",
      "Iteration 506, loss = 0.18022929\n",
      "Iteration 507, loss = 0.17993918\n",
      "Iteration 508, loss = 0.18034825\n",
      "Iteration 509, loss = 0.17978425\n",
      "Iteration 510, loss = 0.17957254\n",
      "Iteration 511, loss = 0.17934133\n",
      "Iteration 512, loss = 0.17906563\n",
      "Iteration 513, loss = 0.17884579\n",
      "Iteration 514, loss = 0.17889324\n",
      "Iteration 515, loss = 0.17839455\n",
      "Iteration 516, loss = 0.17765347\n",
      "Iteration 517, loss = 0.17776847\n",
      "Iteration 518, loss = 0.17761545\n",
      "Iteration 519, loss = 0.17794118\n",
      "Iteration 520, loss = 0.17909735\n",
      "Iteration 521, loss = 0.17788973\n",
      "Iteration 522, loss = 0.17704082\n",
      "Iteration 523, loss = 0.17769011\n",
      "Iteration 524, loss = 0.17673217\n",
      "Iteration 525, loss = 0.17653151\n",
      "Iteration 526, loss = 0.17669321\n",
      "Iteration 527, loss = 0.17644429\n",
      "Iteration 528, loss = 0.17640166\n",
      "Iteration 529, loss = 0.17645917\n",
      "Iteration 530, loss = 0.17617910\n",
      "Iteration 531, loss = 0.17619092\n",
      "Iteration 532, loss = 0.17611487\n",
      "Iteration 533, loss = 0.17599411\n",
      "Iteration 534, loss = 0.17641256\n",
      "Iteration 535, loss = 0.17613418\n",
      "Iteration 536, loss = 0.17588373\n",
      "Iteration 537, loss = 0.17612031\n",
      "Iteration 538, loss = 0.17598952\n",
      "Iteration 539, loss = 0.17655388\n",
      "Iteration 540, loss = 0.17637172\n",
      "Iteration 541, loss = 0.17529019\n",
      "Iteration 542, loss = 0.17609297\n",
      "Iteration 543, loss = 0.17700267\n",
      "Iteration 544, loss = 0.17646275\n",
      "Iteration 545, loss = 0.17547630\n",
      "Iteration 546, loss = 0.17540863\n",
      "Iteration 547, loss = 0.17511615\n",
      "Iteration 548, loss = 0.17546399\n",
      "Iteration 549, loss = 0.17486157\n",
      "Iteration 550, loss = 0.17515393\n",
      "Iteration 551, loss = 0.17525092\n",
      "Iteration 552, loss = 0.17467379\n",
      "Iteration 553, loss = 0.17481036\n",
      "Iteration 554, loss = 0.17514384\n",
      "Iteration 555, loss = 0.17473891\n",
      "Iteration 556, loss = 0.17486101\n",
      "Iteration 557, loss = 0.17470402\n",
      "Iteration 558, loss = 0.17493522\n",
      "Iteration 559, loss = 0.17423367\n",
      "Iteration 560, loss = 0.17475711\n",
      "Iteration 561, loss = 0.17416559\n",
      "Iteration 562, loss = 0.17453765\n",
      "Iteration 563, loss = 0.17451926\n",
      "Iteration 564, loss = 0.17422772\n",
      "Iteration 565, loss = 0.17403067\n",
      "Iteration 566, loss = 0.17406326\n",
      "Iteration 567, loss = 0.17377537\n",
      "Iteration 568, loss = 0.17427483\n",
      "Iteration 569, loss = 0.17396633\n",
      "Iteration 570, loss = 0.17341300\n",
      "Iteration 571, loss = 0.17409798\n",
      "Iteration 572, loss = 0.17462677\n",
      "Iteration 573, loss = 0.17409512\n",
      "Iteration 574, loss = 0.17341753\n",
      "Iteration 575, loss = 0.17313427\n",
      "Iteration 576, loss = 0.17323332\n",
      "Iteration 577, loss = 0.17313298\n",
      "Iteration 578, loss = 0.17327694\n",
      "Iteration 579, loss = 0.17306953\n",
      "Iteration 580, loss = 0.17291903\n",
      "Iteration 581, loss = 0.17320704\n",
      "Iteration 582, loss = 0.17250814\n",
      "Iteration 583, loss = 0.17261845\n",
      "Iteration 584, loss = 0.17235537\n",
      "Iteration 585, loss = 0.17229296\n",
      "Iteration 586, loss = 0.17239552\n",
      "Iteration 587, loss = 0.17313073\n",
      "Iteration 588, loss = 0.17289108\n",
      "Iteration 589, loss = 0.17224572\n",
      "Iteration 590, loss = 0.17262375\n",
      "Iteration 591, loss = 0.17194546\n",
      "Iteration 592, loss = 0.17207343\n",
      "Iteration 593, loss = 0.17187119\n",
      "Iteration 594, loss = 0.17270901\n",
      "Iteration 595, loss = 0.17196831\n",
      "Iteration 596, loss = 0.17191968\n",
      "Iteration 597, loss = 0.17209465\n",
      "Iteration 598, loss = 0.17199854\n",
      "Iteration 599, loss = 0.17180095\n",
      "Iteration 600, loss = 0.17136101\n",
      "Iteration 601, loss = 0.17217963\n",
      "Iteration 602, loss = 0.17287095\n",
      "Iteration 603, loss = 0.17204929\n",
      "Iteration 604, loss = 0.17185625\n",
      "Iteration 605, loss = 0.17190999\n",
      "Iteration 606, loss = 0.17144876\n",
      "Iteration 607, loss = 0.17128774\n",
      "Iteration 608, loss = 0.17126404\n",
      "Iteration 609, loss = 0.17122431\n",
      "Iteration 610, loss = 0.17103447\n",
      "Iteration 611, loss = 0.17106245\n",
      "Iteration 612, loss = 0.17074082\n",
      "Iteration 613, loss = 0.17129904\n",
      "Iteration 614, loss = 0.17104792\n",
      "Iteration 615, loss = 0.17090670\n",
      "Iteration 616, loss = 0.17068488\n",
      "Iteration 617, loss = 0.17089417\n",
      "Iteration 618, loss = 0.17065120\n",
      "Iteration 619, loss = 0.17066251\n",
      "Iteration 620, loss = 0.17053986\n",
      "Iteration 621, loss = 0.17023748\n",
      "Iteration 622, loss = 0.17037542\n",
      "Iteration 623, loss = 0.17051697\n",
      "Iteration 624, loss = 0.17084274\n",
      "Iteration 625, loss = 0.17029131\n",
      "Iteration 626, loss = 0.17083487\n",
      "Iteration 627, loss = 0.17215275\n",
      "Iteration 628, loss = 0.17020707\n",
      "Iteration 629, loss = 0.17025792\n",
      "Iteration 630, loss = 0.17061137\n",
      "Iteration 631, loss = 0.17063464\n",
      "Iteration 632, loss = 0.17000739\n",
      "Iteration 633, loss = 0.17003630\n",
      "Iteration 634, loss = 0.17008889\n",
      "Iteration 635, loss = 0.17015733\n",
      "Iteration 636, loss = 0.16989829\n",
      "Iteration 637, loss = 0.16987682\n",
      "Iteration 638, loss = 0.16995543\n",
      "Iteration 639, loss = 0.16968928\n",
      "Iteration 640, loss = 0.16977215\n",
      "Iteration 641, loss = 0.17053590\n",
      "Iteration 642, loss = 0.17018404\n",
      "Iteration 643, loss = 0.16974652\n",
      "Iteration 644, loss = 0.16971100\n",
      "Iteration 645, loss = 0.17003841\n",
      "Iteration 646, loss = 0.16957257\n",
      "Iteration 647, loss = 0.16982613\n",
      "Iteration 648, loss = 0.17030229\n",
      "Iteration 649, loss = 0.16899648\n",
      "Iteration 650, loss = 0.16915544\n",
      "Iteration 651, loss = 0.16923057\n",
      "Iteration 652, loss = 0.16888264\n",
      "Iteration 653, loss = 0.16912659\n",
      "Iteration 654, loss = 0.16919355\n",
      "Iteration 655, loss = 0.16845608\n",
      "Iteration 656, loss = 0.16927955\n",
      "Iteration 657, loss = 0.16880697\n",
      "Iteration 658, loss = 0.16935098\n",
      "Iteration 659, loss = 0.16955267\n",
      "Iteration 660, loss = 0.16835211\n",
      "Iteration 661, loss = 0.16890637\n",
      "Iteration 662, loss = 0.16880412\n",
      "Iteration 663, loss = 0.16863288\n",
      "Iteration 664, loss = 0.16888920\n",
      "Iteration 665, loss = 0.16880406\n",
      "Iteration 666, loss = 0.16867517\n",
      "Iteration 667, loss = 0.16877575\n",
      "Iteration 668, loss = 0.16874047\n",
      "Iteration 669, loss = 0.16840836\n",
      "Iteration 670, loss = 0.16822065\n",
      "Iteration 671, loss = 0.16832623\n",
      "Iteration 672, loss = 0.16804376\n",
      "Iteration 673, loss = 0.16821829\n",
      "Iteration 674, loss = 0.16917902\n",
      "Iteration 675, loss = 0.16828664\n",
      "Iteration 676, loss = 0.16822138\n",
      "Iteration 677, loss = 0.16806243\n",
      "Iteration 678, loss = 0.16808678\n",
      "Iteration 679, loss = 0.16794620\n",
      "Iteration 680, loss = 0.16786381\n",
      "Iteration 681, loss = 0.16796903\n",
      "Iteration 682, loss = 0.16855015\n",
      "Iteration 683, loss = 0.16765799\n",
      "Iteration 684, loss = 0.16789007\n",
      "Iteration 685, loss = 0.16783890\n",
      "Iteration 686, loss = 0.16770799\n",
      "Iteration 687, loss = 0.16782293\n",
      "Iteration 688, loss = 0.16760815\n",
      "Iteration 689, loss = 0.16747087\n",
      "Iteration 690, loss = 0.16747738\n",
      "Iteration 691, loss = 0.16792723\n",
      "Iteration 692, loss = 0.16821392\n",
      "Iteration 693, loss = 0.16680573\n",
      "Iteration 694, loss = 0.16738577\n",
      "Iteration 695, loss = 0.16734490\n",
      "Iteration 696, loss = 0.16768012\n",
      "Iteration 697, loss = 0.16718878\n",
      "Iteration 698, loss = 0.16726210\n",
      "Iteration 699, loss = 0.16730058\n",
      "Iteration 700, loss = 0.16756810\n",
      "Iteration 701, loss = 0.16718503\n",
      "Iteration 702, loss = 0.16724793\n",
      "Iteration 703, loss = 0.16737854\n",
      "Iteration 704, loss = 0.16744717\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77017226\n",
      "Iteration 2, loss = 0.74408340\n",
      "Iteration 3, loss = 0.72274316\n",
      "Iteration 4, loss = 0.70571683\n",
      "Iteration 5, loss = 0.69129543\n",
      "Iteration 6, loss = 0.67829402\n",
      "Iteration 7, loss = 0.66549040\n",
      "Iteration 8, loss = 0.65004959\n",
      "Iteration 9, loss = 0.63217062\n",
      "Iteration 10, loss = 0.61258641\n",
      "Iteration 11, loss = 0.59312651\n",
      "Iteration 12, loss = 0.57445196\n",
      "Iteration 13, loss = 0.55819890\n",
      "Iteration 14, loss = 0.54450878\n",
      "Iteration 15, loss = 0.53217874\n",
      "Iteration 16, loss = 0.52206310\n",
      "Iteration 17, loss = 0.51259581\n",
      "Iteration 18, loss = 0.50383233\n",
      "Iteration 19, loss = 0.49614772\n",
      "Iteration 20, loss = 0.48905387\n",
      "Iteration 21, loss = 0.48257786\n",
      "Iteration 22, loss = 0.47652078\n",
      "Iteration 23, loss = 0.47124873\n",
      "Iteration 24, loss = 0.46598137\n",
      "Iteration 25, loss = 0.46130908\n",
      "Iteration 26, loss = 0.45647139\n",
      "Iteration 27, loss = 0.45196102\n",
      "Iteration 28, loss = 0.44772234\n",
      "Iteration 29, loss = 0.44357043\n",
      "Iteration 30, loss = 0.43964672\n",
      "Iteration 31, loss = 0.43584386\n",
      "Iteration 32, loss = 0.43209533\n",
      "Iteration 33, loss = 0.42850454\n",
      "Iteration 34, loss = 0.42488801\n",
      "Iteration 35, loss = 0.42149470\n",
      "Iteration 36, loss = 0.41805941\n",
      "Iteration 37, loss = 0.41466779\n",
      "Iteration 38, loss = 0.41150446\n",
      "Iteration 39, loss = 0.40833054\n",
      "Iteration 40, loss = 0.40514183\n",
      "Iteration 41, loss = 0.40218113\n",
      "Iteration 42, loss = 0.39923365\n",
      "Iteration 43, loss = 0.39624864\n",
      "Iteration 44, loss = 0.39354078\n",
      "Iteration 45, loss = 0.39076832\n",
      "Iteration 46, loss = 0.38802085\n",
      "Iteration 47, loss = 0.38568186\n",
      "Iteration 48, loss = 0.38300564\n",
      "Iteration 49, loss = 0.38037396\n",
      "Iteration 50, loss = 0.37799553\n",
      "Iteration 51, loss = 0.37552767\n",
      "Iteration 52, loss = 0.37318136\n",
      "Iteration 53, loss = 0.37093707\n",
      "Iteration 54, loss = 0.36846129\n",
      "Iteration 55, loss = 0.36616368\n",
      "Iteration 56, loss = 0.36382199\n",
      "Iteration 57, loss = 0.36172422\n",
      "Iteration 58, loss = 0.35955161\n",
      "Iteration 59, loss = 0.35757721\n",
      "Iteration 60, loss = 0.35548940\n",
      "Iteration 61, loss = 0.35341222\n",
      "Iteration 62, loss = 0.35155414\n",
      "Iteration 63, loss = 0.34952756\n",
      "Iteration 64, loss = 0.34776889\n",
      "Iteration 65, loss = 0.34535341\n",
      "Iteration 66, loss = 0.34335707\n",
      "Iteration 67, loss = 0.34126703\n",
      "Iteration 68, loss = 0.33944151\n",
      "Iteration 69, loss = 0.33734356\n",
      "Iteration 70, loss = 0.33575804\n",
      "Iteration 71, loss = 0.33444988\n",
      "Iteration 72, loss = 0.33204635\n",
      "Iteration 73, loss = 0.33040201\n",
      "Iteration 74, loss = 0.32867234\n",
      "Iteration 75, loss = 0.32678190\n",
      "Iteration 76, loss = 0.32532215\n",
      "Iteration 77, loss = 0.32346644\n",
      "Iteration 78, loss = 0.32210655\n",
      "Iteration 79, loss = 0.32053277\n",
      "Iteration 80, loss = 0.31905472\n",
      "Iteration 81, loss = 0.31736636\n",
      "Iteration 82, loss = 0.31556649\n",
      "Iteration 83, loss = 0.31482315\n",
      "Iteration 84, loss = 0.31374988\n",
      "Iteration 85, loss = 0.31141728\n",
      "Iteration 86, loss = 0.31019263\n",
      "Iteration 87, loss = 0.30850201\n",
      "Iteration 88, loss = 0.30731458\n",
      "Iteration 89, loss = 0.30612539\n",
      "Iteration 90, loss = 0.30443726\n",
      "Iteration 91, loss = 0.30303443\n",
      "Iteration 92, loss = 0.30175783\n",
      "Iteration 93, loss = 0.30050758\n",
      "Iteration 94, loss = 0.29939833\n",
      "Iteration 95, loss = 0.29787895\n",
      "Iteration 96, loss = 0.29691761\n",
      "Iteration 97, loss = 0.29551996\n",
      "Iteration 98, loss = 0.29459694\n",
      "Iteration 99, loss = 0.29319542\n",
      "Iteration 100, loss = 0.29209834\n",
      "Iteration 101, loss = 0.29110855\n",
      "Iteration 102, loss = 0.29012445\n",
      "Iteration 103, loss = 0.28924356\n",
      "Iteration 104, loss = 0.28768464\n",
      "Iteration 105, loss = 0.28719682\n",
      "Iteration 106, loss = 0.28557237\n",
      "Iteration 107, loss = 0.28478078\n",
      "Iteration 108, loss = 0.28373392\n",
      "Iteration 109, loss = 0.28265456\n",
      "Iteration 110, loss = 0.28173307\n",
      "Iteration 111, loss = 0.28106437\n",
      "Iteration 112, loss = 0.27979725\n",
      "Iteration 113, loss = 0.27879517\n",
      "Iteration 114, loss = 0.27780138\n",
      "Iteration 115, loss = 0.27663247\n",
      "Iteration 116, loss = 0.27574520\n",
      "Iteration 117, loss = 0.27521041\n",
      "Iteration 118, loss = 0.27438602\n",
      "Iteration 119, loss = 0.27308548\n",
      "Iteration 120, loss = 0.27240939\n",
      "Iteration 121, loss = 0.27153138\n",
      "Iteration 122, loss = 0.27065181\n",
      "Iteration 123, loss = 0.26979902\n",
      "Iteration 124, loss = 0.26913382\n",
      "Iteration 125, loss = 0.26823556\n",
      "Iteration 126, loss = 0.26778609\n",
      "Iteration 127, loss = 0.26667693\n",
      "Iteration 128, loss = 0.26609759\n",
      "Iteration 129, loss = 0.26512924\n",
      "Iteration 130, loss = 0.26429780\n",
      "Iteration 131, loss = 0.26388345\n",
      "Iteration 132, loss = 0.26287173\n",
      "Iteration 133, loss = 0.26191485\n",
      "Iteration 134, loss = 0.26147409\n",
      "Iteration 135, loss = 0.26044939\n",
      "Iteration 136, loss = 0.25982210\n",
      "Iteration 137, loss = 0.25926607\n",
      "Iteration 138, loss = 0.25838040\n",
      "Iteration 139, loss = 0.25747126\n",
      "Iteration 140, loss = 0.25724724\n",
      "Iteration 141, loss = 0.25600143\n",
      "Iteration 142, loss = 0.25519267\n",
      "Iteration 143, loss = 0.25456501\n",
      "Iteration 144, loss = 0.25398519\n",
      "Iteration 145, loss = 0.25275764\n",
      "Iteration 146, loss = 0.25231971\n",
      "Iteration 147, loss = 0.25177226\n",
      "Iteration 148, loss = 0.25090597\n",
      "Iteration 149, loss = 0.25032543\n",
      "Iteration 150, loss = 0.24968332\n",
      "Iteration 151, loss = 0.24942405\n",
      "Iteration 152, loss = 0.24847897\n",
      "Iteration 153, loss = 0.24771126\n",
      "Iteration 154, loss = 0.24744109\n",
      "Iteration 155, loss = 0.24707965\n",
      "Iteration 156, loss = 0.24618041\n",
      "Iteration 157, loss = 0.24569917\n",
      "Iteration 158, loss = 0.24478703\n",
      "Iteration 159, loss = 0.24437246\n",
      "Iteration 160, loss = 0.24371366\n",
      "Iteration 161, loss = 0.24298294\n",
      "Iteration 162, loss = 0.24258691\n",
      "Iteration 163, loss = 0.24200370\n",
      "Iteration 164, loss = 0.24154667\n",
      "Iteration 165, loss = 0.24062977\n",
      "Iteration 166, loss = 0.24002162\n",
      "Iteration 167, loss = 0.23971058\n",
      "Iteration 168, loss = 0.23891389\n",
      "Iteration 169, loss = 0.23884630\n",
      "Iteration 170, loss = 0.23795863\n",
      "Iteration 171, loss = 0.23742877\n",
      "Iteration 172, loss = 0.23636433\n",
      "Iteration 173, loss = 0.23641715\n",
      "Iteration 174, loss = 0.23549695\n",
      "Iteration 175, loss = 0.23521397\n",
      "Iteration 176, loss = 0.23488950\n",
      "Iteration 177, loss = 0.23403019\n",
      "Iteration 178, loss = 0.23356953\n",
      "Iteration 179, loss = 0.23307386\n",
      "Iteration 180, loss = 0.23260166\n",
      "Iteration 181, loss = 0.23221338\n",
      "Iteration 182, loss = 0.23169376\n",
      "Iteration 183, loss = 0.23087534\n",
      "Iteration 184, loss = 0.23058912\n",
      "Iteration 185, loss = 0.23001234\n",
      "Iteration 186, loss = 0.22964011\n",
      "Iteration 187, loss = 0.22918355\n",
      "Iteration 188, loss = 0.22908183\n",
      "Iteration 189, loss = 0.22822834\n",
      "Iteration 190, loss = 0.22772029\n",
      "Iteration 191, loss = 0.22732035\n",
      "Iteration 192, loss = 0.22681844\n",
      "Iteration 193, loss = 0.22652875\n",
      "Iteration 194, loss = 0.22639592\n",
      "Iteration 195, loss = 0.22569734\n",
      "Iteration 196, loss = 0.22536479\n",
      "Iteration 197, loss = 0.22511708\n",
      "Iteration 198, loss = 0.22476521\n",
      "Iteration 199, loss = 0.22466391\n",
      "Iteration 200, loss = 0.22358763\n",
      "Iteration 201, loss = 0.22320867\n",
      "Iteration 202, loss = 0.22304526\n",
      "Iteration 203, loss = 0.22221552\n",
      "Iteration 204, loss = 0.22257038\n",
      "Iteration 205, loss = 0.22240943\n",
      "Iteration 206, loss = 0.22157612\n",
      "Iteration 207, loss = 0.22159558\n",
      "Iteration 208, loss = 0.22047363\n",
      "Iteration 209, loss = 0.22024540\n",
      "Iteration 210, loss = 0.22010344\n",
      "Iteration 211, loss = 0.21942633\n",
      "Iteration 212, loss = 0.21969883\n",
      "Iteration 213, loss = 0.21863438\n",
      "Iteration 214, loss = 0.21846732\n",
      "Iteration 215, loss = 0.21848712\n",
      "Iteration 216, loss = 0.21763692\n",
      "Iteration 217, loss = 0.21783918\n",
      "Iteration 218, loss = 0.21731037\n",
      "Iteration 219, loss = 0.21676767\n",
      "Iteration 220, loss = 0.21634984\n",
      "Iteration 221, loss = 0.21604781\n",
      "Iteration 222, loss = 0.21570026\n",
      "Iteration 223, loss = 0.21546400\n",
      "Iteration 224, loss = 0.21545034\n",
      "Iteration 225, loss = 0.21592411\n",
      "Iteration 226, loss = 0.21508781\n",
      "Iteration 227, loss = 0.21369364\n",
      "Iteration 228, loss = 0.21428914\n",
      "Iteration 229, loss = 0.21341277\n",
      "Iteration 230, loss = 0.21353165\n",
      "Iteration 231, loss = 0.21301801\n",
      "Iteration 232, loss = 0.21274651\n",
      "Iteration 233, loss = 0.21223324\n",
      "Iteration 234, loss = 0.21198421\n",
      "Iteration 235, loss = 0.21157753\n",
      "Iteration 236, loss = 0.21147438\n",
      "Iteration 237, loss = 0.21148214\n",
      "Iteration 238, loss = 0.21169537\n",
      "Iteration 239, loss = 0.21112454\n",
      "Iteration 240, loss = 0.21071292\n",
      "Iteration 241, loss = 0.21044465\n",
      "Iteration 242, loss = 0.21038370\n",
      "Iteration 243, loss = 0.20942771\n",
      "Iteration 244, loss = 0.20926223\n",
      "Iteration 245, loss = 0.20955090\n",
      "Iteration 246, loss = 0.20927561\n",
      "Iteration 247, loss = 0.20868062\n",
      "Iteration 248, loss = 0.20823394\n",
      "Iteration 249, loss = 0.20813807\n",
      "Iteration 250, loss = 0.20811619\n",
      "Iteration 251, loss = 0.20749348\n",
      "Iteration 252, loss = 0.20716804\n",
      "Iteration 253, loss = 0.20758397\n",
      "Iteration 254, loss = 0.20683333\n",
      "Iteration 255, loss = 0.20615497\n",
      "Iteration 256, loss = 0.20606096\n",
      "Iteration 257, loss = 0.20598572\n",
      "Iteration 258, loss = 0.20549869\n",
      "Iteration 259, loss = 0.20531818\n",
      "Iteration 260, loss = 0.20504722\n",
      "Iteration 261, loss = 0.20503832\n",
      "Iteration 262, loss = 0.20475162\n",
      "Iteration 263, loss = 0.20448576\n",
      "Iteration 264, loss = 0.20439104\n",
      "Iteration 265, loss = 0.20402423\n",
      "Iteration 266, loss = 0.20439287\n",
      "Iteration 267, loss = 0.20390533\n",
      "Iteration 268, loss = 0.20429243\n",
      "Iteration 269, loss = 0.20337197\n",
      "Iteration 270, loss = 0.20321850\n",
      "Iteration 271, loss = 0.20318015\n",
      "Iteration 272, loss = 0.20274095\n",
      "Iteration 273, loss = 0.20261125\n",
      "Iteration 274, loss = 0.20212381\n",
      "Iteration 275, loss = 0.20211346\n",
      "Iteration 276, loss = 0.20057843\n",
      "Iteration 277, loss = 0.20046166\n",
      "Iteration 278, loss = 0.20003704\n",
      "Iteration 279, loss = 0.19982138\n",
      "Iteration 280, loss = 0.19953113\n",
      "Iteration 281, loss = 0.19928814\n",
      "Iteration 282, loss = 0.19874956\n",
      "Iteration 283, loss = 0.19904371\n",
      "Iteration 284, loss = 0.19905564\n",
      "Iteration 285, loss = 0.19859395\n",
      "Iteration 286, loss = 0.19848898\n",
      "Iteration 287, loss = 0.19819451\n",
      "Iteration 288, loss = 0.19775666\n",
      "Iteration 289, loss = 0.19778007\n",
      "Iteration 290, loss = 0.19776181\n",
      "Iteration 291, loss = 0.19749921\n",
      "Iteration 292, loss = 0.19748141\n",
      "Iteration 293, loss = 0.19771061\n",
      "Iteration 294, loss = 0.19797484\n",
      "Iteration 295, loss = 0.19666285\n",
      "Iteration 296, loss = 0.19689500\n",
      "Iteration 297, loss = 0.19685964\n",
      "Iteration 298, loss = 0.19688010\n",
      "Iteration 299, loss = 0.19697240\n",
      "Iteration 300, loss = 0.19683580\n",
      "Iteration 301, loss = 0.19559498\n",
      "Iteration 302, loss = 0.19610267\n",
      "Iteration 303, loss = 0.19579665\n",
      "Iteration 304, loss = 0.19565153\n",
      "Iteration 305, loss = 0.19528008\n",
      "Iteration 306, loss = 0.19584239\n",
      "Iteration 307, loss = 0.19547973\n",
      "Iteration 308, loss = 0.19529357\n",
      "Iteration 309, loss = 0.19473115\n",
      "Iteration 310, loss = 0.19481712\n",
      "Iteration 311, loss = 0.19430421\n",
      "Iteration 312, loss = 0.19462455\n",
      "Iteration 313, loss = 0.19417831\n",
      "Iteration 314, loss = 0.19431897\n",
      "Iteration 315, loss = 0.19439322\n",
      "Iteration 316, loss = 0.19374904\n",
      "Iteration 317, loss = 0.19357202\n",
      "Iteration 318, loss = 0.19404870\n",
      "Iteration 319, loss = 0.19354027\n",
      "Iteration 320, loss = 0.19348494\n",
      "Iteration 321, loss = 0.19333128\n",
      "Iteration 322, loss = 0.19305926\n",
      "Iteration 323, loss = 0.19291146\n",
      "Iteration 324, loss = 0.19267050\n",
      "Iteration 325, loss = 0.19237737\n",
      "Iteration 326, loss = 0.19292711\n",
      "Iteration 327, loss = 0.19187159\n",
      "Iteration 328, loss = 0.19207026\n",
      "Iteration 329, loss = 0.19178770\n",
      "Iteration 330, loss = 0.19193707\n",
      "Iteration 331, loss = 0.19171427\n",
      "Iteration 332, loss = 0.19142551\n",
      "Iteration 333, loss = 0.19182348\n",
      "Iteration 334, loss = 0.19112297\n",
      "Iteration 335, loss = 0.19105443\n",
      "Iteration 336, loss = 0.19088238\n",
      "Iteration 337, loss = 0.19101150\n",
      "Iteration 338, loss = 0.19042442\n",
      "Iteration 339, loss = 0.19048123\n",
      "Iteration 340, loss = 0.19025081\n",
      "Iteration 341, loss = 0.19033038\n",
      "Iteration 342, loss = 0.18996523\n",
      "Iteration 343, loss = 0.19005184\n",
      "Iteration 344, loss = 0.18977423\n",
      "Iteration 345, loss = 0.18974920\n",
      "Iteration 346, loss = 0.18918069\n",
      "Iteration 347, loss = 0.18953864\n",
      "Iteration 348, loss = 0.18935804\n",
      "Iteration 349, loss = 0.18943900\n",
      "Iteration 350, loss = 0.18919407\n",
      "Iteration 351, loss = 0.18897684\n",
      "Iteration 352, loss = 0.18887551\n",
      "Iteration 353, loss = 0.18907524\n",
      "Iteration 354, loss = 0.18868676\n",
      "Iteration 355, loss = 0.18844149\n",
      "Iteration 356, loss = 0.18853422\n",
      "Iteration 357, loss = 0.18859377\n",
      "Iteration 358, loss = 0.18819527\n",
      "Iteration 359, loss = 0.18852534\n",
      "Iteration 360, loss = 0.18798287\n",
      "Iteration 361, loss = 0.18838367\n",
      "Iteration 362, loss = 0.18805162\n",
      "Iteration 363, loss = 0.18778139\n",
      "Iteration 364, loss = 0.18776139\n",
      "Iteration 365, loss = 0.18785062\n",
      "Iteration 366, loss = 0.18733434\n",
      "Iteration 367, loss = 0.18754623\n",
      "Iteration 368, loss = 0.18729284\n",
      "Iteration 369, loss = 0.18765292\n",
      "Iteration 370, loss = 0.18725337\n",
      "Iteration 371, loss = 0.18740828\n",
      "Iteration 372, loss = 0.18709485\n",
      "Iteration 373, loss = 0.18667462\n",
      "Iteration 374, loss = 0.18654324\n",
      "Iteration 375, loss = 0.18669238\n",
      "Iteration 376, loss = 0.18669667\n",
      "Iteration 377, loss = 0.18689927\n",
      "Iteration 378, loss = 0.18567037\n",
      "Iteration 379, loss = 0.18539921\n",
      "Iteration 380, loss = 0.18544393\n",
      "Iteration 381, loss = 0.18557000\n",
      "Iteration 382, loss = 0.18510996\n",
      "Iteration 383, loss = 0.18466433\n",
      "Iteration 384, loss = 0.18454077\n",
      "Iteration 385, loss = 0.18405872\n",
      "Iteration 386, loss = 0.18397319\n",
      "Iteration 387, loss = 0.18359615\n",
      "Iteration 388, loss = 0.18394581\n",
      "Iteration 389, loss = 0.18404316\n",
      "Iteration 390, loss = 0.18358402\n",
      "Iteration 391, loss = 0.18388315\n",
      "Iteration 392, loss = 0.18310450\n",
      "Iteration 393, loss = 0.18294416\n",
      "Iteration 394, loss = 0.18415809\n",
      "Iteration 395, loss = 0.18475529\n",
      "Iteration 396, loss = 0.18281392\n",
      "Iteration 397, loss = 0.18269408\n",
      "Iteration 398, loss = 0.18216217\n",
      "Iteration 399, loss = 0.18244367\n",
      "Iteration 400, loss = 0.18220884\n",
      "Iteration 401, loss = 0.18201404\n",
      "Iteration 402, loss = 0.18227900\n",
      "Iteration 403, loss = 0.18200981\n",
      "Iteration 404, loss = 0.18169267\n",
      "Iteration 405, loss = 0.18179829\n",
      "Iteration 406, loss = 0.18221278\n",
      "Iteration 407, loss = 0.18256333\n",
      "Iteration 408, loss = 0.18235307\n",
      "Iteration 409, loss = 0.18135732\n",
      "Iteration 410, loss = 0.18244762\n",
      "Iteration 411, loss = 0.18177997\n",
      "Iteration 412, loss = 0.18124058\n",
      "Iteration 413, loss = 0.18151181\n",
      "Iteration 414, loss = 0.18093127\n",
      "Iteration 415, loss = 0.18086899\n",
      "Iteration 416, loss = 0.18066140\n",
      "Iteration 417, loss = 0.18046203\n",
      "Iteration 418, loss = 0.18070566\n",
      "Iteration 419, loss = 0.18044018\n",
      "Iteration 420, loss = 0.18044779\n",
      "Iteration 421, loss = 0.18089948\n",
      "Iteration 422, loss = 0.18053937\n",
      "Iteration 423, loss = 0.18023067\n",
      "Iteration 424, loss = 0.18041103\n",
      "Iteration 425, loss = 0.18030987\n",
      "Iteration 426, loss = 0.18021316\n",
      "Iteration 427, loss = 0.18071385\n",
      "Iteration 428, loss = 0.18107568\n",
      "Iteration 429, loss = 0.17982096\n",
      "Iteration 430, loss = 0.18028122\n",
      "Iteration 431, loss = 0.18016282\n",
      "Iteration 432, loss = 0.18021784\n",
      "Iteration 433, loss = 0.18028564\n",
      "Iteration 434, loss = 0.18086971\n",
      "Iteration 435, loss = 0.18055712\n",
      "Iteration 436, loss = 0.18056231\n",
      "Iteration 437, loss = 0.17966603\n",
      "Iteration 438, loss = 0.17926142\n",
      "Iteration 439, loss = 0.18016543\n",
      "Iteration 440, loss = 0.17951653\n",
      "Iteration 441, loss = 0.17967651\n",
      "Iteration 442, loss = 0.17969934\n",
      "Iteration 443, loss = 0.17906929\n",
      "Iteration 444, loss = 0.17905739\n",
      "Iteration 445, loss = 0.17903443\n",
      "Iteration 446, loss = 0.17935950\n",
      "Iteration 447, loss = 0.17918426\n",
      "Iteration 448, loss = 0.17916027\n",
      "Iteration 449, loss = 0.17925594\n",
      "Iteration 450, loss = 0.17920584\n",
      "Iteration 451, loss = 0.17887459\n",
      "Iteration 452, loss = 0.17888483\n",
      "Iteration 453, loss = 0.17903020\n",
      "Iteration 454, loss = 0.18039623\n",
      "Iteration 455, loss = 0.17935403\n",
      "Iteration 456, loss = 0.17991605\n",
      "Iteration 457, loss = 0.17929285\n",
      "Iteration 458, loss = 0.17865512\n",
      "Iteration 459, loss = 0.17961819\n",
      "Iteration 460, loss = 0.17937568\n",
      "Iteration 461, loss = 0.17948409\n",
      "Iteration 462, loss = 0.17829059\n",
      "Iteration 463, loss = 0.17862751\n",
      "Iteration 464, loss = 0.17816041\n",
      "Iteration 465, loss = 0.17791055\n",
      "Iteration 466, loss = 0.17787549\n",
      "Iteration 467, loss = 0.17769012\n",
      "Iteration 468, loss = 0.17830575\n",
      "Iteration 469, loss = 0.17842897\n",
      "Iteration 470, loss = 0.17792914\n",
      "Iteration 471, loss = 0.17777980\n",
      "Iteration 472, loss = 0.17748531\n",
      "Iteration 473, loss = 0.17754672\n",
      "Iteration 474, loss = 0.17962457\n",
      "Iteration 475, loss = 0.17809601\n",
      "Iteration 476, loss = 0.17810961\n",
      "Iteration 477, loss = 0.17749399\n",
      "Iteration 478, loss = 0.17801932\n",
      "Iteration 479, loss = 0.17771675\n",
      "Iteration 480, loss = 0.17732427\n",
      "Iteration 481, loss = 0.17711283\n",
      "Iteration 482, loss = 0.17715725\n",
      "Iteration 483, loss = 0.17734272\n",
      "Iteration 484, loss = 0.17710197\n",
      "Iteration 485, loss = 0.17724877\n",
      "Iteration 486, loss = 0.17718182\n",
      "Iteration 487, loss = 0.17663048\n",
      "Iteration 488, loss = 0.17682569\n",
      "Iteration 489, loss = 0.17654802\n",
      "Iteration 490, loss = 0.17683929\n",
      "Iteration 491, loss = 0.17681392\n",
      "Iteration 492, loss = 0.17722909\n",
      "Iteration 493, loss = 0.17765745\n",
      "Iteration 494, loss = 0.17694214\n",
      "Iteration 495, loss = 0.17676729\n",
      "Iteration 496, loss = 0.17670121\n",
      "Iteration 497, loss = 0.17686537\n",
      "Iteration 498, loss = 0.17660575\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76978837\n",
      "Iteration 2, loss = 0.74370979\n",
      "Iteration 3, loss = 0.72137126\n",
      "Iteration 4, loss = 0.70415680\n",
      "Iteration 5, loss = 0.68986975\n",
      "Iteration 6, loss = 0.67637304\n",
      "Iteration 7, loss = 0.66197651\n",
      "Iteration 8, loss = 0.64618967\n",
      "Iteration 9, loss = 0.62760334\n",
      "Iteration 10, loss = 0.60863957\n",
      "Iteration 11, loss = 0.58901831\n",
      "Iteration 12, loss = 0.57110550\n",
      "Iteration 13, loss = 0.55512185\n",
      "Iteration 14, loss = 0.54119623\n",
      "Iteration 15, loss = 0.52938822\n",
      "Iteration 16, loss = 0.51918436\n",
      "Iteration 17, loss = 0.50977960\n",
      "Iteration 18, loss = 0.50143807\n",
      "Iteration 19, loss = 0.49403524\n",
      "Iteration 20, loss = 0.48722927\n",
      "Iteration 21, loss = 0.48105244\n",
      "Iteration 22, loss = 0.47548984\n",
      "Iteration 23, loss = 0.47021954\n",
      "Iteration 24, loss = 0.46531701\n",
      "Iteration 25, loss = 0.46045475\n",
      "Iteration 26, loss = 0.45573788\n",
      "Iteration 27, loss = 0.45169056\n",
      "Iteration 28, loss = 0.44736802\n",
      "Iteration 29, loss = 0.44354559\n",
      "Iteration 30, loss = 0.43940378\n",
      "Iteration 31, loss = 0.43548793\n",
      "Iteration 32, loss = 0.43208100\n",
      "Iteration 33, loss = 0.42844909\n",
      "Iteration 34, loss = 0.42469492\n",
      "Iteration 35, loss = 0.42118156\n",
      "Iteration 36, loss = 0.41805741\n",
      "Iteration 37, loss = 0.41414542\n",
      "Iteration 38, loss = 0.41091062\n",
      "Iteration 39, loss = 0.40782837\n",
      "Iteration 40, loss = 0.40483686\n",
      "Iteration 41, loss = 0.40166920\n",
      "Iteration 42, loss = 0.39888703\n",
      "Iteration 43, loss = 0.39589397\n",
      "Iteration 44, loss = 0.39313513\n",
      "Iteration 45, loss = 0.39057324\n",
      "Iteration 46, loss = 0.38787712\n",
      "Iteration 47, loss = 0.38523902\n",
      "Iteration 48, loss = 0.38266971\n",
      "Iteration 49, loss = 0.38038256\n",
      "Iteration 50, loss = 0.37748707\n",
      "Iteration 51, loss = 0.37484837\n",
      "Iteration 52, loss = 0.37263046\n",
      "Iteration 53, loss = 0.37028327\n",
      "Iteration 54, loss = 0.36825368\n",
      "Iteration 55, loss = 0.36573933\n",
      "Iteration 56, loss = 0.36377817\n",
      "Iteration 57, loss = 0.36161081\n",
      "Iteration 58, loss = 0.35941329\n",
      "Iteration 59, loss = 0.35744003\n",
      "Iteration 60, loss = 0.35525264\n",
      "Iteration 61, loss = 0.35335592\n",
      "Iteration 62, loss = 0.35123840\n",
      "Iteration 63, loss = 0.34949795\n",
      "Iteration 64, loss = 0.34762795\n",
      "Iteration 65, loss = 0.34575382\n",
      "Iteration 66, loss = 0.34379487\n",
      "Iteration 67, loss = 0.34220881\n",
      "Iteration 68, loss = 0.34042280\n",
      "Iteration 69, loss = 0.33829274\n",
      "Iteration 70, loss = 0.33673153\n",
      "Iteration 71, loss = 0.33490922\n",
      "Iteration 72, loss = 0.33295118\n",
      "Iteration 73, loss = 0.33148297\n",
      "Iteration 74, loss = 0.32996200\n",
      "Iteration 75, loss = 0.32798121\n",
      "Iteration 76, loss = 0.32639460\n",
      "Iteration 77, loss = 0.32459460\n",
      "Iteration 78, loss = 0.32323640\n",
      "Iteration 79, loss = 0.32113926\n",
      "Iteration 80, loss = 0.31996439\n",
      "Iteration 81, loss = 0.31818386\n",
      "Iteration 82, loss = 0.31681357\n",
      "Iteration 83, loss = 0.31521758\n",
      "Iteration 84, loss = 0.31381939\n",
      "Iteration 85, loss = 0.31234763\n",
      "Iteration 86, loss = 0.31087154\n",
      "Iteration 87, loss = 0.30948531\n",
      "Iteration 88, loss = 0.30828874\n",
      "Iteration 89, loss = 0.30679103\n",
      "Iteration 90, loss = 0.30558108\n",
      "Iteration 91, loss = 0.30458828\n",
      "Iteration 92, loss = 0.30301207\n",
      "Iteration 93, loss = 0.30152988\n",
      "Iteration 94, loss = 0.30032913\n",
      "Iteration 95, loss = 0.29913108\n",
      "Iteration 96, loss = 0.29787522\n",
      "Iteration 97, loss = 0.29679495\n",
      "Iteration 98, loss = 0.29582708\n",
      "Iteration 99, loss = 0.29454746\n",
      "Iteration 100, loss = 0.29314952\n",
      "Iteration 101, loss = 0.29224805\n",
      "Iteration 102, loss = 0.29095979\n",
      "Iteration 103, loss = 0.29015623\n",
      "Iteration 104, loss = 0.28893044\n",
      "Iteration 105, loss = 0.28880096\n",
      "Iteration 106, loss = 0.28654946\n",
      "Iteration 107, loss = 0.28624604\n",
      "Iteration 108, loss = 0.28471608\n",
      "Iteration 109, loss = 0.28434444\n",
      "Iteration 110, loss = 0.28302440\n",
      "Iteration 111, loss = 0.28213025\n",
      "Iteration 112, loss = 0.28082637\n",
      "Iteration 113, loss = 0.27993177\n",
      "Iteration 114, loss = 0.27893912\n",
      "Iteration 115, loss = 0.27804224\n",
      "Iteration 116, loss = 0.27722878\n",
      "Iteration 117, loss = 0.27641552\n",
      "Iteration 118, loss = 0.27545775\n",
      "Iteration 119, loss = 0.27464785\n",
      "Iteration 120, loss = 0.27422590\n",
      "Iteration 121, loss = 0.27308961\n",
      "Iteration 122, loss = 0.27201767\n",
      "Iteration 123, loss = 0.27170311\n",
      "Iteration 124, loss = 0.27065920\n",
      "Iteration 125, loss = 0.26962714\n",
      "Iteration 126, loss = 0.26890423\n",
      "Iteration 127, loss = 0.26854177\n",
      "Iteration 128, loss = 0.26817404\n",
      "Iteration 129, loss = 0.26694136\n",
      "Iteration 130, loss = 0.26597874\n",
      "Iteration 131, loss = 0.26508391\n",
      "Iteration 132, loss = 0.26446116\n",
      "Iteration 133, loss = 0.26386072\n",
      "Iteration 134, loss = 0.26306270\n",
      "Iteration 135, loss = 0.26245676\n",
      "Iteration 136, loss = 0.26180209\n",
      "Iteration 137, loss = 0.26135883\n",
      "Iteration 138, loss = 0.26086060\n",
      "Iteration 139, loss = 0.25964254\n",
      "Iteration 140, loss = 0.25921165\n",
      "Iteration 141, loss = 0.25872737\n",
      "Iteration 142, loss = 0.25803472\n",
      "Iteration 143, loss = 0.25744535\n",
      "Iteration 144, loss = 0.25668597\n",
      "Iteration 145, loss = 0.25672778\n",
      "Iteration 146, loss = 0.25613528\n",
      "Iteration 147, loss = 0.25486169\n",
      "Iteration 148, loss = 0.25453580\n",
      "Iteration 149, loss = 0.25383834\n",
      "Iteration 150, loss = 0.25331532\n",
      "Iteration 151, loss = 0.25301295\n",
      "Iteration 152, loss = 0.25290050\n",
      "Iteration 153, loss = 0.25204495\n",
      "Iteration 154, loss = 0.25242896\n",
      "Iteration 155, loss = 0.25058314\n",
      "Iteration 156, loss = 0.25020374\n",
      "Iteration 157, loss = 0.24989636\n",
      "Iteration 158, loss = 0.24899109\n",
      "Iteration 159, loss = 0.24890200\n",
      "Iteration 160, loss = 0.24813862\n",
      "Iteration 161, loss = 0.24732283\n",
      "Iteration 162, loss = 0.24694101\n",
      "Iteration 163, loss = 0.24630611\n",
      "Iteration 164, loss = 0.24585486\n",
      "Iteration 165, loss = 0.24571567\n",
      "Iteration 166, loss = 0.24496495\n",
      "Iteration 167, loss = 0.24471593\n",
      "Iteration 168, loss = 0.24376514\n",
      "Iteration 169, loss = 0.24361709\n",
      "Iteration 170, loss = 0.24367122\n",
      "Iteration 171, loss = 0.24254269\n",
      "Iteration 172, loss = 0.24188431\n",
      "Iteration 173, loss = 0.24120456\n",
      "Iteration 174, loss = 0.24086252\n",
      "Iteration 175, loss = 0.24051300\n",
      "Iteration 176, loss = 0.23991307\n",
      "Iteration 177, loss = 0.23950844\n",
      "Iteration 178, loss = 0.23898267\n",
      "Iteration 179, loss = 0.23842671\n",
      "Iteration 180, loss = 0.23772146\n",
      "Iteration 181, loss = 0.23728782\n",
      "Iteration 182, loss = 0.23701090\n",
      "Iteration 183, loss = 0.23651607\n",
      "Iteration 184, loss = 0.23624182\n",
      "Iteration 185, loss = 0.23549559\n",
      "Iteration 186, loss = 0.23557539\n",
      "Iteration 187, loss = 0.23549181\n",
      "Iteration 188, loss = 0.23432242\n",
      "Iteration 189, loss = 0.23414600\n",
      "Iteration 190, loss = 0.23338570\n",
      "Iteration 191, loss = 0.23357584\n",
      "Iteration 192, loss = 0.23341914\n",
      "Iteration 193, loss = 0.23255905\n",
      "Iteration 194, loss = 0.23184046\n",
      "Iteration 195, loss = 0.23134312\n",
      "Iteration 196, loss = 0.23131143\n",
      "Iteration 197, loss = 0.23085790\n",
      "Iteration 198, loss = 0.23002133\n",
      "Iteration 199, loss = 0.23001319\n",
      "Iteration 200, loss = 0.22969858\n",
      "Iteration 201, loss = 0.22890575\n",
      "Iteration 202, loss = 0.22825737\n",
      "Iteration 203, loss = 0.22824443\n",
      "Iteration 204, loss = 0.22755793\n",
      "Iteration 205, loss = 0.22773583\n",
      "Iteration 206, loss = 0.22690813\n",
      "Iteration 207, loss = 0.22677628\n",
      "Iteration 208, loss = 0.22636554\n",
      "Iteration 209, loss = 0.22593801\n",
      "Iteration 210, loss = 0.22578844\n",
      "Iteration 211, loss = 0.22547976\n",
      "Iteration 212, loss = 0.22483505\n",
      "Iteration 213, loss = 0.22423630\n",
      "Iteration 214, loss = 0.22385373\n",
      "Iteration 215, loss = 0.22368342\n",
      "Iteration 216, loss = 0.22305503\n",
      "Iteration 217, loss = 0.22287974\n",
      "Iteration 218, loss = 0.22232611\n",
      "Iteration 219, loss = 0.22197280\n",
      "Iteration 220, loss = 0.22166192\n",
      "Iteration 221, loss = 0.22132969\n",
      "Iteration 222, loss = 0.22067793\n",
      "Iteration 223, loss = 0.22035009\n",
      "Iteration 224, loss = 0.22010305\n",
      "Iteration 225, loss = 0.22006913\n",
      "Iteration 226, loss = 0.21904457\n",
      "Iteration 227, loss = 0.21852760\n",
      "Iteration 228, loss = 0.21827085\n",
      "Iteration 229, loss = 0.21774434\n",
      "Iteration 230, loss = 0.21717299\n",
      "Iteration 231, loss = 0.21756180\n",
      "Iteration 232, loss = 0.21670349\n",
      "Iteration 233, loss = 0.21618370\n",
      "Iteration 234, loss = 0.21612690\n",
      "Iteration 235, loss = 0.21569103\n",
      "Iteration 236, loss = 0.21527060\n",
      "Iteration 237, loss = 0.21491268\n",
      "Iteration 238, loss = 0.21487976\n",
      "Iteration 239, loss = 0.21459284\n",
      "Iteration 240, loss = 0.21419972\n",
      "Iteration 241, loss = 0.21422524\n",
      "Iteration 242, loss = 0.21333105\n",
      "Iteration 243, loss = 0.21325321\n",
      "Iteration 244, loss = 0.21275459\n",
      "Iteration 245, loss = 0.21308630\n",
      "Iteration 246, loss = 0.21230731\n",
      "Iteration 247, loss = 0.21141065\n",
      "Iteration 248, loss = 0.21178212\n",
      "Iteration 249, loss = 0.21094761\n",
      "Iteration 250, loss = 0.21087357\n",
      "Iteration 251, loss = 0.21035226\n",
      "Iteration 252, loss = 0.21011067\n",
      "Iteration 253, loss = 0.20958111\n",
      "Iteration 254, loss = 0.20935856\n",
      "Iteration 255, loss = 0.20973660\n",
      "Iteration 256, loss = 0.20883647\n",
      "Iteration 257, loss = 0.20887361\n",
      "Iteration 258, loss = 0.20831813\n",
      "Iteration 259, loss = 0.20818249\n",
      "Iteration 260, loss = 0.20806440\n",
      "Iteration 261, loss = 0.20760654\n",
      "Iteration 262, loss = 0.20765740\n",
      "Iteration 263, loss = 0.20720934\n",
      "Iteration 264, loss = 0.20684920\n",
      "Iteration 265, loss = 0.20661127\n",
      "Iteration 266, loss = 0.20638690\n",
      "Iteration 267, loss = 0.20577758\n",
      "Iteration 268, loss = 0.20764205\n",
      "Iteration 269, loss = 0.20511229\n",
      "Iteration 270, loss = 0.20549856\n",
      "Iteration 271, loss = 0.20524714\n",
      "Iteration 272, loss = 0.20456731\n",
      "Iteration 273, loss = 0.20452869\n",
      "Iteration 274, loss = 0.20417567\n",
      "Iteration 275, loss = 0.20396316\n",
      "Iteration 276, loss = 0.20388953\n",
      "Iteration 277, loss = 0.20395151\n",
      "Iteration 278, loss = 0.20335060\n",
      "Iteration 279, loss = 0.20344375\n",
      "Iteration 280, loss = 0.20276603\n",
      "Iteration 281, loss = 0.20310540\n",
      "Iteration 282, loss = 0.20249879\n",
      "Iteration 283, loss = 0.20235759\n",
      "Iteration 284, loss = 0.20187527\n",
      "Iteration 285, loss = 0.20207127\n",
      "Iteration 286, loss = 0.20118584\n",
      "Iteration 287, loss = 0.20142734\n",
      "Iteration 288, loss = 0.20123735\n",
      "Iteration 289, loss = 0.20075601\n",
      "Iteration 290, loss = 0.20184252\n",
      "Iteration 291, loss = 0.20031608\n",
      "Iteration 292, loss = 0.20039651\n",
      "Iteration 293, loss = 0.19988762\n",
      "Iteration 294, loss = 0.19968591\n",
      "Iteration 295, loss = 0.19950928\n",
      "Iteration 296, loss = 0.19935881\n",
      "Iteration 297, loss = 0.19891494\n",
      "Iteration 298, loss = 0.19972373\n",
      "Iteration 299, loss = 0.20046341\n",
      "Iteration 300, loss = 0.19914260\n",
      "Iteration 301, loss = 0.19830940\n",
      "Iteration 302, loss = 0.19831813\n",
      "Iteration 303, loss = 0.19790227\n",
      "Iteration 304, loss = 0.19792242\n",
      "Iteration 305, loss = 0.19843461\n",
      "Iteration 306, loss = 0.19734022\n",
      "Iteration 307, loss = 0.19726163\n",
      "Iteration 308, loss = 0.19724184\n",
      "Iteration 309, loss = 0.19697800\n",
      "Iteration 310, loss = 0.19720351\n",
      "Iteration 311, loss = 0.19673126\n",
      "Iteration 312, loss = 0.19648526\n",
      "Iteration 313, loss = 0.19639084\n",
      "Iteration 314, loss = 0.19599056\n",
      "Iteration 315, loss = 0.19603871\n",
      "Iteration 316, loss = 0.19573497\n",
      "Iteration 317, loss = 0.19522788\n",
      "Iteration 318, loss = 0.19534598\n",
      "Iteration 319, loss = 0.19482370\n",
      "Iteration 320, loss = 0.19480287\n",
      "Iteration 321, loss = 0.19543999\n",
      "Iteration 322, loss = 0.19458696\n",
      "Iteration 323, loss = 0.19456154\n",
      "Iteration 324, loss = 0.19403171\n",
      "Iteration 325, loss = 0.19432225\n",
      "Iteration 326, loss = 0.19411109\n",
      "Iteration 327, loss = 0.19379044\n",
      "Iteration 328, loss = 0.19367995\n",
      "Iteration 329, loss = 0.19345533\n",
      "Iteration 330, loss = 0.19380761\n",
      "Iteration 331, loss = 0.19306544\n",
      "Iteration 332, loss = 0.19340603\n",
      "Iteration 333, loss = 0.19339099\n",
      "Iteration 334, loss = 0.19260770\n",
      "Iteration 335, loss = 0.19290273\n",
      "Iteration 336, loss = 0.19244065\n",
      "Iteration 337, loss = 0.19210167\n",
      "Iteration 338, loss = 0.19228734\n",
      "Iteration 339, loss = 0.19265603\n",
      "Iteration 340, loss = 0.19182228\n",
      "Iteration 341, loss = 0.19194818\n",
      "Iteration 342, loss = 0.19145244\n",
      "Iteration 343, loss = 0.19168885\n",
      "Iteration 344, loss = 0.19230096\n",
      "Iteration 345, loss = 0.19105222\n",
      "Iteration 346, loss = 0.19110447\n",
      "Iteration 347, loss = 0.19058395\n",
      "Iteration 348, loss = 0.19069016\n",
      "Iteration 349, loss = 0.19066463\n",
      "Iteration 350, loss = 0.19052265\n",
      "Iteration 351, loss = 0.19097553\n",
      "Iteration 352, loss = 0.19043284\n",
      "Iteration 353, loss = 0.19089972\n",
      "Iteration 354, loss = 0.19003708\n",
      "Iteration 355, loss = 0.19023770\n",
      "Iteration 356, loss = 0.18979267\n",
      "Iteration 357, loss = 0.18980440\n",
      "Iteration 358, loss = 0.18927908\n",
      "Iteration 359, loss = 0.18923650\n",
      "Iteration 360, loss = 0.18902867\n",
      "Iteration 361, loss = 0.18957800\n",
      "Iteration 362, loss = 0.18881469\n",
      "Iteration 363, loss = 0.18853015\n",
      "Iteration 364, loss = 0.18906368\n",
      "Iteration 365, loss = 0.18867362\n",
      "Iteration 366, loss = 0.18832188\n",
      "Iteration 367, loss = 0.18822207\n",
      "Iteration 368, loss = 0.18837049\n",
      "Iteration 369, loss = 0.18800877\n",
      "Iteration 370, loss = 0.18817538\n",
      "Iteration 371, loss = 0.18782947\n",
      "Iteration 372, loss = 0.18773640\n",
      "Iteration 373, loss = 0.18814915\n",
      "Iteration 374, loss = 0.18789005\n",
      "Iteration 375, loss = 0.18765356\n",
      "Iteration 376, loss = 0.18739418\n",
      "Iteration 377, loss = 0.18711208\n",
      "Iteration 378, loss = 0.18710649\n",
      "Iteration 379, loss = 0.18672551\n",
      "Iteration 380, loss = 0.18652864\n",
      "Iteration 381, loss = 0.18669005\n",
      "Iteration 382, loss = 0.18660432\n",
      "Iteration 383, loss = 0.18642463\n",
      "Iteration 384, loss = 0.18665857\n",
      "Iteration 385, loss = 0.18667241\n",
      "Iteration 386, loss = 0.18633196\n",
      "Iteration 387, loss = 0.18633285\n",
      "Iteration 388, loss = 0.18663621\n",
      "Iteration 389, loss = 0.18586360\n",
      "Iteration 390, loss = 0.18610975\n",
      "Iteration 391, loss = 0.18553657\n",
      "Iteration 392, loss = 0.18536285\n",
      "Iteration 393, loss = 0.18538785\n",
      "Iteration 394, loss = 0.18552337\n",
      "Iteration 395, loss = 0.18558863\n",
      "Iteration 396, loss = 0.18520871\n",
      "Iteration 397, loss = 0.18506751\n",
      "Iteration 398, loss = 0.18518054\n",
      "Iteration 399, loss = 0.18487656\n",
      "Iteration 400, loss = 0.18506246\n",
      "Iteration 401, loss = 0.18597189\n",
      "Iteration 402, loss = 0.18551210\n",
      "Iteration 403, loss = 0.18456086\n",
      "Iteration 404, loss = 0.18442457\n",
      "Iteration 405, loss = 0.18423559\n",
      "Iteration 406, loss = 0.18413420\n",
      "Iteration 407, loss = 0.18399032\n",
      "Iteration 408, loss = 0.18380902\n",
      "Iteration 409, loss = 0.18401771\n",
      "Iteration 410, loss = 0.18421916\n",
      "Iteration 411, loss = 0.18395070\n",
      "Iteration 412, loss = 0.18375423\n",
      "Iteration 413, loss = 0.18390315\n",
      "Iteration 414, loss = 0.18426328\n",
      "Iteration 415, loss = 0.18319397\n",
      "Iteration 416, loss = 0.18401734\n",
      "Iteration 417, loss = 0.18330570\n",
      "Iteration 418, loss = 0.18333592\n",
      "Iteration 419, loss = 0.18314796\n",
      "Iteration 420, loss = 0.18328966\n",
      "Iteration 421, loss = 0.18354375\n",
      "Iteration 422, loss = 0.18277479\n",
      "Iteration 423, loss = 0.18315246\n",
      "Iteration 424, loss = 0.18326814\n",
      "Iteration 425, loss = 0.18301648\n",
      "Iteration 426, loss = 0.18313187\n",
      "Iteration 427, loss = 0.18294269\n",
      "Iteration 428, loss = 0.18240419\n",
      "Iteration 429, loss = 0.18246110\n",
      "Iteration 430, loss = 0.18231100\n",
      "Iteration 431, loss = 0.18257027\n",
      "Iteration 432, loss = 0.18209165\n",
      "Iteration 433, loss = 0.18226584\n",
      "Iteration 434, loss = 0.18229508\n",
      "Iteration 435, loss = 0.18195944\n",
      "Iteration 436, loss = 0.18236160\n",
      "Iteration 437, loss = 0.18154475\n",
      "Iteration 438, loss = 0.18153531\n",
      "Iteration 439, loss = 0.18140522\n",
      "Iteration 440, loss = 0.18189496\n",
      "Iteration 441, loss = 0.18168718\n",
      "Iteration 442, loss = 0.18113856\n",
      "Iteration 443, loss = 0.18140657\n",
      "Iteration 444, loss = 0.18108422\n",
      "Iteration 445, loss = 0.18142781\n",
      "Iteration 446, loss = 0.18114071\n",
      "Iteration 447, loss = 0.18129408\n",
      "Iteration 448, loss = 0.18095885\n",
      "Iteration 449, loss = 0.18131805\n",
      "Iteration 450, loss = 0.18172258\n",
      "Iteration 451, loss = 0.18118249\n",
      "Iteration 452, loss = 0.18111755\n",
      "Iteration 453, loss = 0.18066516\n",
      "Iteration 454, loss = 0.18060623\n",
      "Iteration 455, loss = 0.18090000\n",
      "Iteration 456, loss = 0.18091891\n",
      "Iteration 457, loss = 0.18118534\n",
      "Iteration 458, loss = 0.18039001\n",
      "Iteration 459, loss = 0.18119518\n",
      "Iteration 460, loss = 0.18010016\n",
      "Iteration 461, loss = 0.18135934\n",
      "Iteration 462, loss = 0.18065633\n",
      "Iteration 463, loss = 0.18025968\n",
      "Iteration 464, loss = 0.18000703\n",
      "Iteration 465, loss = 0.18002950\n",
      "Iteration 466, loss = 0.18057048\n",
      "Iteration 467, loss = 0.17990850\n",
      "Iteration 468, loss = 0.18065614\n",
      "Iteration 469, loss = 0.17968874\n",
      "Iteration 470, loss = 0.17975295\n",
      "Iteration 471, loss = 0.17956808\n",
      "Iteration 472, loss = 0.17922319\n",
      "Iteration 473, loss = 0.17961532\n",
      "Iteration 474, loss = 0.17954773\n",
      "Iteration 475, loss = 0.17928089\n",
      "Iteration 476, loss = 0.17929402\n",
      "Iteration 477, loss = 0.17910062\n",
      "Iteration 478, loss = 0.17892316\n",
      "Iteration 479, loss = 0.17940086\n",
      "Iteration 480, loss = 0.17908258\n",
      "Iteration 481, loss = 0.17905689\n",
      "Iteration 482, loss = 0.17924644\n",
      "Iteration 483, loss = 0.17877050\n",
      "Iteration 484, loss = 0.17927794\n",
      "Iteration 485, loss = 0.17842649\n",
      "Iteration 486, loss = 0.17905761\n",
      "Iteration 487, loss = 0.18018138\n",
      "Iteration 488, loss = 0.17848359\n",
      "Iteration 489, loss = 0.18008193\n",
      "Iteration 490, loss = 0.17827845\n",
      "Iteration 491, loss = 0.17832692\n",
      "Iteration 492, loss = 0.17851599\n",
      "Iteration 493, loss = 0.17802221\n",
      "Iteration 494, loss = 0.17854211\n",
      "Iteration 495, loss = 0.17802930\n",
      "Iteration 496, loss = 0.17829657\n",
      "Iteration 497, loss = 0.17801001\n",
      "Iteration 498, loss = 0.17810854\n",
      "Iteration 499, loss = 0.17774203\n",
      "Iteration 500, loss = 0.17856505\n",
      "Iteration 501, loss = 0.17775915\n",
      "Iteration 502, loss = 0.17735745\n",
      "Iteration 503, loss = 0.17710752\n",
      "Iteration 504, loss = 0.17713471\n",
      "Iteration 505, loss = 0.17680397\n",
      "Iteration 506, loss = 0.17601914\n",
      "Iteration 507, loss = 0.17644012\n",
      "Iteration 508, loss = 0.17509466\n",
      "Iteration 509, loss = 0.17504187\n",
      "Iteration 510, loss = 0.17482999\n",
      "Iteration 511, loss = 0.17486155\n",
      "Iteration 512, loss = 0.17485833\n",
      "Iteration 513, loss = 0.17440670\n",
      "Iteration 514, loss = 0.17426228\n",
      "Iteration 515, loss = 0.17413801\n",
      "Iteration 516, loss = 0.17466136\n",
      "Iteration 517, loss = 0.17404618\n",
      "Iteration 518, loss = 0.17452864\n",
      "Iteration 519, loss = 0.17380468\n",
      "Iteration 520, loss = 0.17392870\n",
      "Iteration 521, loss = 0.17383284\n",
      "Iteration 522, loss = 0.17381797\n",
      "Iteration 523, loss = 0.17423282\n",
      "Iteration 524, loss = 0.17418003\n",
      "Iteration 525, loss = 0.17379881\n",
      "Iteration 526, loss = 0.17330265\n",
      "Iteration 527, loss = 0.17402891\n",
      "Iteration 528, loss = 0.17369925\n",
      "Iteration 529, loss = 0.17356421\n",
      "Iteration 530, loss = 0.17333490\n",
      "Iteration 531, loss = 0.17316227\n",
      "Iteration 532, loss = 0.17330089\n",
      "Iteration 533, loss = 0.17305043\n",
      "Iteration 534, loss = 0.17325284\n",
      "Iteration 535, loss = 0.17290585\n",
      "Iteration 536, loss = 0.17323161\n",
      "Iteration 537, loss = 0.17316841\n",
      "Iteration 538, loss = 0.17335894\n",
      "Iteration 539, loss = 0.17320248\n",
      "Iteration 540, loss = 0.17336710\n",
      "Iteration 541, loss = 0.17344886\n",
      "Iteration 542, loss = 0.17276466\n",
      "Iteration 543, loss = 0.17284361\n",
      "Iteration 544, loss = 0.17301953\n",
      "Iteration 545, loss = 0.17343992\n",
      "Iteration 546, loss = 0.17278957\n",
      "Iteration 547, loss = 0.17296674\n",
      "Iteration 548, loss = 0.17267275\n",
      "Iteration 549, loss = 0.17263682\n",
      "Iteration 550, loss = 0.17229563\n",
      "Iteration 551, loss = 0.17247016\n",
      "Iteration 552, loss = 0.17276869\n",
      "Iteration 553, loss = 0.17297679\n",
      "Iteration 554, loss = 0.17259327\n",
      "Iteration 555, loss = 0.17240449\n",
      "Iteration 556, loss = 0.17248519\n",
      "Iteration 557, loss = 0.17267963\n",
      "Iteration 558, loss = 0.17218574\n",
      "Iteration 559, loss = 0.17259198\n",
      "Iteration 560, loss = 0.17215189\n",
      "Iteration 561, loss = 0.17229407\n",
      "Iteration 562, loss = 0.17211757\n",
      "Iteration 563, loss = 0.17319321\n",
      "Iteration 564, loss = 0.17279392\n",
      "Iteration 565, loss = 0.17231055\n",
      "Iteration 566, loss = 0.17184436\n",
      "Iteration 567, loss = 0.17344981\n",
      "Iteration 568, loss = 0.17250004\n",
      "Iteration 569, loss = 0.17175201\n",
      "Iteration 570, loss = 0.17241542\n",
      "Iteration 571, loss = 0.17218909\n",
      "Iteration 572, loss = 0.17190949\n",
      "Iteration 573, loss = 0.17192345\n",
      "Iteration 574, loss = 0.17294647\n",
      "Iteration 575, loss = 0.17235528\n",
      "Iteration 576, loss = 0.17199860\n",
      "Iteration 577, loss = 0.17167557\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76816861\n",
      "Iteration 2, loss = 0.74252884\n",
      "Iteration 3, loss = 0.72074994\n",
      "Iteration 4, loss = 0.70354890\n",
      "Iteration 5, loss = 0.68919952\n",
      "Iteration 6, loss = 0.67648994\n",
      "Iteration 7, loss = 0.66309960\n",
      "Iteration 8, loss = 0.64802344\n",
      "Iteration 9, loss = 0.62994022\n",
      "Iteration 10, loss = 0.61034896\n",
      "Iteration 11, loss = 0.59045212\n",
      "Iteration 12, loss = 0.57103167\n",
      "Iteration 13, loss = 0.55452032\n",
      "Iteration 14, loss = 0.54069885\n",
      "Iteration 15, loss = 0.52869522\n",
      "Iteration 16, loss = 0.51867909\n",
      "Iteration 17, loss = 0.50911668\n",
      "Iteration 18, loss = 0.50066858\n",
      "Iteration 19, loss = 0.49305453\n",
      "Iteration 20, loss = 0.48607188\n",
      "Iteration 21, loss = 0.47977340\n",
      "Iteration 22, loss = 0.47347367\n",
      "Iteration 23, loss = 0.46817220\n",
      "Iteration 24, loss = 0.46299117\n",
      "Iteration 25, loss = 0.45822510\n",
      "Iteration 26, loss = 0.45363954\n",
      "Iteration 27, loss = 0.44917185\n",
      "Iteration 28, loss = 0.44482203\n",
      "Iteration 29, loss = 0.44074819\n",
      "Iteration 30, loss = 0.43667837\n",
      "Iteration 31, loss = 0.43298610\n",
      "Iteration 32, loss = 0.42904104\n",
      "Iteration 33, loss = 0.42544473\n",
      "Iteration 34, loss = 0.42183160\n",
      "Iteration 35, loss = 0.41835830\n",
      "Iteration 36, loss = 0.41510212\n",
      "Iteration 37, loss = 0.41188157\n",
      "Iteration 38, loss = 0.40867402\n",
      "Iteration 39, loss = 0.40554196\n",
      "Iteration 40, loss = 0.40258245\n",
      "Iteration 41, loss = 0.39946634\n",
      "Iteration 42, loss = 0.39660795\n",
      "Iteration 43, loss = 0.39419866\n",
      "Iteration 44, loss = 0.39100885\n",
      "Iteration 45, loss = 0.38862456\n",
      "Iteration 46, loss = 0.38567049\n",
      "Iteration 47, loss = 0.38337946\n",
      "Iteration 48, loss = 0.38073065\n",
      "Iteration 49, loss = 0.37844269\n",
      "Iteration 50, loss = 0.37598703\n",
      "Iteration 51, loss = 0.37367883\n",
      "Iteration 52, loss = 0.37150256\n",
      "Iteration 53, loss = 0.36919502\n",
      "Iteration 54, loss = 0.36699921\n",
      "Iteration 55, loss = 0.36469118\n",
      "Iteration 56, loss = 0.36274667\n",
      "Iteration 57, loss = 0.36075425\n",
      "Iteration 58, loss = 0.35866371\n",
      "Iteration 59, loss = 0.35675378\n",
      "Iteration 60, loss = 0.35504539\n",
      "Iteration 61, loss = 0.35305385\n",
      "Iteration 62, loss = 0.35104712\n",
      "Iteration 63, loss = 0.34923438\n",
      "Iteration 64, loss = 0.34746780\n",
      "Iteration 65, loss = 0.34569279\n",
      "Iteration 66, loss = 0.34380590\n",
      "Iteration 67, loss = 0.34209848\n",
      "Iteration 68, loss = 0.34019207\n",
      "Iteration 69, loss = 0.33821999\n",
      "Iteration 70, loss = 0.33652849\n",
      "Iteration 71, loss = 0.33455389\n",
      "Iteration 72, loss = 0.33275984\n",
      "Iteration 73, loss = 0.33102964\n",
      "Iteration 74, loss = 0.32921255\n",
      "Iteration 75, loss = 0.32745134\n",
      "Iteration 76, loss = 0.32574147\n",
      "Iteration 77, loss = 0.32438962\n",
      "Iteration 78, loss = 0.32238359\n",
      "Iteration 79, loss = 0.32103021\n",
      "Iteration 80, loss = 0.31912252\n",
      "Iteration 81, loss = 0.31762707\n",
      "Iteration 82, loss = 0.31628169\n",
      "Iteration 83, loss = 0.31469587\n",
      "Iteration 84, loss = 0.31321971\n",
      "Iteration 85, loss = 0.31174893\n",
      "Iteration 86, loss = 0.31050306\n",
      "Iteration 87, loss = 0.30889257\n",
      "Iteration 88, loss = 0.30750229\n",
      "Iteration 89, loss = 0.30595728\n",
      "Iteration 90, loss = 0.30468706\n",
      "Iteration 91, loss = 0.30332917\n",
      "Iteration 92, loss = 0.30206547\n",
      "Iteration 93, loss = 0.30094281\n",
      "Iteration 94, loss = 0.29953642\n",
      "Iteration 95, loss = 0.29815967\n",
      "Iteration 96, loss = 0.29697616\n",
      "Iteration 97, loss = 0.29612101\n",
      "Iteration 98, loss = 0.29470809\n",
      "Iteration 99, loss = 0.29350018\n",
      "Iteration 100, loss = 0.29238912\n",
      "Iteration 101, loss = 0.29148192\n",
      "Iteration 102, loss = 0.29028891\n",
      "Iteration 103, loss = 0.28929296\n",
      "Iteration 104, loss = 0.28827797\n",
      "Iteration 105, loss = 0.28716991\n",
      "Iteration 106, loss = 0.28600300\n",
      "Iteration 107, loss = 0.28457635\n",
      "Iteration 108, loss = 0.28376112\n",
      "Iteration 109, loss = 0.28259689\n",
      "Iteration 110, loss = 0.28165022\n",
      "Iteration 111, loss = 0.28049609\n",
      "Iteration 112, loss = 0.27967401\n",
      "Iteration 113, loss = 0.27851755\n",
      "Iteration 114, loss = 0.27745925\n",
      "Iteration 115, loss = 0.27646125\n",
      "Iteration 116, loss = 0.27560671\n",
      "Iteration 117, loss = 0.27460817\n",
      "Iteration 118, loss = 0.27349039\n",
      "Iteration 119, loss = 0.27322365\n",
      "Iteration 120, loss = 0.27173130\n",
      "Iteration 121, loss = 0.27084177\n",
      "Iteration 122, loss = 0.27080933\n",
      "Iteration 123, loss = 0.26910176\n",
      "Iteration 124, loss = 0.26829963\n",
      "Iteration 125, loss = 0.26764441\n",
      "Iteration 126, loss = 0.26681481\n",
      "Iteration 127, loss = 0.26571223\n",
      "Iteration 128, loss = 0.26512811\n",
      "Iteration 129, loss = 0.26513352\n",
      "Iteration 130, loss = 0.26429419\n",
      "Iteration 131, loss = 0.26290959\n",
      "Iteration 132, loss = 0.26172501\n",
      "Iteration 133, loss = 0.26105030\n",
      "Iteration 134, loss = 0.26021995\n",
      "Iteration 135, loss = 0.25981733\n",
      "Iteration 136, loss = 0.25899427\n",
      "Iteration 137, loss = 0.25829448\n",
      "Iteration 138, loss = 0.25752681\n",
      "Iteration 139, loss = 0.25686620\n",
      "Iteration 140, loss = 0.25644693\n",
      "Iteration 141, loss = 0.25550778\n",
      "Iteration 142, loss = 0.25516827\n",
      "Iteration 143, loss = 0.25429733\n",
      "Iteration 144, loss = 0.25374026\n",
      "Iteration 145, loss = 0.25337822\n",
      "Iteration 146, loss = 0.25233321\n",
      "Iteration 147, loss = 0.25263480\n",
      "Iteration 148, loss = 0.25099707\n",
      "Iteration 149, loss = 0.25084823\n",
      "Iteration 150, loss = 0.24993641\n",
      "Iteration 151, loss = 0.24948199\n",
      "Iteration 152, loss = 0.24877558\n",
      "Iteration 153, loss = 0.24837463\n",
      "Iteration 154, loss = 0.24785010\n",
      "Iteration 155, loss = 0.24725130\n",
      "Iteration 156, loss = 0.24652392\n",
      "Iteration 157, loss = 0.24630371\n",
      "Iteration 158, loss = 0.24574892\n",
      "Iteration 159, loss = 0.24474369\n",
      "Iteration 160, loss = 0.24458328\n",
      "Iteration 161, loss = 0.24401671\n",
      "Iteration 162, loss = 0.24374555\n",
      "Iteration 163, loss = 0.24325342\n",
      "Iteration 164, loss = 0.24220512\n",
      "Iteration 165, loss = 0.24170034\n",
      "Iteration 166, loss = 0.24117782\n",
      "Iteration 167, loss = 0.24082547\n",
      "Iteration 168, loss = 0.24024262\n",
      "Iteration 169, loss = 0.24045559\n",
      "Iteration 170, loss = 0.23943144\n",
      "Iteration 171, loss = 0.23962376\n",
      "Iteration 172, loss = 0.23846279\n",
      "Iteration 173, loss = 0.23787137\n",
      "Iteration 174, loss = 0.23764055\n",
      "Iteration 175, loss = 0.23698244\n",
      "Iteration 176, loss = 0.23681720\n",
      "Iteration 177, loss = 0.23634535\n",
      "Iteration 178, loss = 0.23597224\n",
      "Iteration 179, loss = 0.23555856\n",
      "Iteration 180, loss = 0.23503802\n",
      "Iteration 181, loss = 0.23459220\n",
      "Iteration 182, loss = 0.23409849\n",
      "Iteration 183, loss = 0.23378334\n",
      "Iteration 184, loss = 0.23328333\n",
      "Iteration 185, loss = 0.23326134\n",
      "Iteration 186, loss = 0.23258898\n",
      "Iteration 187, loss = 0.23214899\n",
      "Iteration 188, loss = 0.23192288\n",
      "Iteration 189, loss = 0.23157310\n",
      "Iteration 190, loss = 0.23119339\n",
      "Iteration 191, loss = 0.23076216\n",
      "Iteration 192, loss = 0.23041778\n",
      "Iteration 193, loss = 0.23005789\n",
      "Iteration 194, loss = 0.22945405\n",
      "Iteration 195, loss = 0.22889333\n",
      "Iteration 196, loss = 0.22856766\n",
      "Iteration 197, loss = 0.22845036\n",
      "Iteration 198, loss = 0.22841001\n",
      "Iteration 199, loss = 0.22793325\n",
      "Iteration 200, loss = 0.22754491\n",
      "Iteration 201, loss = 0.22674431\n",
      "Iteration 202, loss = 0.22668980\n",
      "Iteration 203, loss = 0.22613703\n",
      "Iteration 204, loss = 0.22594228\n",
      "Iteration 205, loss = 0.22559093\n",
      "Iteration 206, loss = 0.22519651\n",
      "Iteration 207, loss = 0.22493001\n",
      "Iteration 208, loss = 0.22452107\n",
      "Iteration 209, loss = 0.22427535\n",
      "Iteration 210, loss = 0.22388168\n",
      "Iteration 211, loss = 0.22402044\n",
      "Iteration 212, loss = 0.22359155\n",
      "Iteration 213, loss = 0.22325895\n",
      "Iteration 214, loss = 0.22269093\n",
      "Iteration 215, loss = 0.22263262\n",
      "Iteration 216, loss = 0.22268467\n",
      "Iteration 217, loss = 0.22189367\n",
      "Iteration 218, loss = 0.22156179\n",
      "Iteration 219, loss = 0.22164170\n",
      "Iteration 220, loss = 0.22203226\n",
      "Iteration 221, loss = 0.22085482\n",
      "Iteration 222, loss = 0.22111656\n",
      "Iteration 223, loss = 0.22135830\n",
      "Iteration 224, loss = 0.22041455\n",
      "Iteration 225, loss = 0.21962196\n",
      "Iteration 226, loss = 0.22010426\n",
      "Iteration 227, loss = 0.21941230\n",
      "Iteration 228, loss = 0.21891409\n",
      "Iteration 229, loss = 0.21897668\n",
      "Iteration 230, loss = 0.21841343\n",
      "Iteration 231, loss = 0.21848021\n",
      "Iteration 232, loss = 0.21780937\n",
      "Iteration 233, loss = 0.21780819\n",
      "Iteration 234, loss = 0.21805045\n",
      "Iteration 235, loss = 0.21761319\n",
      "Iteration 236, loss = 0.21754160\n",
      "Iteration 237, loss = 0.21696398\n",
      "Iteration 238, loss = 0.21670126\n",
      "Iteration 239, loss = 0.21647426\n",
      "Iteration 240, loss = 0.21673413\n",
      "Iteration 241, loss = 0.21615637\n",
      "Iteration 242, loss = 0.21609583\n",
      "Iteration 243, loss = 0.21577894\n",
      "Iteration 244, loss = 0.21575399\n",
      "Iteration 245, loss = 0.21513521\n",
      "Iteration 246, loss = 0.21526033\n",
      "Iteration 247, loss = 0.21488891\n",
      "Iteration 248, loss = 0.21509712\n",
      "Iteration 249, loss = 0.21502017\n",
      "Iteration 250, loss = 0.21481597\n",
      "Iteration 251, loss = 0.21428027\n",
      "Iteration 252, loss = 0.21436374\n",
      "Iteration 253, loss = 0.21481132\n",
      "Iteration 254, loss = 0.21437351\n",
      "Iteration 255, loss = 0.21353130\n",
      "Iteration 256, loss = 0.21373398\n",
      "Iteration 257, loss = 0.21300529\n",
      "Iteration 258, loss = 0.21372609\n",
      "Iteration 259, loss = 0.21277430\n",
      "Iteration 260, loss = 0.21278538\n",
      "Iteration 261, loss = 0.21276472\n",
      "Iteration 262, loss = 0.21248177\n",
      "Iteration 263, loss = 0.21229238\n",
      "Iteration 264, loss = 0.21361111\n",
      "Iteration 265, loss = 0.21180306\n",
      "Iteration 266, loss = 0.21240952\n",
      "Iteration 267, loss = 0.21118084\n",
      "Iteration 268, loss = 0.21150698\n",
      "Iteration 269, loss = 0.21186353\n",
      "Iteration 270, loss = 0.21186343\n",
      "Iteration 271, loss = 0.21138006\n",
      "Iteration 272, loss = 0.21072893\n",
      "Iteration 273, loss = 0.21057363\n",
      "Iteration 274, loss = 0.21044518\n",
      "Iteration 275, loss = 0.21039791\n",
      "Iteration 276, loss = 0.21036989\n",
      "Iteration 277, loss = 0.20973249\n",
      "Iteration 278, loss = 0.21002541\n",
      "Iteration 279, loss = 0.20984894\n",
      "Iteration 280, loss = 0.20931399\n",
      "Iteration 281, loss = 0.20972969\n",
      "Iteration 282, loss = 0.20969086\n",
      "Iteration 283, loss = 0.20883052\n",
      "Iteration 284, loss = 0.20922460\n",
      "Iteration 285, loss = 0.20872922\n",
      "Iteration 286, loss = 0.20874898\n",
      "Iteration 287, loss = 0.20892223\n",
      "Iteration 288, loss = 0.20830618\n",
      "Iteration 289, loss = 0.20817514\n",
      "Iteration 290, loss = 0.20810224\n",
      "Iteration 291, loss = 0.20774435\n",
      "Iteration 292, loss = 0.20847831\n",
      "Iteration 293, loss = 0.20770866\n",
      "Iteration 294, loss = 0.20768803\n",
      "Iteration 295, loss = 0.20761570\n",
      "Iteration 296, loss = 0.20728488\n",
      "Iteration 297, loss = 0.20779106\n",
      "Iteration 298, loss = 0.20646278\n",
      "Iteration 299, loss = 0.20714615\n",
      "Iteration 300, loss = 0.20667164\n",
      "Iteration 301, loss = 0.20633085\n",
      "Iteration 302, loss = 0.20607716\n",
      "Iteration 303, loss = 0.20600556\n",
      "Iteration 304, loss = 0.20555247\n",
      "Iteration 305, loss = 0.20556737\n",
      "Iteration 306, loss = 0.20574467\n",
      "Iteration 307, loss = 0.20496033\n",
      "Iteration 308, loss = 0.20479785\n",
      "Iteration 309, loss = 0.20466062\n",
      "Iteration 310, loss = 0.20426400\n",
      "Iteration 311, loss = 0.20359864\n",
      "Iteration 312, loss = 0.20323433\n",
      "Iteration 313, loss = 0.20288001\n",
      "Iteration 314, loss = 0.20278933\n",
      "Iteration 315, loss = 0.20297202\n",
      "Iteration 316, loss = 0.20231984\n",
      "Iteration 317, loss = 0.20186328\n",
      "Iteration 318, loss = 0.20215792\n",
      "Iteration 319, loss = 0.20157275\n",
      "Iteration 320, loss = 0.20121330\n",
      "Iteration 321, loss = 0.20077059\n",
      "Iteration 322, loss = 0.20024779\n",
      "Iteration 323, loss = 0.20102349\n",
      "Iteration 324, loss = 0.20096933\n",
      "Iteration 325, loss = 0.20086506\n",
      "Iteration 326, loss = 0.19943249\n",
      "Iteration 327, loss = 0.19980359\n",
      "Iteration 328, loss = 0.19966753\n",
      "Iteration 329, loss = 0.19873987\n",
      "Iteration 330, loss = 0.19864237\n",
      "Iteration 331, loss = 0.19895169\n",
      "Iteration 332, loss = 0.19831973\n",
      "Iteration 333, loss = 0.19786636\n",
      "Iteration 334, loss = 0.19832397\n",
      "Iteration 335, loss = 0.19752754\n",
      "Iteration 336, loss = 0.19741742\n",
      "Iteration 337, loss = 0.19741807\n",
      "Iteration 338, loss = 0.19730294\n",
      "Iteration 339, loss = 0.19736235\n",
      "Iteration 340, loss = 0.19714723\n",
      "Iteration 341, loss = 0.19654644\n",
      "Iteration 342, loss = 0.19666379\n",
      "Iteration 343, loss = 0.19650471\n",
      "Iteration 344, loss = 0.19653752\n",
      "Iteration 345, loss = 0.19597876\n",
      "Iteration 346, loss = 0.19650657\n",
      "Iteration 347, loss = 0.19618661\n",
      "Iteration 348, loss = 0.19589036\n",
      "Iteration 349, loss = 0.19581999\n",
      "Iteration 350, loss = 0.19531766\n",
      "Iteration 351, loss = 0.19546752\n",
      "Iteration 352, loss = 0.19502631\n",
      "Iteration 353, loss = 0.19508699\n",
      "Iteration 354, loss = 0.19522151\n",
      "Iteration 355, loss = 0.19497416\n",
      "Iteration 356, loss = 0.19490364\n",
      "Iteration 357, loss = 0.19481606\n",
      "Iteration 358, loss = 0.19474260\n",
      "Iteration 359, loss = 0.19450125\n",
      "Iteration 360, loss = 0.19417836\n",
      "Iteration 361, loss = 0.19415457\n",
      "Iteration 362, loss = 0.19430743\n",
      "Iteration 363, loss = 0.19394799\n",
      "Iteration 364, loss = 0.19375296\n",
      "Iteration 365, loss = 0.19429086\n",
      "Iteration 366, loss = 0.19377244\n",
      "Iteration 367, loss = 0.19380418\n",
      "Iteration 368, loss = 0.19349243\n",
      "Iteration 369, loss = 0.19309157\n",
      "Iteration 370, loss = 0.19381630\n",
      "Iteration 371, loss = 0.19325014\n",
      "Iteration 372, loss = 0.19283297\n",
      "Iteration 373, loss = 0.19235433\n",
      "Iteration 374, loss = 0.19380493\n",
      "Iteration 375, loss = 0.19298682\n",
      "Iteration 376, loss = 0.19312936\n",
      "Iteration 377, loss = 0.19272087\n",
      "Iteration 378, loss = 0.19266644\n",
      "Iteration 379, loss = 0.19201744\n",
      "Iteration 380, loss = 0.19209980\n",
      "Iteration 381, loss = 0.19239154\n",
      "Iteration 382, loss = 0.19194974\n",
      "Iteration 383, loss = 0.19222817\n",
      "Iteration 384, loss = 0.19149371\n",
      "Iteration 385, loss = 0.19220275\n",
      "Iteration 386, loss = 0.19178719\n",
      "Iteration 387, loss = 0.19156418\n",
      "Iteration 388, loss = 0.19141939\n",
      "Iteration 389, loss = 0.19137179\n",
      "Iteration 390, loss = 0.19136416\n",
      "Iteration 391, loss = 0.19139276\n",
      "Iteration 392, loss = 0.19116921\n",
      "Iteration 393, loss = 0.19131556\n",
      "Iteration 394, loss = 0.19155551\n",
      "Iteration 395, loss = 0.19081841\n",
      "Iteration 396, loss = 0.19106850\n",
      "Iteration 397, loss = 0.19088480\n",
      "Iteration 398, loss = 0.19060976\n",
      "Iteration 399, loss = 0.19066424\n",
      "Iteration 400, loss = 0.19047312\n",
      "Iteration 401, loss = 0.19164006\n",
      "Iteration 402, loss = 0.19067536\n",
      "Iteration 403, loss = 0.19028370\n",
      "Iteration 404, loss = 0.19020512\n",
      "Iteration 405, loss = 0.19034115\n",
      "Iteration 406, loss = 0.19041395\n",
      "Iteration 407, loss = 0.19020261\n",
      "Iteration 408, loss = 0.19014131\n",
      "Iteration 409, loss = 0.18972260\n",
      "Iteration 410, loss = 0.19000705\n",
      "Iteration 411, loss = 0.19012697\n",
      "Iteration 412, loss = 0.18949096\n",
      "Iteration 413, loss = 0.18949603\n",
      "Iteration 414, loss = 0.18965371\n",
      "Iteration 415, loss = 0.19021970\n",
      "Iteration 416, loss = 0.18932289\n",
      "Iteration 417, loss = 0.18941413\n",
      "Iteration 418, loss = 0.18920367\n",
      "Iteration 419, loss = 0.18930039\n",
      "Iteration 420, loss = 0.18908460\n",
      "Iteration 421, loss = 0.18911803\n",
      "Iteration 422, loss = 0.18905367\n",
      "Iteration 423, loss = 0.18906343\n",
      "Iteration 424, loss = 0.18869838\n",
      "Iteration 425, loss = 0.18908089\n",
      "Iteration 426, loss = 0.18904636\n",
      "Iteration 427, loss = 0.18878010\n",
      "Iteration 428, loss = 0.18834773\n",
      "Iteration 429, loss = 0.18889104\n",
      "Iteration 430, loss = 0.18831857\n",
      "Iteration 431, loss = 0.18896586\n",
      "Iteration 432, loss = 0.18871786\n",
      "Iteration 433, loss = 0.18877855\n",
      "Iteration 434, loss = 0.18821312\n",
      "Iteration 435, loss = 0.18857159\n",
      "Iteration 436, loss = 0.18846635\n",
      "Iteration 437, loss = 0.18834282\n",
      "Iteration 438, loss = 0.18871164\n",
      "Iteration 439, loss = 0.18766113\n",
      "Iteration 440, loss = 0.18779801\n",
      "Iteration 441, loss = 0.18778248\n",
      "Iteration 442, loss = 0.18850843\n",
      "Iteration 443, loss = 0.18874387\n",
      "Iteration 444, loss = 0.18916377\n",
      "Iteration 445, loss = 0.18788266\n",
      "Iteration 446, loss = 0.18775650\n",
      "Iteration 447, loss = 0.18764269\n",
      "Iteration 448, loss = 0.18743889\n",
      "Iteration 449, loss = 0.18825156\n",
      "Iteration 450, loss = 0.18782573\n",
      "Iteration 451, loss = 0.18799468\n",
      "Iteration 452, loss = 0.18745447\n",
      "Iteration 453, loss = 0.18861336\n",
      "Iteration 454, loss = 0.18727310\n",
      "Iteration 455, loss = 0.18727981\n",
      "Iteration 456, loss = 0.18733281\n",
      "Iteration 457, loss = 0.18725969\n",
      "Iteration 458, loss = 0.18688137\n",
      "Iteration 459, loss = 0.18710911\n",
      "Iteration 460, loss = 0.18692935\n",
      "Iteration 461, loss = 0.18755199\n",
      "Iteration 462, loss = 0.18811688\n",
      "Iteration 463, loss = 0.18674178\n",
      "Iteration 464, loss = 0.18698342\n",
      "Iteration 465, loss = 0.18728868\n",
      "Iteration 466, loss = 0.18689033\n",
      "Iteration 467, loss = 0.18732593\n",
      "Iteration 468, loss = 0.18803544\n",
      "Iteration 469, loss = 0.18762608\n",
      "Iteration 470, loss = 0.18812194\n",
      "Iteration 471, loss = 0.18773827\n",
      "Iteration 472, loss = 0.18686195\n",
      "Iteration 473, loss = 0.18648523\n",
      "Iteration 474, loss = 0.18739994\n",
      "Iteration 475, loss = 0.18636138\n",
      "Iteration 476, loss = 0.18674500\n",
      "Iteration 477, loss = 0.18645505\n",
      "Iteration 478, loss = 0.18668530\n",
      "Iteration 479, loss = 0.18615720\n",
      "Iteration 480, loss = 0.18664075\n",
      "Iteration 481, loss = 0.18652409\n",
      "Iteration 482, loss = 0.18615960\n",
      "Iteration 483, loss = 0.18553050\n",
      "Iteration 484, loss = 0.18635382\n",
      "Iteration 485, loss = 0.18599637\n",
      "Iteration 486, loss = 0.18599777\n",
      "Iteration 487, loss = 0.18596717\n",
      "Iteration 488, loss = 0.18586666\n",
      "Iteration 489, loss = 0.18592982\n",
      "Iteration 490, loss = 0.18636585\n",
      "Iteration 491, loss = 0.18587721\n",
      "Iteration 492, loss = 0.18555890\n",
      "Iteration 493, loss = 0.18598680\n",
      "Iteration 494, loss = 0.18562627\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77070320\n",
      "Iteration 2, loss = 0.74458445\n",
      "Iteration 3, loss = 0.72257616\n",
      "Iteration 4, loss = 0.70523312\n",
      "Iteration 5, loss = 0.69052394\n",
      "Iteration 6, loss = 0.67630563\n",
      "Iteration 7, loss = 0.66086070\n",
      "Iteration 8, loss = 0.64324622\n",
      "Iteration 9, loss = 0.62352494\n",
      "Iteration 10, loss = 0.60340926\n",
      "Iteration 11, loss = 0.58382889\n",
      "Iteration 12, loss = 0.56615349\n",
      "Iteration 13, loss = 0.55086450\n",
      "Iteration 14, loss = 0.53710310\n",
      "Iteration 15, loss = 0.52506965\n",
      "Iteration 16, loss = 0.51491320\n",
      "Iteration 17, loss = 0.50592479\n",
      "Iteration 18, loss = 0.49801304\n",
      "Iteration 19, loss = 0.49107928\n",
      "Iteration 20, loss = 0.48481032\n",
      "Iteration 21, loss = 0.47851768\n",
      "Iteration 22, loss = 0.47292555\n",
      "Iteration 23, loss = 0.46758393\n",
      "Iteration 24, loss = 0.46264762\n",
      "Iteration 25, loss = 0.45781150\n",
      "Iteration 26, loss = 0.45311324\n",
      "Iteration 27, loss = 0.44893559\n",
      "Iteration 28, loss = 0.44489870\n",
      "Iteration 29, loss = 0.44088574\n",
      "Iteration 30, loss = 0.43698685\n",
      "Iteration 31, loss = 0.43333312\n",
      "Iteration 32, loss = 0.42965339\n",
      "Iteration 33, loss = 0.42607920\n",
      "Iteration 34, loss = 0.42265394\n",
      "Iteration 35, loss = 0.41944044\n",
      "Iteration 36, loss = 0.41613624\n",
      "Iteration 37, loss = 0.41300169\n",
      "Iteration 38, loss = 0.40973558\n",
      "Iteration 39, loss = 0.40673672\n",
      "Iteration 40, loss = 0.40356515\n",
      "Iteration 41, loss = 0.40060376\n",
      "Iteration 42, loss = 0.39773717\n",
      "Iteration 43, loss = 0.39484224\n",
      "Iteration 44, loss = 0.39214726\n",
      "Iteration 45, loss = 0.38961770\n",
      "Iteration 46, loss = 0.38684188\n",
      "Iteration 47, loss = 0.38444881\n",
      "Iteration 48, loss = 0.38178697\n",
      "Iteration 49, loss = 0.37925247\n",
      "Iteration 50, loss = 0.37691502\n",
      "Iteration 51, loss = 0.37456476\n",
      "Iteration 52, loss = 0.37201070\n",
      "Iteration 53, loss = 0.36985533\n",
      "Iteration 54, loss = 0.36744368\n",
      "Iteration 55, loss = 0.36536657\n",
      "Iteration 56, loss = 0.36317256\n",
      "Iteration 57, loss = 0.36094692\n",
      "Iteration 58, loss = 0.35907670\n",
      "Iteration 59, loss = 0.35687822\n",
      "Iteration 60, loss = 0.35485108\n",
      "Iteration 61, loss = 0.35287764\n",
      "Iteration 62, loss = 0.35099846\n",
      "Iteration 63, loss = 0.34903311\n",
      "Iteration 64, loss = 0.34722310\n",
      "Iteration 65, loss = 0.34530704\n",
      "Iteration 66, loss = 0.34352634\n",
      "Iteration 67, loss = 0.34165269\n",
      "Iteration 68, loss = 0.33982349\n",
      "Iteration 69, loss = 0.33808730\n",
      "Iteration 70, loss = 0.33631931\n",
      "Iteration 71, loss = 0.33461538\n",
      "Iteration 72, loss = 0.33310846\n",
      "Iteration 73, loss = 0.33140722\n",
      "Iteration 74, loss = 0.32957117\n",
      "Iteration 75, loss = 0.32802177\n",
      "Iteration 76, loss = 0.32645196\n",
      "Iteration 77, loss = 0.32469573\n",
      "Iteration 78, loss = 0.32303241\n",
      "Iteration 79, loss = 0.32157344\n",
      "Iteration 80, loss = 0.32000028\n",
      "Iteration 81, loss = 0.31838948\n",
      "Iteration 82, loss = 0.31681191\n",
      "Iteration 83, loss = 0.31537257\n",
      "Iteration 84, loss = 0.31383699\n",
      "Iteration 85, loss = 0.31227806\n",
      "Iteration 86, loss = 0.31071255\n",
      "Iteration 87, loss = 0.30925525\n",
      "Iteration 88, loss = 0.30798223\n",
      "Iteration 89, loss = 0.30633268\n",
      "Iteration 90, loss = 0.30566745\n",
      "Iteration 91, loss = 0.30356623\n",
      "Iteration 92, loss = 0.30239295\n",
      "Iteration 93, loss = 0.30131712\n",
      "Iteration 94, loss = 0.30001411\n",
      "Iteration 95, loss = 0.29856753\n",
      "Iteration 96, loss = 0.29742676\n",
      "Iteration 97, loss = 0.29625872\n",
      "Iteration 98, loss = 0.29508013\n",
      "Iteration 99, loss = 0.29387471\n",
      "Iteration 100, loss = 0.29257909\n",
      "Iteration 101, loss = 0.29163143\n",
      "Iteration 102, loss = 0.29077794\n",
      "Iteration 103, loss = 0.28985137\n",
      "Iteration 104, loss = 0.28842520\n",
      "Iteration 105, loss = 0.28746367\n",
      "Iteration 106, loss = 0.28634345\n",
      "Iteration 107, loss = 0.28486505\n",
      "Iteration 108, loss = 0.28395119\n",
      "Iteration 109, loss = 0.28350424\n",
      "Iteration 110, loss = 0.28210093\n",
      "Iteration 111, loss = 0.28109976\n",
      "Iteration 112, loss = 0.28003003\n",
      "Iteration 113, loss = 0.27964775\n",
      "Iteration 114, loss = 0.27814243\n",
      "Iteration 115, loss = 0.27731233\n",
      "Iteration 116, loss = 0.27647486\n",
      "Iteration 117, loss = 0.27646117\n",
      "Iteration 118, loss = 0.27465417\n",
      "Iteration 119, loss = 0.27402933\n",
      "Iteration 120, loss = 0.27286171\n",
      "Iteration 121, loss = 0.27233014\n",
      "Iteration 122, loss = 0.27152962\n",
      "Iteration 123, loss = 0.27059093\n",
      "Iteration 124, loss = 0.26967824\n",
      "Iteration 125, loss = 0.26881760\n",
      "Iteration 126, loss = 0.26796316\n",
      "Iteration 127, loss = 0.26735266\n",
      "Iteration 128, loss = 0.26668029\n",
      "Iteration 129, loss = 0.26593697\n",
      "Iteration 130, loss = 0.26494267\n",
      "Iteration 131, loss = 0.26431612\n",
      "Iteration 132, loss = 0.26366641\n",
      "Iteration 133, loss = 0.26269640\n",
      "Iteration 134, loss = 0.26258791\n",
      "Iteration 135, loss = 0.26183280\n",
      "Iteration 136, loss = 0.26122587\n",
      "Iteration 137, loss = 0.26050259\n",
      "Iteration 138, loss = 0.25976414\n",
      "Iteration 139, loss = 0.25901854\n",
      "Iteration 140, loss = 0.25849478\n",
      "Iteration 141, loss = 0.25779814\n",
      "Iteration 142, loss = 0.25717976\n",
      "Iteration 143, loss = 0.25687872\n",
      "Iteration 144, loss = 0.25604631\n",
      "Iteration 145, loss = 0.25567690\n",
      "Iteration 146, loss = 0.25509024\n",
      "Iteration 147, loss = 0.25446223\n",
      "Iteration 148, loss = 0.25383428\n",
      "Iteration 149, loss = 0.25348103\n",
      "Iteration 150, loss = 0.25313823\n",
      "Iteration 151, loss = 0.25242964\n",
      "Iteration 152, loss = 0.25193198\n",
      "Iteration 153, loss = 0.25183103\n",
      "Iteration 154, loss = 0.25088353\n",
      "Iteration 155, loss = 0.25018857\n",
      "Iteration 156, loss = 0.24970572\n",
      "Iteration 157, loss = 0.24884454\n",
      "Iteration 158, loss = 0.24867589\n",
      "Iteration 159, loss = 0.24811354\n",
      "Iteration 160, loss = 0.24750405\n",
      "Iteration 161, loss = 0.24687663\n",
      "Iteration 162, loss = 0.24642932\n",
      "Iteration 163, loss = 0.24586867\n",
      "Iteration 164, loss = 0.24547680\n",
      "Iteration 165, loss = 0.24495887\n",
      "Iteration 166, loss = 0.24469701\n",
      "Iteration 167, loss = 0.24426682\n",
      "Iteration 168, loss = 0.24346662\n",
      "Iteration 169, loss = 0.24280902\n",
      "Iteration 170, loss = 0.24268688\n",
      "Iteration 171, loss = 0.24214664\n",
      "Iteration 172, loss = 0.24191325\n",
      "Iteration 173, loss = 0.24116160\n",
      "Iteration 174, loss = 0.24089966\n",
      "Iteration 175, loss = 0.24006391\n",
      "Iteration 176, loss = 0.24002293\n",
      "Iteration 177, loss = 0.23938267\n",
      "Iteration 178, loss = 0.23917325\n",
      "Iteration 179, loss = 0.23867775\n",
      "Iteration 180, loss = 0.23842329\n",
      "Iteration 181, loss = 0.23752670\n",
      "Iteration 182, loss = 0.23761955\n",
      "Iteration 183, loss = 0.23695415\n",
      "Iteration 184, loss = 0.23645258\n",
      "Iteration 185, loss = 0.23629527\n",
      "Iteration 186, loss = 0.23610934\n",
      "Iteration 187, loss = 0.23544363\n",
      "Iteration 188, loss = 0.23579426\n",
      "Iteration 189, loss = 0.23553192\n",
      "Iteration 190, loss = 0.23467622\n",
      "Iteration 191, loss = 0.23429602\n",
      "Iteration 192, loss = 0.23355772\n",
      "Iteration 193, loss = 0.23365854\n",
      "Iteration 194, loss = 0.23320537\n",
      "Iteration 195, loss = 0.23258781\n",
      "Iteration 196, loss = 0.23238920\n",
      "Iteration 197, loss = 0.23200064\n",
      "Iteration 198, loss = 0.23192893\n",
      "Iteration 199, loss = 0.23164352\n",
      "Iteration 200, loss = 0.23099176\n",
      "Iteration 201, loss = 0.23116676\n",
      "Iteration 202, loss = 0.23051268\n",
      "Iteration 203, loss = 0.23038667\n",
      "Iteration 204, loss = 0.23029945\n",
      "Iteration 205, loss = 0.22998329\n",
      "Iteration 206, loss = 0.22993493\n",
      "Iteration 207, loss = 0.22907634\n",
      "Iteration 208, loss = 0.22873014\n",
      "Iteration 209, loss = 0.22866164\n",
      "Iteration 210, loss = 0.22814177\n",
      "Iteration 211, loss = 0.22802224\n",
      "Iteration 212, loss = 0.22814003\n",
      "Iteration 213, loss = 0.22794719\n",
      "Iteration 214, loss = 0.22730515\n",
      "Iteration 215, loss = 0.22739120\n",
      "Iteration 216, loss = 0.22662250\n",
      "Iteration 217, loss = 0.22652694\n",
      "Iteration 218, loss = 0.22668908\n",
      "Iteration 219, loss = 0.22590235\n",
      "Iteration 220, loss = 0.22586821\n",
      "Iteration 221, loss = 0.22551225\n",
      "Iteration 222, loss = 0.22598258\n",
      "Iteration 223, loss = 0.22521318\n",
      "Iteration 224, loss = 0.22515090\n",
      "Iteration 225, loss = 0.22461723\n",
      "Iteration 226, loss = 0.22452758\n",
      "Iteration 227, loss = 0.22431305\n",
      "Iteration 228, loss = 0.22397638\n",
      "Iteration 229, loss = 0.22454555\n",
      "Iteration 230, loss = 0.22413171\n",
      "Iteration 231, loss = 0.22386220\n",
      "Iteration 232, loss = 0.22315485\n",
      "Iteration 233, loss = 0.22275111\n",
      "Iteration 234, loss = 0.22247618\n",
      "Iteration 235, loss = 0.22252210\n",
      "Iteration 236, loss = 0.22235700\n",
      "Iteration 237, loss = 0.22203168\n",
      "Iteration 238, loss = 0.22209185\n",
      "Iteration 239, loss = 0.22187585\n",
      "Iteration 240, loss = 0.22180250\n",
      "Iteration 241, loss = 0.22210504\n",
      "Iteration 242, loss = 0.22119915\n",
      "Iteration 243, loss = 0.22082515\n",
      "Iteration 244, loss = 0.22087443\n",
      "Iteration 245, loss = 0.22057196\n",
      "Iteration 246, loss = 0.22082210\n",
      "Iteration 247, loss = 0.21984824\n",
      "Iteration 248, loss = 0.22037605\n",
      "Iteration 249, loss = 0.21967166\n",
      "Iteration 250, loss = 0.21978035\n",
      "Iteration 251, loss = 0.21947063\n",
      "Iteration 252, loss = 0.21928382\n",
      "Iteration 253, loss = 0.21891174\n",
      "Iteration 254, loss = 0.21912547\n",
      "Iteration 255, loss = 0.21861622\n",
      "Iteration 256, loss = 0.21816921\n",
      "Iteration 257, loss = 0.21813706\n",
      "Iteration 258, loss = 0.21784764\n",
      "Iteration 259, loss = 0.21796533\n",
      "Iteration 260, loss = 0.21734740\n",
      "Iteration 261, loss = 0.21746798\n",
      "Iteration 262, loss = 0.21726946\n",
      "Iteration 263, loss = 0.21740375\n",
      "Iteration 264, loss = 0.21673614\n",
      "Iteration 265, loss = 0.21680542\n",
      "Iteration 266, loss = 0.21633015\n",
      "Iteration 267, loss = 0.21654149\n",
      "Iteration 268, loss = 0.21623985\n",
      "Iteration 269, loss = 0.21593517\n",
      "Iteration 270, loss = 0.21582938\n",
      "Iteration 271, loss = 0.21579357\n",
      "Iteration 272, loss = 0.21560124\n",
      "Iteration 273, loss = 0.21539036\n",
      "Iteration 274, loss = 0.21536121\n",
      "Iteration 275, loss = 0.21507288\n",
      "Iteration 276, loss = 0.21497399\n",
      "Iteration 277, loss = 0.21467980\n",
      "Iteration 278, loss = 0.21481737\n",
      "Iteration 279, loss = 0.21464177\n",
      "Iteration 280, loss = 0.21403778\n",
      "Iteration 281, loss = 0.21415504\n",
      "Iteration 282, loss = 0.21424642\n",
      "Iteration 283, loss = 0.21488629\n",
      "Iteration 284, loss = 0.21381521\n",
      "Iteration 285, loss = 0.21402900\n",
      "Iteration 286, loss = 0.21382340\n",
      "Iteration 287, loss = 0.21300906\n",
      "Iteration 288, loss = 0.21357486\n",
      "Iteration 289, loss = 0.21311529\n",
      "Iteration 290, loss = 0.21273437\n",
      "Iteration 291, loss = 0.21268350\n",
      "Iteration 292, loss = 0.21222272\n",
      "Iteration 293, loss = 0.21225926\n",
      "Iteration 294, loss = 0.21189993\n",
      "Iteration 295, loss = 0.21207657\n",
      "Iteration 296, loss = 0.21152251\n",
      "Iteration 297, loss = 0.21146683\n",
      "Iteration 298, loss = 0.21151012\n",
      "Iteration 299, loss = 0.21108607\n",
      "Iteration 300, loss = 0.21099252\n",
      "Iteration 301, loss = 0.21077835\n",
      "Iteration 302, loss = 0.21060537\n",
      "Iteration 303, loss = 0.21042900\n",
      "Iteration 304, loss = 0.21032713\n",
      "Iteration 305, loss = 0.20996150\n",
      "Iteration 306, loss = 0.21005758\n",
      "Iteration 307, loss = 0.21001080\n",
      "Iteration 308, loss = 0.20963666\n",
      "Iteration 309, loss = 0.20992975\n",
      "Iteration 310, loss = 0.20965412\n",
      "Iteration 311, loss = 0.20916482\n",
      "Iteration 312, loss = 0.20911275\n",
      "Iteration 313, loss = 0.20931566\n",
      "Iteration 314, loss = 0.20917125\n",
      "Iteration 315, loss = 0.20895839\n",
      "Iteration 316, loss = 0.20881340\n",
      "Iteration 317, loss = 0.20847864\n",
      "Iteration 318, loss = 0.20833863\n",
      "Iteration 319, loss = 0.20849112\n",
      "Iteration 320, loss = 0.20889008\n",
      "Iteration 321, loss = 0.20876593\n",
      "Iteration 322, loss = 0.20775431\n",
      "Iteration 323, loss = 0.20774090\n",
      "Iteration 324, loss = 0.20805388\n",
      "Iteration 325, loss = 0.20752090\n",
      "Iteration 326, loss = 0.20753850\n",
      "Iteration 327, loss = 0.20717391\n",
      "Iteration 328, loss = 0.20766225\n",
      "Iteration 329, loss = 0.20810946\n",
      "Iteration 330, loss = 0.20728659\n",
      "Iteration 331, loss = 0.20775565\n",
      "Iteration 332, loss = 0.20675935\n",
      "Iteration 333, loss = 0.20741271\n",
      "Iteration 334, loss = 0.20728264\n",
      "Iteration 335, loss = 0.20656490\n",
      "Iteration 336, loss = 0.20655782\n",
      "Iteration 337, loss = 0.20647356\n",
      "Iteration 338, loss = 0.20659960\n",
      "Iteration 339, loss = 0.20630057\n",
      "Iteration 340, loss = 0.20623366\n",
      "Iteration 341, loss = 0.20563928\n",
      "Iteration 342, loss = 0.20600550\n",
      "Iteration 343, loss = 0.20561510\n",
      "Iteration 344, loss = 0.20569321\n",
      "Iteration 345, loss = 0.20573323\n",
      "Iteration 346, loss = 0.20558477\n",
      "Iteration 347, loss = 0.20550756\n",
      "Iteration 348, loss = 0.20520673\n",
      "Iteration 349, loss = 0.20510322\n",
      "Iteration 350, loss = 0.20496058\n",
      "Iteration 351, loss = 0.20480246\n",
      "Iteration 352, loss = 0.20465173\n",
      "Iteration 353, loss = 0.20491259\n",
      "Iteration 354, loss = 0.20497605\n",
      "Iteration 355, loss = 0.20474099\n",
      "Iteration 356, loss = 0.20467328\n",
      "Iteration 357, loss = 0.20452267\n",
      "Iteration 358, loss = 0.20458670\n",
      "Iteration 359, loss = 0.20491227\n",
      "Iteration 360, loss = 0.20440615\n",
      "Iteration 361, loss = 0.20401365\n",
      "Iteration 362, loss = 0.20392526\n",
      "Iteration 363, loss = 0.20440374\n",
      "Iteration 364, loss = 0.20437414\n",
      "Iteration 365, loss = 0.20460440\n",
      "Iteration 366, loss = 0.20372366\n",
      "Iteration 367, loss = 0.20348773\n",
      "Iteration 368, loss = 0.20399067\n",
      "Iteration 369, loss = 0.20346190\n",
      "Iteration 370, loss = 0.20331640\n",
      "Iteration 371, loss = 0.20348126\n",
      "Iteration 372, loss = 0.20438407\n",
      "Iteration 373, loss = 0.20388969\n",
      "Iteration 374, loss = 0.20291519\n",
      "Iteration 375, loss = 0.20330831\n",
      "Iteration 376, loss = 0.20294681\n",
      "Iteration 377, loss = 0.20285591\n",
      "Iteration 378, loss = 0.20289532\n",
      "Iteration 379, loss = 0.20275413\n",
      "Iteration 380, loss = 0.20322321\n",
      "Iteration 381, loss = 0.20298976\n",
      "Iteration 382, loss = 0.20264935\n",
      "Iteration 383, loss = 0.20288830\n",
      "Iteration 384, loss = 0.20258787\n",
      "Iteration 385, loss = 0.20230257\n",
      "Iteration 386, loss = 0.20238125\n",
      "Iteration 387, loss = 0.20240108\n",
      "Iteration 388, loss = 0.20232743\n",
      "Iteration 389, loss = 0.20207976\n",
      "Iteration 390, loss = 0.20195976\n",
      "Iteration 391, loss = 0.20206718\n",
      "Iteration 392, loss = 0.20187133\n",
      "Iteration 393, loss = 0.20191579\n",
      "Iteration 394, loss = 0.20181012\n",
      "Iteration 395, loss = 0.20169942\n",
      "Iteration 396, loss = 0.20152545\n",
      "Iteration 397, loss = 0.20182813\n",
      "Iteration 398, loss = 0.20171115\n",
      "Iteration 399, loss = 0.20174997\n",
      "Iteration 400, loss = 0.20139088\n",
      "Iteration 401, loss = 0.20133652\n",
      "Iteration 402, loss = 0.20121358\n",
      "Iteration 403, loss = 0.20099629\n",
      "Iteration 404, loss = 0.20150839\n",
      "Iteration 405, loss = 0.20100236\n",
      "Iteration 406, loss = 0.20120218\n",
      "Iteration 407, loss = 0.20108068\n",
      "Iteration 408, loss = 0.20091708\n",
      "Iteration 409, loss = 0.20140986\n",
      "Iteration 410, loss = 0.20077433\n",
      "Iteration 411, loss = 0.20067142\n",
      "Iteration 412, loss = 0.20058339\n",
      "Iteration 413, loss = 0.20078664\n",
      "Iteration 414, loss = 0.20084823\n",
      "Iteration 415, loss = 0.20083162\n",
      "Iteration 416, loss = 0.20056558\n",
      "Iteration 417, loss = 0.20067164\n",
      "Iteration 418, loss = 0.20037454\n",
      "Iteration 419, loss = 0.20053586\n",
      "Iteration 420, loss = 0.20068226\n",
      "Iteration 421, loss = 0.20056742\n",
      "Iteration 422, loss = 0.20040539\n",
      "Iteration 423, loss = 0.20085769\n",
      "Iteration 424, loss = 0.20017327\n",
      "Iteration 425, loss = 0.20012674\n",
      "Iteration 426, loss = 0.20081543\n",
      "Iteration 427, loss = 0.20008509\n",
      "Iteration 428, loss = 0.20056997\n",
      "Iteration 429, loss = 0.20048426\n",
      "Iteration 430, loss = 0.19983236\n",
      "Iteration 431, loss = 0.20000076\n",
      "Iteration 432, loss = 0.19994604\n",
      "Iteration 433, loss = 0.20021479\n",
      "Iteration 434, loss = 0.20017782\n",
      "Iteration 435, loss = 0.20049983\n",
      "Iteration 436, loss = 0.19982926\n",
      "Iteration 437, loss = 0.19946556\n",
      "Iteration 438, loss = 0.19939008\n",
      "Iteration 439, loss = 0.19937774\n",
      "Iteration 440, loss = 0.19947616\n",
      "Iteration 441, loss = 0.19963866\n",
      "Iteration 442, loss = 0.19979481\n",
      "Iteration 443, loss = 0.20032071\n",
      "Iteration 444, loss = 0.19935421\n",
      "Iteration 445, loss = 0.19926256\n",
      "Iteration 446, loss = 0.19900050\n",
      "Iteration 447, loss = 0.19923446\n",
      "Iteration 448, loss = 0.19922330\n",
      "Iteration 449, loss = 0.19928978\n",
      "Iteration 450, loss = 0.19905712\n",
      "Iteration 451, loss = 0.19914712\n",
      "Iteration 452, loss = 0.19881567\n",
      "Iteration 453, loss = 0.19901368\n",
      "Iteration 454, loss = 0.19882523\n",
      "Iteration 455, loss = 0.19925621\n",
      "Iteration 456, loss = 0.19862936\n",
      "Iteration 457, loss = 0.19908623\n",
      "Iteration 458, loss = 0.19842328\n",
      "Iteration 459, loss = 0.19870869\n",
      "Iteration 460, loss = 0.19855862\n",
      "Iteration 461, loss = 0.19828259\n",
      "Iteration 462, loss = 0.19864001\n",
      "Iteration 463, loss = 0.19825980\n",
      "Iteration 464, loss = 0.19848782\n",
      "Iteration 465, loss = 0.19858582\n",
      "Iteration 466, loss = 0.19807139\n",
      "Iteration 467, loss = 0.19792825\n",
      "Iteration 468, loss = 0.19853747\n",
      "Iteration 469, loss = 0.19829972\n",
      "Iteration 470, loss = 0.19812126\n",
      "Iteration 471, loss = 0.19776783\n",
      "Iteration 472, loss = 0.19784110\n",
      "Iteration 473, loss = 0.19777988\n",
      "Iteration 474, loss = 0.19752630\n",
      "Iteration 475, loss = 0.19760676\n",
      "Iteration 476, loss = 0.19753060\n",
      "Iteration 477, loss = 0.19806562\n",
      "Iteration 478, loss = 0.19745798\n",
      "Iteration 479, loss = 0.19753345\n",
      "Iteration 480, loss = 0.19731832\n",
      "Iteration 481, loss = 0.19748787\n",
      "Iteration 482, loss = 0.19754583\n",
      "Iteration 483, loss = 0.19734447\n",
      "Iteration 484, loss = 0.19718133\n",
      "Iteration 485, loss = 0.19701309\n",
      "Iteration 486, loss = 0.19687098\n",
      "Iteration 487, loss = 0.19805288\n",
      "Iteration 488, loss = 0.19744307\n",
      "Iteration 489, loss = 0.19698475\n",
      "Iteration 490, loss = 0.19671690\n",
      "Iteration 491, loss = 0.19714410\n",
      "Iteration 492, loss = 0.19665819\n",
      "Iteration 493, loss = 0.19667673\n",
      "Iteration 494, loss = 0.19650633\n",
      "Iteration 495, loss = 0.19663588\n",
      "Iteration 496, loss = 0.19681783\n",
      "Iteration 497, loss = 0.19654485\n",
      "Iteration 498, loss = 0.19639822\n",
      "Iteration 499, loss = 0.19656764\n",
      "Iteration 500, loss = 0.19633450\n",
      "Iteration 501, loss = 0.19650694\n",
      "Iteration 502, loss = 0.19737111\n",
      "Iteration 503, loss = 0.19609969\n",
      "Iteration 504, loss = 0.19611495\n",
      "Iteration 505, loss = 0.19611764\n",
      "Iteration 506, loss = 0.19698493\n",
      "Iteration 507, loss = 0.19746061\n",
      "Iteration 508, loss = 0.19572236\n",
      "Iteration 509, loss = 0.19645873\n",
      "Iteration 510, loss = 0.19630875\n",
      "Iteration 511, loss = 0.19714624\n",
      "Iteration 512, loss = 0.19573331\n",
      "Iteration 513, loss = 0.19574806\n",
      "Iteration 514, loss = 0.19576138\n",
      "Iteration 515, loss = 0.19568406\n",
      "Iteration 516, loss = 0.19554886\n",
      "Iteration 517, loss = 0.19558084\n",
      "Iteration 518, loss = 0.19546919\n",
      "Iteration 519, loss = 0.19546788\n",
      "Iteration 520, loss = 0.19568072\n",
      "Iteration 521, loss = 0.19546984\n",
      "Iteration 522, loss = 0.19523593\n",
      "Iteration 523, loss = 0.19525286\n",
      "Iteration 524, loss = 0.19512786\n",
      "Iteration 525, loss = 0.19517037\n",
      "Iteration 526, loss = 0.19497878\n",
      "Iteration 527, loss = 0.19494072\n",
      "Iteration 528, loss = 0.19505761\n",
      "Iteration 529, loss = 0.19469721\n",
      "Iteration 530, loss = 0.19457125\n",
      "Iteration 531, loss = 0.19473728\n",
      "Iteration 532, loss = 0.19445481\n",
      "Iteration 533, loss = 0.19420681\n",
      "Iteration 534, loss = 0.19412557\n",
      "Iteration 535, loss = 0.19379558\n",
      "Iteration 536, loss = 0.19417755\n",
      "Iteration 537, loss = 0.19420567\n",
      "Iteration 538, loss = 0.19415646\n",
      "Iteration 539, loss = 0.19401257\n",
      "Iteration 540, loss = 0.19373875\n",
      "Iteration 541, loss = 0.19403557\n",
      "Iteration 542, loss = 0.19384492\n",
      "Iteration 543, loss = 0.19349491\n",
      "Iteration 544, loss = 0.19343392\n",
      "Iteration 545, loss = 0.19325129\n",
      "Iteration 546, loss = 0.19382777\n",
      "Iteration 547, loss = 0.19318798\n",
      "Iteration 548, loss = 0.19345827\n",
      "Iteration 549, loss = 0.19358145\n",
      "Iteration 550, loss = 0.19295477\n",
      "Iteration 551, loss = 0.19290946\n",
      "Iteration 552, loss = 0.19337959\n",
      "Iteration 553, loss = 0.19266800\n",
      "Iteration 554, loss = 0.19318789\n",
      "Iteration 555, loss = 0.19231235\n",
      "Iteration 556, loss = 0.19254728\n",
      "Iteration 557, loss = 0.19250233\n",
      "Iteration 558, loss = 0.19251662\n",
      "Iteration 559, loss = 0.19199679\n",
      "Iteration 560, loss = 0.19209465\n",
      "Iteration 561, loss = 0.19169972\n",
      "Iteration 562, loss = 0.19158687\n",
      "Iteration 563, loss = 0.19180518\n",
      "Iteration 564, loss = 0.19151799\n",
      "Iteration 565, loss = 0.19151581\n",
      "Iteration 566, loss = 0.19133010\n",
      "Iteration 567, loss = 0.19117986\n",
      "Iteration 568, loss = 0.19171556\n",
      "Iteration 569, loss = 0.19114581\n",
      "Iteration 570, loss = 0.19125650\n",
      "Iteration 571, loss = 0.19119984\n",
      "Iteration 572, loss = 0.19187809\n",
      "Iteration 573, loss = 0.19026127\n",
      "Iteration 574, loss = 0.19236469\n",
      "Iteration 575, loss = 0.19117640\n",
      "Iteration 576, loss = 0.19088667\n",
      "Iteration 577, loss = 0.19048391\n",
      "Iteration 578, loss = 0.19064254\n",
      "Iteration 579, loss = 0.19060117\n",
      "Iteration 580, loss = 0.18999722\n",
      "Iteration 581, loss = 0.18982274\n",
      "Iteration 582, loss = 0.19011433\n",
      "Iteration 583, loss = 0.18951819\n",
      "Iteration 584, loss = 0.18958269\n",
      "Iteration 585, loss = 0.18985640\n",
      "Iteration 586, loss = 0.18994745\n",
      "Iteration 587, loss = 0.18939674\n",
      "Iteration 588, loss = 0.18947020\n",
      "Iteration 589, loss = 0.18897101\n",
      "Iteration 590, loss = 0.18954420\n",
      "Iteration 591, loss = 0.18938445\n",
      "Iteration 592, loss = 0.18913282\n",
      "Iteration 593, loss = 0.18904296\n",
      "Iteration 594, loss = 0.18843297\n",
      "Iteration 595, loss = 0.18899803\n",
      "Iteration 596, loss = 0.18912741\n",
      "Iteration 597, loss = 0.18809754\n",
      "Iteration 598, loss = 0.18797513\n",
      "Iteration 599, loss = 0.18855166\n",
      "Iteration 600, loss = 0.18801542\n",
      "Iteration 601, loss = 0.18872210\n",
      "Iteration 602, loss = 0.18888510\n",
      "Iteration 603, loss = 0.18754781\n",
      "Iteration 604, loss = 0.18766415\n",
      "Iteration 605, loss = 0.18718647\n",
      "Iteration 606, loss = 0.18765791\n",
      "Iteration 607, loss = 0.18745023\n",
      "Iteration 608, loss = 0.18704419\n",
      "Iteration 609, loss = 0.18772322\n",
      "Iteration 610, loss = 0.18686820\n",
      "Iteration 611, loss = 0.18673422\n",
      "Iteration 612, loss = 0.18699073\n",
      "Iteration 613, loss = 0.18635311\n",
      "Iteration 614, loss = 0.18638947\n",
      "Iteration 615, loss = 0.18636946\n",
      "Iteration 616, loss = 0.18619972\n",
      "Iteration 617, loss = 0.18608868\n",
      "Iteration 618, loss = 0.18610003\n",
      "Iteration 619, loss = 0.18597139\n",
      "Iteration 620, loss = 0.18622958\n",
      "Iteration 621, loss = 0.18563290\n",
      "Iteration 622, loss = 0.18515941\n",
      "Iteration 623, loss = 0.18460678\n",
      "Iteration 624, loss = 0.18475528\n",
      "Iteration 625, loss = 0.18467646\n",
      "Iteration 626, loss = 0.18409166\n",
      "Iteration 627, loss = 0.18394534\n",
      "Iteration 628, loss = 0.18363642\n",
      "Iteration 629, loss = 0.18367653\n",
      "Iteration 630, loss = 0.18312033\n",
      "Iteration 631, loss = 0.18328501\n",
      "Iteration 632, loss = 0.18319226\n",
      "Iteration 633, loss = 0.18267471\n",
      "Iteration 634, loss = 0.18274382\n",
      "Iteration 635, loss = 0.18280118\n",
      "Iteration 636, loss = 0.18233746\n",
      "Iteration 637, loss = 0.18222391\n",
      "Iteration 638, loss = 0.18240159\n",
      "Iteration 639, loss = 0.18319305\n",
      "Iteration 640, loss = 0.18195941\n",
      "Iteration 641, loss = 0.18180158\n",
      "Iteration 642, loss = 0.18179914\n",
      "Iteration 643, loss = 0.18139055\n",
      "Iteration 644, loss = 0.18161023\n",
      "Iteration 645, loss = 0.18168358\n",
      "Iteration 646, loss = 0.18327584\n",
      "Iteration 647, loss = 0.18131310\n",
      "Iteration 648, loss = 0.18221186\n",
      "Iteration 649, loss = 0.18112000\n",
      "Iteration 650, loss = 0.18146740\n",
      "Iteration 651, loss = 0.18195743\n",
      "Iteration 652, loss = 0.18050717\n",
      "Iteration 653, loss = 0.18070328\n",
      "Iteration 654, loss = 0.18076681\n",
      "Iteration 655, loss = 0.18009653\n",
      "Iteration 656, loss = 0.18035995\n",
      "Iteration 657, loss = 0.17999943\n",
      "Iteration 658, loss = 0.18015045\n",
      "Iteration 659, loss = 0.17991411\n",
      "Iteration 660, loss = 0.17963786\n",
      "Iteration 661, loss = 0.17982980\n",
      "Iteration 662, loss = 0.17988094\n",
      "Iteration 663, loss = 0.17973036\n",
      "Iteration 664, loss = 0.17951539\n",
      "Iteration 665, loss = 0.17913722\n",
      "Iteration 666, loss = 0.17905020\n",
      "Iteration 667, loss = 0.17929122\n",
      "Iteration 668, loss = 0.17918916\n",
      "Iteration 669, loss = 0.17887425\n",
      "Iteration 670, loss = 0.17857403\n",
      "Iteration 671, loss = 0.17905698\n",
      "Iteration 672, loss = 0.17965958\n",
      "Iteration 673, loss = 0.17873618\n",
      "Iteration 674, loss = 0.17846617\n",
      "Iteration 675, loss = 0.17860467\n",
      "Iteration 676, loss = 0.17835085\n",
      "Iteration 677, loss = 0.17919380\n",
      "Iteration 678, loss = 0.17808673\n",
      "Iteration 679, loss = 0.17805538\n",
      "Iteration 680, loss = 0.17788635\n",
      "Iteration 681, loss = 0.17797850\n",
      "Iteration 682, loss = 0.17768132\n",
      "Iteration 683, loss = 0.17774508\n",
      "Iteration 684, loss = 0.17762693\n",
      "Iteration 685, loss = 0.17778199\n",
      "Iteration 686, loss = 0.17752319\n",
      "Iteration 687, loss = 0.17800853\n",
      "Iteration 688, loss = 0.17752949\n",
      "Iteration 689, loss = 0.17715617\n",
      "Iteration 690, loss = 0.17739477\n",
      "Iteration 691, loss = 0.17751734\n",
      "Iteration 692, loss = 0.17699592\n",
      "Iteration 693, loss = 0.17729820\n",
      "Iteration 694, loss = 0.17742566\n",
      "Iteration 695, loss = 0.17698708\n",
      "Iteration 696, loss = 0.17721976\n",
      "Iteration 697, loss = 0.17664716\n",
      "Iteration 698, loss = 0.17699669\n",
      "Iteration 699, loss = 0.17774578\n",
      "Iteration 700, loss = 0.17649257\n",
      "Iteration 701, loss = 0.17656003\n",
      "Iteration 702, loss = 0.17639133\n",
      "Iteration 703, loss = 0.17670156\n",
      "Iteration 704, loss = 0.17658732\n",
      "Iteration 705, loss = 0.17674700\n",
      "Iteration 706, loss = 0.17696684\n",
      "Iteration 707, loss = 0.17714528\n",
      "Iteration 708, loss = 0.17783327\n",
      "Iteration 709, loss = 0.17633984\n",
      "Iteration 710, loss = 0.17653973\n",
      "Iteration 711, loss = 0.17601920\n",
      "Iteration 712, loss = 0.17574837\n",
      "Iteration 713, loss = 0.17606465\n",
      "Iteration 714, loss = 0.17690922\n",
      "Iteration 715, loss = 0.17532551\n",
      "Iteration 716, loss = 0.17606211\n",
      "Iteration 717, loss = 0.17573144\n",
      "Iteration 718, loss = 0.17550950\n",
      "Iteration 719, loss = 0.17575416\n",
      "Iteration 720, loss = 0.17562911\n",
      "Iteration 721, loss = 0.17528748\n",
      "Iteration 722, loss = 0.17564203\n",
      "Iteration 723, loss = 0.17583563\n",
      "Iteration 724, loss = 0.17546879\n",
      "Iteration 725, loss = 0.17670137\n",
      "Iteration 726, loss = 0.17520328\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77081723\n",
      "Iteration 2, loss = 0.74412824\n",
      "Iteration 3, loss = 0.72156421\n",
      "Iteration 4, loss = 0.70406224\n",
      "Iteration 5, loss = 0.68970430\n",
      "Iteration 6, loss = 0.67671257\n",
      "Iteration 7, loss = 0.66265884\n",
      "Iteration 8, loss = 0.64709054\n",
      "Iteration 9, loss = 0.62870654\n",
      "Iteration 10, loss = 0.60998980\n",
      "Iteration 11, loss = 0.59016977\n",
      "Iteration 12, loss = 0.57229406\n",
      "Iteration 13, loss = 0.55572900\n",
      "Iteration 14, loss = 0.54174410\n",
      "Iteration 15, loss = 0.52963468\n",
      "Iteration 16, loss = 0.51923139\n",
      "Iteration 17, loss = 0.50967213\n",
      "Iteration 18, loss = 0.50150146\n",
      "Iteration 19, loss = 0.49434264\n",
      "Iteration 20, loss = 0.48754522\n",
      "Iteration 21, loss = 0.48127994\n",
      "Iteration 22, loss = 0.47542721\n",
      "Iteration 23, loss = 0.46980742\n",
      "Iteration 24, loss = 0.46477619\n",
      "Iteration 25, loss = 0.45983084\n",
      "Iteration 26, loss = 0.45532255\n",
      "Iteration 27, loss = 0.45108485\n",
      "Iteration 28, loss = 0.44692330\n",
      "Iteration 29, loss = 0.44290792\n",
      "Iteration 30, loss = 0.43927066\n",
      "Iteration 31, loss = 0.43530186\n",
      "Iteration 32, loss = 0.43176664\n",
      "Iteration 33, loss = 0.42828886\n",
      "Iteration 34, loss = 0.42470023\n",
      "Iteration 35, loss = 0.42144975\n",
      "Iteration 36, loss = 0.41832580\n",
      "Iteration 37, loss = 0.41489246\n",
      "Iteration 38, loss = 0.41198297\n",
      "Iteration 39, loss = 0.40884795\n",
      "Iteration 40, loss = 0.40582022\n",
      "Iteration 41, loss = 0.40267474\n",
      "Iteration 42, loss = 0.39987887\n",
      "Iteration 43, loss = 0.39693507\n",
      "Iteration 44, loss = 0.39430284\n",
      "Iteration 45, loss = 0.39155130\n",
      "Iteration 46, loss = 0.38888500\n",
      "Iteration 47, loss = 0.38632143\n",
      "Iteration 48, loss = 0.38390044\n",
      "Iteration 49, loss = 0.38146368\n",
      "Iteration 50, loss = 0.37902672\n",
      "Iteration 51, loss = 0.37679671\n",
      "Iteration 52, loss = 0.37437990\n",
      "Iteration 53, loss = 0.37193316\n",
      "Iteration 54, loss = 0.36983911\n",
      "Iteration 55, loss = 0.36743930\n",
      "Iteration 56, loss = 0.36533024\n",
      "Iteration 57, loss = 0.36322532\n",
      "Iteration 58, loss = 0.36105469\n",
      "Iteration 59, loss = 0.35918170\n",
      "Iteration 60, loss = 0.35700311\n",
      "Iteration 61, loss = 0.35514422\n",
      "Iteration 62, loss = 0.35317357\n",
      "Iteration 63, loss = 0.35092387\n",
      "Iteration 64, loss = 0.34940158\n",
      "Iteration 65, loss = 0.34750929\n",
      "Iteration 66, loss = 0.34555418\n",
      "Iteration 67, loss = 0.34381696\n",
      "Iteration 68, loss = 0.34195928\n",
      "Iteration 69, loss = 0.34012650\n",
      "Iteration 70, loss = 0.33847312\n",
      "Iteration 71, loss = 0.33671482\n",
      "Iteration 72, loss = 0.33507233\n",
      "Iteration 73, loss = 0.33339463\n",
      "Iteration 74, loss = 0.33204988\n",
      "Iteration 75, loss = 0.33021457\n",
      "Iteration 76, loss = 0.32885264\n",
      "Iteration 77, loss = 0.32697263\n",
      "Iteration 78, loss = 0.32575046\n",
      "Iteration 79, loss = 0.32391987\n",
      "Iteration 80, loss = 0.32228531\n",
      "Iteration 81, loss = 0.32069126\n",
      "Iteration 82, loss = 0.31921814\n",
      "Iteration 83, loss = 0.31764859\n",
      "Iteration 84, loss = 0.31640229\n",
      "Iteration 85, loss = 0.31474277\n",
      "Iteration 86, loss = 0.31356467\n",
      "Iteration 87, loss = 0.31182975\n",
      "Iteration 88, loss = 0.31133948\n",
      "Iteration 89, loss = 0.30930430\n",
      "Iteration 90, loss = 0.30801654\n",
      "Iteration 91, loss = 0.30698603\n",
      "Iteration 92, loss = 0.30593927\n",
      "Iteration 93, loss = 0.30424064\n",
      "Iteration 94, loss = 0.30349215\n",
      "Iteration 95, loss = 0.30205201\n",
      "Iteration 96, loss = 0.30142946\n",
      "Iteration 97, loss = 0.30015246\n",
      "Iteration 98, loss = 0.29861848\n",
      "Iteration 99, loss = 0.29742643\n",
      "Iteration 100, loss = 0.29632624\n",
      "Iteration 101, loss = 0.29579802\n",
      "Iteration 102, loss = 0.29432029\n",
      "Iteration 103, loss = 0.29306419\n",
      "Iteration 104, loss = 0.29198901\n",
      "Iteration 105, loss = 0.29116469\n",
      "Iteration 106, loss = 0.29020164\n",
      "Iteration 107, loss = 0.28927148\n",
      "Iteration 108, loss = 0.28829876\n",
      "Iteration 109, loss = 0.28751905\n",
      "Iteration 110, loss = 0.28624487\n",
      "Iteration 111, loss = 0.28530891\n",
      "Iteration 112, loss = 0.28438925\n",
      "Iteration 113, loss = 0.28329386\n",
      "Iteration 114, loss = 0.28257924\n",
      "Iteration 115, loss = 0.28154470\n",
      "Iteration 116, loss = 0.28078127\n",
      "Iteration 117, loss = 0.27963204\n",
      "Iteration 118, loss = 0.27895158\n",
      "Iteration 119, loss = 0.27793661\n",
      "Iteration 120, loss = 0.27735681\n",
      "Iteration 121, loss = 0.27627721\n",
      "Iteration 122, loss = 0.27558384\n",
      "Iteration 123, loss = 0.27466156\n",
      "Iteration 124, loss = 0.27382434\n",
      "Iteration 125, loss = 0.27291896\n",
      "Iteration 126, loss = 0.27201822\n",
      "Iteration 127, loss = 0.27159964\n",
      "Iteration 128, loss = 0.27167105\n",
      "Iteration 129, loss = 0.27016832\n",
      "Iteration 130, loss = 0.26910814\n",
      "Iteration 131, loss = 0.26838809\n",
      "Iteration 132, loss = 0.26759342\n",
      "Iteration 133, loss = 0.26672244\n",
      "Iteration 134, loss = 0.26622805\n",
      "Iteration 135, loss = 0.26540403\n",
      "Iteration 136, loss = 0.26484272\n",
      "Iteration 137, loss = 0.26401568\n",
      "Iteration 138, loss = 0.26358040\n",
      "Iteration 139, loss = 0.26285161\n",
      "Iteration 140, loss = 0.26219808\n",
      "Iteration 141, loss = 0.26150649\n",
      "Iteration 142, loss = 0.26075607\n",
      "Iteration 143, loss = 0.26019913\n",
      "Iteration 144, loss = 0.25977638\n",
      "Iteration 145, loss = 0.25897419\n",
      "Iteration 146, loss = 0.25866379\n",
      "Iteration 147, loss = 0.25794520\n",
      "Iteration 148, loss = 0.25729393\n",
      "Iteration 149, loss = 0.25681227\n",
      "Iteration 150, loss = 0.25606451\n",
      "Iteration 151, loss = 0.25594347\n",
      "Iteration 152, loss = 0.25513689\n",
      "Iteration 153, loss = 0.25445091\n",
      "Iteration 154, loss = 0.25488454\n",
      "Iteration 155, loss = 0.25349819\n",
      "Iteration 156, loss = 0.25285123\n",
      "Iteration 157, loss = 0.25286073\n",
      "Iteration 158, loss = 0.25184651\n",
      "Iteration 159, loss = 0.25151030\n",
      "Iteration 160, loss = 0.25129101\n",
      "Iteration 161, loss = 0.25042124\n",
      "Iteration 162, loss = 0.25007706\n",
      "Iteration 163, loss = 0.24947515\n",
      "Iteration 164, loss = 0.24890326\n",
      "Iteration 165, loss = 0.24853477\n",
      "Iteration 166, loss = 0.24815555\n",
      "Iteration 167, loss = 0.24745901\n",
      "Iteration 168, loss = 0.24725811\n",
      "Iteration 169, loss = 0.24656894\n",
      "Iteration 170, loss = 0.24637229\n",
      "Iteration 171, loss = 0.24575735\n",
      "Iteration 172, loss = 0.24531184\n",
      "Iteration 173, loss = 0.24485070\n",
      "Iteration 174, loss = 0.24486096\n",
      "Iteration 175, loss = 0.24431315\n",
      "Iteration 176, loss = 0.24431026\n",
      "Iteration 177, loss = 0.24323785\n",
      "Iteration 178, loss = 0.24292044\n",
      "Iteration 179, loss = 0.24258884\n",
      "Iteration 180, loss = 0.24213800\n",
      "Iteration 181, loss = 0.24156913\n",
      "Iteration 182, loss = 0.24187067\n",
      "Iteration 183, loss = 0.24090124\n",
      "Iteration 184, loss = 0.24073616\n",
      "Iteration 185, loss = 0.24000699\n",
      "Iteration 186, loss = 0.23980486\n",
      "Iteration 187, loss = 0.23939757\n",
      "Iteration 188, loss = 0.23898367\n",
      "Iteration 189, loss = 0.23888242\n",
      "Iteration 190, loss = 0.23826826\n",
      "Iteration 191, loss = 0.23739977\n",
      "Iteration 192, loss = 0.23737086\n",
      "Iteration 193, loss = 0.23646480\n",
      "Iteration 194, loss = 0.23580928\n",
      "Iteration 195, loss = 0.23483657\n",
      "Iteration 196, loss = 0.23455366\n",
      "Iteration 197, loss = 0.23405735\n",
      "Iteration 198, loss = 0.23335694\n",
      "Iteration 199, loss = 0.23339518\n",
      "Iteration 200, loss = 0.23319401\n",
      "Iteration 201, loss = 0.23263684\n",
      "Iteration 202, loss = 0.23209459\n",
      "Iteration 203, loss = 0.23107841\n",
      "Iteration 204, loss = 0.23107649\n",
      "Iteration 205, loss = 0.23097243\n",
      "Iteration 206, loss = 0.23022990\n",
      "Iteration 207, loss = 0.22994393\n",
      "Iteration 208, loss = 0.22930939\n",
      "Iteration 209, loss = 0.22895303\n",
      "Iteration 210, loss = 0.22844414\n",
      "Iteration 211, loss = 0.22856336\n",
      "Iteration 212, loss = 0.22819918\n",
      "Iteration 213, loss = 0.22770781\n",
      "Iteration 214, loss = 0.22719807\n",
      "Iteration 215, loss = 0.22747801\n",
      "Iteration 216, loss = 0.22684442\n",
      "Iteration 217, loss = 0.22644285\n",
      "Iteration 218, loss = 0.22595711\n",
      "Iteration 219, loss = 0.22568232\n",
      "Iteration 220, loss = 0.22570539\n",
      "Iteration 221, loss = 0.22506556\n",
      "Iteration 222, loss = 0.22554500\n",
      "Iteration 223, loss = 0.22483206\n",
      "Iteration 224, loss = 0.22429169\n",
      "Iteration 225, loss = 0.22372982\n",
      "Iteration 226, loss = 0.22334051\n",
      "Iteration 227, loss = 0.22307206\n",
      "Iteration 228, loss = 0.22265148\n",
      "Iteration 229, loss = 0.22253647\n",
      "Iteration 230, loss = 0.22187739\n",
      "Iteration 231, loss = 0.22190937\n",
      "Iteration 232, loss = 0.22133997\n",
      "Iteration 233, loss = 0.22113772\n",
      "Iteration 234, loss = 0.22090115\n",
      "Iteration 235, loss = 0.22029508\n",
      "Iteration 236, loss = 0.22002785\n",
      "Iteration 237, loss = 0.21978088\n",
      "Iteration 238, loss = 0.21934956\n",
      "Iteration 239, loss = 0.21942249\n",
      "Iteration 240, loss = 0.21908395\n",
      "Iteration 241, loss = 0.21894738\n",
      "Iteration 242, loss = 0.21805436\n",
      "Iteration 243, loss = 0.21759884\n",
      "Iteration 244, loss = 0.21748185\n",
      "Iteration 245, loss = 0.21702795\n",
      "Iteration 246, loss = 0.21721474\n",
      "Iteration 247, loss = 0.21637306\n",
      "Iteration 248, loss = 0.21602339\n",
      "Iteration 249, loss = 0.21553588\n",
      "Iteration 250, loss = 0.21572634\n",
      "Iteration 251, loss = 0.21526690\n",
      "Iteration 252, loss = 0.21495803\n",
      "Iteration 253, loss = 0.21463751\n",
      "Iteration 254, loss = 0.21415631\n",
      "Iteration 255, loss = 0.21414211\n",
      "Iteration 256, loss = 0.21371977\n",
      "Iteration 257, loss = 0.21392945\n",
      "Iteration 258, loss = 0.21306772\n",
      "Iteration 259, loss = 0.21345350\n",
      "Iteration 260, loss = 0.21354698\n",
      "Iteration 261, loss = 0.21260092\n",
      "Iteration 262, loss = 0.21209620\n",
      "Iteration 263, loss = 0.21249129\n",
      "Iteration 264, loss = 0.21287778\n",
      "Iteration 265, loss = 0.21133259\n",
      "Iteration 266, loss = 0.21115657\n",
      "Iteration 267, loss = 0.21088280\n",
      "Iteration 268, loss = 0.21173175\n",
      "Iteration 269, loss = 0.21075726\n",
      "Iteration 270, loss = 0.21095155\n",
      "Iteration 271, loss = 0.21033785\n",
      "Iteration 272, loss = 0.20976684\n",
      "Iteration 273, loss = 0.20997603\n",
      "Iteration 274, loss = 0.20960118\n",
      "Iteration 275, loss = 0.20916723\n",
      "Iteration 276, loss = 0.20881995\n",
      "Iteration 277, loss = 0.20876281\n",
      "Iteration 278, loss = 0.20873050\n",
      "Iteration 279, loss = 0.20846678\n",
      "Iteration 280, loss = 0.20810678\n",
      "Iteration 281, loss = 0.20777522\n",
      "Iteration 282, loss = 0.20736393\n",
      "Iteration 283, loss = 0.20735614\n",
      "Iteration 284, loss = 0.20697441\n",
      "Iteration 285, loss = 0.20657456\n",
      "Iteration 286, loss = 0.20676369\n",
      "Iteration 287, loss = 0.20632510\n",
      "Iteration 288, loss = 0.20652071\n",
      "Iteration 289, loss = 0.20586030\n",
      "Iteration 290, loss = 0.20744730\n",
      "Iteration 291, loss = 0.20542979\n",
      "Iteration 292, loss = 0.20624034\n",
      "Iteration 293, loss = 0.20539077\n",
      "Iteration 294, loss = 0.20517834\n",
      "Iteration 295, loss = 0.20491928\n",
      "Iteration 296, loss = 0.20466889\n",
      "Iteration 297, loss = 0.20454777\n",
      "Iteration 298, loss = 0.20457489\n",
      "Iteration 299, loss = 0.20524467\n",
      "Iteration 300, loss = 0.20392372\n",
      "Iteration 301, loss = 0.20388355\n",
      "Iteration 302, loss = 0.20385772\n",
      "Iteration 303, loss = 0.20347376\n",
      "Iteration 304, loss = 0.20340823\n",
      "Iteration 305, loss = 0.20358275\n",
      "Iteration 306, loss = 0.20340040\n",
      "Iteration 307, loss = 0.20381940\n",
      "Iteration 308, loss = 0.20224671\n",
      "Iteration 309, loss = 0.20269130\n",
      "Iteration 310, loss = 0.20256478\n",
      "Iteration 311, loss = 0.20222403\n",
      "Iteration 312, loss = 0.20207239\n",
      "Iteration 313, loss = 0.20184676\n",
      "Iteration 314, loss = 0.20146804\n",
      "Iteration 315, loss = 0.20147648\n",
      "Iteration 316, loss = 0.20196463\n",
      "Iteration 317, loss = 0.20062259\n",
      "Iteration 318, loss = 0.20196886\n",
      "Iteration 319, loss = 0.20092546\n",
      "Iteration 320, loss = 0.20129649\n",
      "Iteration 321, loss = 0.20105997\n",
      "Iteration 322, loss = 0.20044365\n",
      "Iteration 323, loss = 0.20014237\n",
      "Iteration 324, loss = 0.20045275\n",
      "Iteration 325, loss = 0.20005033\n",
      "Iteration 326, loss = 0.19948640\n",
      "Iteration 327, loss = 0.19913002\n",
      "Iteration 328, loss = 0.19906748\n",
      "Iteration 329, loss = 0.19918275\n",
      "Iteration 330, loss = 0.19879575\n",
      "Iteration 331, loss = 0.19859926\n",
      "Iteration 332, loss = 0.19821714\n",
      "Iteration 333, loss = 0.19916656\n",
      "Iteration 334, loss = 0.19802198\n",
      "Iteration 335, loss = 0.19769015\n",
      "Iteration 336, loss = 0.19806019\n",
      "Iteration 337, loss = 0.19777991\n",
      "Iteration 338, loss = 0.19758551\n",
      "Iteration 339, loss = 0.19702829\n",
      "Iteration 340, loss = 0.19696213\n",
      "Iteration 341, loss = 0.19725357\n",
      "Iteration 342, loss = 0.19665610\n",
      "Iteration 343, loss = 0.19678991\n",
      "Iteration 344, loss = 0.19656962\n",
      "Iteration 345, loss = 0.19643183\n",
      "Iteration 346, loss = 0.19607341\n",
      "Iteration 347, loss = 0.19624774\n",
      "Iteration 348, loss = 0.19560320\n",
      "Iteration 349, loss = 0.19542559\n",
      "Iteration 350, loss = 0.19520826\n",
      "Iteration 351, loss = 0.19487319\n",
      "Iteration 352, loss = 0.19513489\n",
      "Iteration 353, loss = 0.19495127\n",
      "Iteration 354, loss = 0.19463670\n",
      "Iteration 355, loss = 0.19471157\n",
      "Iteration 356, loss = 0.19435819\n",
      "Iteration 357, loss = 0.19410973\n",
      "Iteration 358, loss = 0.19383115\n",
      "Iteration 359, loss = 0.19395370\n",
      "Iteration 360, loss = 0.19374046\n",
      "Iteration 361, loss = 0.19366598\n",
      "Iteration 362, loss = 0.19352778\n",
      "Iteration 363, loss = 0.19351548\n",
      "Iteration 364, loss = 0.19398290\n",
      "Iteration 365, loss = 0.19317329\n",
      "Iteration 366, loss = 0.19293319\n",
      "Iteration 367, loss = 0.19339626\n",
      "Iteration 368, loss = 0.19309677\n",
      "Iteration 369, loss = 0.19293169\n",
      "Iteration 370, loss = 0.19288694\n",
      "Iteration 371, loss = 0.19249405\n",
      "Iteration 372, loss = 0.19225683\n",
      "Iteration 373, loss = 0.19186417\n",
      "Iteration 374, loss = 0.19216570\n",
      "Iteration 375, loss = 0.19195162\n",
      "Iteration 376, loss = 0.19167531\n",
      "Iteration 377, loss = 0.19297237\n",
      "Iteration 378, loss = 0.19315461\n",
      "Iteration 379, loss = 0.19107220\n",
      "Iteration 380, loss = 0.19139127\n",
      "Iteration 381, loss = 0.19131039\n",
      "Iteration 382, loss = 0.19139816\n",
      "Iteration 383, loss = 0.19092692\n",
      "Iteration 384, loss = 0.19097976\n",
      "Iteration 385, loss = 0.19101794\n",
      "Iteration 386, loss = 0.19098120\n",
      "Iteration 387, loss = 0.19067152\n",
      "Iteration 388, loss = 0.19068021\n",
      "Iteration 389, loss = 0.19061398\n",
      "Iteration 390, loss = 0.19022846\n",
      "Iteration 391, loss = 0.18993666\n",
      "Iteration 392, loss = 0.18981228\n",
      "Iteration 393, loss = 0.18983949\n",
      "Iteration 394, loss = 0.18978892\n",
      "Iteration 395, loss = 0.19002376\n",
      "Iteration 396, loss = 0.18989291\n",
      "Iteration 397, loss = 0.18955279\n",
      "Iteration 398, loss = 0.18988157\n",
      "Iteration 399, loss = 0.18924157\n",
      "Iteration 400, loss = 0.18914525\n",
      "Iteration 401, loss = 0.18963255\n",
      "Iteration 402, loss = 0.18903803\n",
      "Iteration 403, loss = 0.18919429\n",
      "Iteration 404, loss = 0.18901574\n",
      "Iteration 405, loss = 0.18885326\n",
      "Iteration 406, loss = 0.18887976\n",
      "Iteration 407, loss = 0.18935408\n",
      "Iteration 408, loss = 0.18862195\n",
      "Iteration 409, loss = 0.18821729\n",
      "Iteration 410, loss = 0.18811500\n",
      "Iteration 411, loss = 0.18793587\n",
      "Iteration 412, loss = 0.18787765\n",
      "Iteration 413, loss = 0.18783033\n",
      "Iteration 414, loss = 0.18817846\n",
      "Iteration 415, loss = 0.18764519\n",
      "Iteration 416, loss = 0.18731646\n",
      "Iteration 417, loss = 0.18801001\n",
      "Iteration 418, loss = 0.18768845\n",
      "Iteration 419, loss = 0.18714652\n",
      "Iteration 420, loss = 0.18736346\n",
      "Iteration 421, loss = 0.18793102\n",
      "Iteration 422, loss = 0.18709592\n",
      "Iteration 423, loss = 0.18686979\n",
      "Iteration 424, loss = 0.18847169\n",
      "Iteration 425, loss = 0.18698326\n",
      "Iteration 426, loss = 0.18802341\n",
      "Iteration 427, loss = 0.18635742\n",
      "Iteration 428, loss = 0.18667579\n",
      "Iteration 429, loss = 0.18654814\n",
      "Iteration 430, loss = 0.18645503\n",
      "Iteration 431, loss = 0.18633804\n",
      "Iteration 432, loss = 0.18620358\n",
      "Iteration 433, loss = 0.18628569\n",
      "Iteration 434, loss = 0.18681311\n",
      "Iteration 435, loss = 0.18632289\n",
      "Iteration 436, loss = 0.18672516\n",
      "Iteration 437, loss = 0.18611106\n",
      "Iteration 438, loss = 0.18568280\n",
      "Iteration 439, loss = 0.18543821\n",
      "Iteration 440, loss = 0.18524925\n",
      "Iteration 441, loss = 0.18545711\n",
      "Iteration 442, loss = 0.18546916\n",
      "Iteration 443, loss = 0.18523159\n",
      "Iteration 444, loss = 0.18495080\n",
      "Iteration 445, loss = 0.18481032\n",
      "Iteration 446, loss = 0.18631233\n",
      "Iteration 447, loss = 0.18552216\n",
      "Iteration 448, loss = 0.18489072\n",
      "Iteration 449, loss = 0.18530268\n",
      "Iteration 450, loss = 0.18533729\n",
      "Iteration 451, loss = 0.18439115\n",
      "Iteration 452, loss = 0.18497531\n",
      "Iteration 453, loss = 0.18429669\n",
      "Iteration 454, loss = 0.18428390\n",
      "Iteration 455, loss = 0.18415998\n",
      "Iteration 456, loss = 0.18425265\n",
      "Iteration 457, loss = 0.18418505\n",
      "Iteration 458, loss = 0.18322762\n",
      "Iteration 459, loss = 0.18325576\n",
      "Iteration 460, loss = 0.18250262\n",
      "Iteration 461, loss = 0.18188657\n",
      "Iteration 462, loss = 0.18160043\n",
      "Iteration 463, loss = 0.18126650\n",
      "Iteration 464, loss = 0.18100184\n",
      "Iteration 465, loss = 0.18104893\n",
      "Iteration 466, loss = 0.18129886\n",
      "Iteration 467, loss = 0.18114126\n",
      "Iteration 468, loss = 0.18159791\n",
      "Iteration 469, loss = 0.18031682\n",
      "Iteration 470, loss = 0.18123932\n",
      "Iteration 471, loss = 0.18156687\n",
      "Iteration 472, loss = 0.18022280\n",
      "Iteration 473, loss = 0.18048500\n",
      "Iteration 474, loss = 0.18002594\n",
      "Iteration 475, loss = 0.18000745\n",
      "Iteration 476, loss = 0.17975162\n",
      "Iteration 477, loss = 0.18004617\n",
      "Iteration 478, loss = 0.17975286\n",
      "Iteration 479, loss = 0.18002725\n",
      "Iteration 480, loss = 0.17979304\n",
      "Iteration 481, loss = 0.17944026\n",
      "Iteration 482, loss = 0.17987110\n",
      "Iteration 483, loss = 0.17919809\n",
      "Iteration 484, loss = 0.18068574\n",
      "Iteration 485, loss = 0.17970756\n",
      "Iteration 486, loss = 0.17979880\n",
      "Iteration 487, loss = 0.18034617\n",
      "Iteration 488, loss = 0.18004280\n",
      "Iteration 489, loss = 0.17996095\n",
      "Iteration 490, loss = 0.17962254\n",
      "Iteration 491, loss = 0.17892198\n",
      "Iteration 492, loss = 0.17928013\n",
      "Iteration 493, loss = 0.17891114\n",
      "Iteration 494, loss = 0.17884632\n",
      "Iteration 495, loss = 0.17843375\n",
      "Iteration 496, loss = 0.17873232\n",
      "Iteration 497, loss = 0.17806209\n",
      "Iteration 498, loss = 0.17808070\n",
      "Iteration 499, loss = 0.17834525\n",
      "Iteration 500, loss = 0.17803237\n",
      "Iteration 501, loss = 0.17784388\n",
      "Iteration 502, loss = 0.17827612\n",
      "Iteration 503, loss = 0.17816527\n",
      "Iteration 504, loss = 0.17763867\n",
      "Iteration 505, loss = 0.17805615\n",
      "Iteration 506, loss = 0.17815638\n",
      "Iteration 507, loss = 0.17989841\n",
      "Iteration 508, loss = 0.17796839\n",
      "Iteration 509, loss = 0.17808657\n",
      "Iteration 510, loss = 0.17741424\n",
      "Iteration 511, loss = 0.17777833\n",
      "Iteration 512, loss = 0.17748934\n",
      "Iteration 513, loss = 0.17752270\n",
      "Iteration 514, loss = 0.17786210\n",
      "Iteration 515, loss = 0.17802764\n",
      "Iteration 516, loss = 0.17729376\n",
      "Iteration 517, loss = 0.17687527\n",
      "Iteration 518, loss = 0.17724675\n",
      "Iteration 519, loss = 0.17756036\n",
      "Iteration 520, loss = 0.17757337\n",
      "Iteration 521, loss = 0.17710690\n",
      "Iteration 522, loss = 0.17738750\n",
      "Iteration 523, loss = 0.17704424\n",
      "Iteration 524, loss = 0.17693804\n",
      "Iteration 525, loss = 0.17643983\n",
      "Iteration 526, loss = 0.17614621\n",
      "Iteration 527, loss = 0.17644705\n",
      "Iteration 528, loss = 0.17642579\n",
      "Iteration 529, loss = 0.17642403\n",
      "Iteration 530, loss = 0.17745311\n",
      "Iteration 531, loss = 0.17756733\n",
      "Iteration 532, loss = 0.17696878\n",
      "Iteration 533, loss = 0.17652500\n",
      "Iteration 534, loss = 0.17611401\n",
      "Iteration 535, loss = 0.17561374\n",
      "Iteration 536, loss = 0.17617020\n",
      "Iteration 537, loss = 0.17765640\n",
      "Iteration 538, loss = 0.17754050\n",
      "Iteration 539, loss = 0.17679038\n",
      "Iteration 540, loss = 0.17575371\n",
      "Iteration 541, loss = 0.17554904\n",
      "Iteration 542, loss = 0.17549320\n",
      "Iteration 543, loss = 0.17530678\n",
      "Iteration 544, loss = 0.17543780\n",
      "Iteration 545, loss = 0.17518440\n",
      "Iteration 546, loss = 0.17571125\n",
      "Iteration 547, loss = 0.17573327\n",
      "Iteration 548, loss = 0.17517354\n",
      "Iteration 549, loss = 0.17511595\n",
      "Iteration 550, loss = 0.17526984\n",
      "Iteration 551, loss = 0.17523197\n",
      "Iteration 552, loss = 0.17487517\n",
      "Iteration 553, loss = 0.17592595\n",
      "Iteration 554, loss = 0.17485888\n",
      "Iteration 555, loss = 0.17466907\n",
      "Iteration 556, loss = 0.17595615\n",
      "Iteration 557, loss = 0.17596521\n",
      "Iteration 558, loss = 0.17580180\n",
      "Iteration 559, loss = 0.17555313\n",
      "Iteration 560, loss = 0.17444717\n",
      "Iteration 561, loss = 0.17444548\n",
      "Iteration 562, loss = 0.17517455\n",
      "Iteration 563, loss = 0.17455449\n",
      "Iteration 564, loss = 0.17414939\n",
      "Iteration 565, loss = 0.17459559\n",
      "Iteration 566, loss = 0.17464696\n",
      "Iteration 567, loss = 0.17523502\n",
      "Iteration 568, loss = 0.17505111\n",
      "Iteration 569, loss = 0.17402648\n",
      "Iteration 570, loss = 0.17399498\n",
      "Iteration 571, loss = 0.17454363\n",
      "Iteration 572, loss = 0.17426374\n",
      "Iteration 573, loss = 0.17407860\n",
      "Iteration 574, loss = 0.17486259\n",
      "Iteration 575, loss = 0.17453241\n",
      "Iteration 576, loss = 0.17377856\n",
      "Iteration 577, loss = 0.17412150\n",
      "Iteration 578, loss = 0.17376859\n",
      "Iteration 579, loss = 0.17430150\n",
      "Iteration 580, loss = 0.17361090\n",
      "Iteration 581, loss = 0.17521553\n",
      "Iteration 582, loss = 0.17391187\n",
      "Iteration 583, loss = 0.17376163\n",
      "Iteration 584, loss = 0.17327445\n",
      "Iteration 585, loss = 0.17324383\n",
      "Iteration 586, loss = 0.17340219\n",
      "Iteration 587, loss = 0.17362896\n",
      "Iteration 588, loss = 0.17378149\n",
      "Iteration 589, loss = 0.17316973\n",
      "Iteration 590, loss = 0.17373039\n",
      "Iteration 591, loss = 0.17409858\n",
      "Iteration 592, loss = 0.17349708\n",
      "Iteration 593, loss = 0.17325251\n",
      "Iteration 594, loss = 0.17309754\n",
      "Iteration 595, loss = 0.17327703\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76946987\n",
      "Iteration 2, loss = 0.74417289\n",
      "Iteration 3, loss = 0.72244185\n",
      "Iteration 4, loss = 0.70474609\n",
      "Iteration 5, loss = 0.69074942\n",
      "Iteration 6, loss = 0.67754772\n",
      "Iteration 7, loss = 0.66357360\n",
      "Iteration 8, loss = 0.64792652\n",
      "Iteration 9, loss = 0.62932402\n",
      "Iteration 10, loss = 0.61054142\n",
      "Iteration 11, loss = 0.59042523\n",
      "Iteration 12, loss = 0.57208549\n",
      "Iteration 13, loss = 0.55478192\n",
      "Iteration 14, loss = 0.54070520\n",
      "Iteration 15, loss = 0.52834841\n",
      "Iteration 16, loss = 0.51729690\n",
      "Iteration 17, loss = 0.50738352\n",
      "Iteration 18, loss = 0.49929635\n",
      "Iteration 19, loss = 0.49172570\n",
      "Iteration 20, loss = 0.48482466\n",
      "Iteration 21, loss = 0.47797203\n",
      "Iteration 22, loss = 0.47200170\n",
      "Iteration 23, loss = 0.46644526\n",
      "Iteration 24, loss = 0.46164406\n",
      "Iteration 25, loss = 0.45702824\n",
      "Iteration 26, loss = 0.45242191\n",
      "Iteration 27, loss = 0.44798766\n",
      "Iteration 28, loss = 0.44383624\n",
      "Iteration 29, loss = 0.43985050\n",
      "Iteration 30, loss = 0.43604162\n",
      "Iteration 31, loss = 0.43218754\n",
      "Iteration 32, loss = 0.42852884\n",
      "Iteration 33, loss = 0.42474585\n",
      "Iteration 34, loss = 0.42124756\n",
      "Iteration 35, loss = 0.41784372\n",
      "Iteration 36, loss = 0.41443919\n",
      "Iteration 37, loss = 0.41119832\n",
      "Iteration 38, loss = 0.40778685\n",
      "Iteration 39, loss = 0.40450214\n",
      "Iteration 40, loss = 0.40153249\n",
      "Iteration 41, loss = 0.39852630\n",
      "Iteration 42, loss = 0.39555639\n",
      "Iteration 43, loss = 0.39276606\n",
      "Iteration 44, loss = 0.38998416\n",
      "Iteration 45, loss = 0.38730398\n",
      "Iteration 46, loss = 0.38448838\n",
      "Iteration 47, loss = 0.38218754\n",
      "Iteration 48, loss = 0.37950366\n",
      "Iteration 49, loss = 0.37713004\n",
      "Iteration 50, loss = 0.37451111\n",
      "Iteration 51, loss = 0.37206786\n",
      "Iteration 52, loss = 0.36989441\n",
      "Iteration 53, loss = 0.36748132\n",
      "Iteration 54, loss = 0.36531395\n",
      "Iteration 55, loss = 0.36309072\n",
      "Iteration 56, loss = 0.36101763\n",
      "Iteration 57, loss = 0.35902900\n",
      "Iteration 58, loss = 0.35674712\n",
      "Iteration 59, loss = 0.35493733\n",
      "Iteration 60, loss = 0.35285841\n",
      "Iteration 61, loss = 0.35113674\n",
      "Iteration 62, loss = 0.34899238\n",
      "Iteration 63, loss = 0.34709155\n",
      "Iteration 64, loss = 0.34529847\n",
      "Iteration 65, loss = 0.34350189\n",
      "Iteration 66, loss = 0.34137609\n",
      "Iteration 67, loss = 0.33962137\n",
      "Iteration 68, loss = 0.33786491\n",
      "Iteration 69, loss = 0.33595041\n",
      "Iteration 70, loss = 0.33487017\n",
      "Iteration 71, loss = 0.33305020\n",
      "Iteration 72, loss = 0.33200883\n",
      "Iteration 73, loss = 0.32943899\n",
      "Iteration 74, loss = 0.32818028\n",
      "Iteration 75, loss = 0.32627922\n",
      "Iteration 76, loss = 0.32462188\n",
      "Iteration 77, loss = 0.32315280\n",
      "Iteration 78, loss = 0.32168018\n",
      "Iteration 79, loss = 0.32026234\n",
      "Iteration 80, loss = 0.31860459\n",
      "Iteration 81, loss = 0.31709181\n",
      "Iteration 82, loss = 0.31557242\n",
      "Iteration 83, loss = 0.31455943\n",
      "Iteration 84, loss = 0.31306257\n",
      "Iteration 85, loss = 0.31199197\n",
      "Iteration 86, loss = 0.31071231\n",
      "Iteration 87, loss = 0.30897746\n",
      "Iteration 88, loss = 0.30772924\n",
      "Iteration 89, loss = 0.30638192\n",
      "Iteration 90, loss = 0.30522124\n",
      "Iteration 91, loss = 0.30378795\n",
      "Iteration 92, loss = 0.30306413\n",
      "Iteration 93, loss = 0.30134565\n",
      "Iteration 94, loss = 0.30078456\n",
      "Iteration 95, loss = 0.29929557\n",
      "Iteration 96, loss = 0.29797929\n",
      "Iteration 97, loss = 0.29656140\n",
      "Iteration 98, loss = 0.29529342\n",
      "Iteration 99, loss = 0.29408470\n",
      "Iteration 100, loss = 0.29332065\n",
      "Iteration 101, loss = 0.29209627\n",
      "Iteration 102, loss = 0.29099399\n",
      "Iteration 103, loss = 0.28975735\n",
      "Iteration 104, loss = 0.28873552\n",
      "Iteration 105, loss = 0.28766217\n",
      "Iteration 106, loss = 0.28664378\n",
      "Iteration 107, loss = 0.28554436\n",
      "Iteration 108, loss = 0.28496513\n",
      "Iteration 109, loss = 0.28348238\n",
      "Iteration 110, loss = 0.28284656\n",
      "Iteration 111, loss = 0.28184413\n",
      "Iteration 112, loss = 0.28073842\n",
      "Iteration 113, loss = 0.28010436\n",
      "Iteration 114, loss = 0.27896405\n",
      "Iteration 115, loss = 0.27843532\n",
      "Iteration 116, loss = 0.27779253\n",
      "Iteration 117, loss = 0.27637174\n",
      "Iteration 118, loss = 0.27522144\n",
      "Iteration 119, loss = 0.27466729\n",
      "Iteration 120, loss = 0.27346611\n",
      "Iteration 121, loss = 0.27268217\n",
      "Iteration 122, loss = 0.27193562\n",
      "Iteration 123, loss = 0.27116785\n",
      "Iteration 124, loss = 0.27044129\n",
      "Iteration 125, loss = 0.26946596\n",
      "Iteration 126, loss = 0.26866339\n",
      "Iteration 127, loss = 0.26818631\n",
      "Iteration 128, loss = 0.26726814\n",
      "Iteration 129, loss = 0.26636144\n",
      "Iteration 130, loss = 0.26563745\n",
      "Iteration 131, loss = 0.26479911\n",
      "Iteration 132, loss = 0.26405960\n",
      "Iteration 133, loss = 0.26351358\n",
      "Iteration 134, loss = 0.26313543\n",
      "Iteration 135, loss = 0.26201194\n",
      "Iteration 136, loss = 0.26126976\n",
      "Iteration 137, loss = 0.26044954\n",
      "Iteration 138, loss = 0.25975059\n",
      "Iteration 139, loss = 0.25904255\n",
      "Iteration 140, loss = 0.25844089\n",
      "Iteration 141, loss = 0.25763464\n",
      "Iteration 142, loss = 0.25698706\n",
      "Iteration 143, loss = 0.25626347\n",
      "Iteration 144, loss = 0.25584055\n",
      "Iteration 145, loss = 0.25519082\n",
      "Iteration 146, loss = 0.25440545\n",
      "Iteration 147, loss = 0.25390732\n",
      "Iteration 148, loss = 0.25308190\n",
      "Iteration 149, loss = 0.25271179\n",
      "Iteration 150, loss = 0.25157371\n",
      "Iteration 151, loss = 0.25129644\n",
      "Iteration 152, loss = 0.25076172\n",
      "Iteration 153, loss = 0.25023784\n",
      "Iteration 154, loss = 0.24952704\n",
      "Iteration 155, loss = 0.24901086\n",
      "Iteration 156, loss = 0.24826997\n",
      "Iteration 157, loss = 0.24819635\n",
      "Iteration 158, loss = 0.24739145\n",
      "Iteration 159, loss = 0.24713483\n",
      "Iteration 160, loss = 0.24622225\n",
      "Iteration 161, loss = 0.24552995\n",
      "Iteration 162, loss = 0.24529368\n",
      "Iteration 163, loss = 0.24464902\n",
      "Iteration 164, loss = 0.24420002\n",
      "Iteration 165, loss = 0.24377393\n",
      "Iteration 166, loss = 0.24319159\n",
      "Iteration 167, loss = 0.24284232\n",
      "Iteration 168, loss = 0.24274563\n",
      "Iteration 169, loss = 0.24168729\n",
      "Iteration 170, loss = 0.24141799\n",
      "Iteration 171, loss = 0.24076900\n",
      "Iteration 172, loss = 0.24041549\n",
      "Iteration 173, loss = 0.24001150\n",
      "Iteration 174, loss = 0.23937325\n",
      "Iteration 175, loss = 0.23877968\n",
      "Iteration 176, loss = 0.23837314\n",
      "Iteration 177, loss = 0.23790955\n",
      "Iteration 178, loss = 0.23749145\n",
      "Iteration 179, loss = 0.23705410\n",
      "Iteration 180, loss = 0.23731397\n",
      "Iteration 181, loss = 0.23671813\n",
      "Iteration 182, loss = 0.23584305\n",
      "Iteration 183, loss = 0.23517927\n",
      "Iteration 184, loss = 0.23480261\n",
      "Iteration 185, loss = 0.23434412\n",
      "Iteration 186, loss = 0.23409462\n",
      "Iteration 187, loss = 0.23335598\n",
      "Iteration 188, loss = 0.23291663\n",
      "Iteration 189, loss = 0.23279763\n",
      "Iteration 190, loss = 0.23300012\n",
      "Iteration 191, loss = 0.23173269\n",
      "Iteration 192, loss = 0.23168281\n",
      "Iteration 193, loss = 0.23110748\n",
      "Iteration 194, loss = 0.23143807\n",
      "Iteration 195, loss = 0.23006533\n",
      "Iteration 196, loss = 0.23014280\n",
      "Iteration 197, loss = 0.22975286\n",
      "Iteration 198, loss = 0.22907393\n",
      "Iteration 199, loss = 0.22892986\n",
      "Iteration 200, loss = 0.22885456\n",
      "Iteration 201, loss = 0.22837111\n",
      "Iteration 202, loss = 0.22792144\n",
      "Iteration 203, loss = 0.22753866\n",
      "Iteration 204, loss = 0.22723615\n",
      "Iteration 205, loss = 0.22709436\n",
      "Iteration 206, loss = 0.22720110\n",
      "Iteration 207, loss = 0.22630298\n",
      "Iteration 208, loss = 0.22599113\n",
      "Iteration 209, loss = 0.22558649\n",
      "Iteration 210, loss = 0.22564597\n",
      "Iteration 211, loss = 0.22547028\n",
      "Iteration 212, loss = 0.22504039\n",
      "Iteration 213, loss = 0.22458423\n",
      "Iteration 214, loss = 0.22435520\n",
      "Iteration 215, loss = 0.22398330\n",
      "Iteration 216, loss = 0.22354323\n",
      "Iteration 217, loss = 0.22341388\n",
      "Iteration 218, loss = 0.22346816\n",
      "Iteration 219, loss = 0.22302417\n",
      "Iteration 220, loss = 0.22262864\n",
      "Iteration 221, loss = 0.22255264\n",
      "Iteration 222, loss = 0.22216813\n",
      "Iteration 223, loss = 0.22205734\n",
      "Iteration 224, loss = 0.22138500\n",
      "Iteration 225, loss = 0.22121547\n",
      "Iteration 226, loss = 0.22106835\n",
      "Iteration 227, loss = 0.22120891\n",
      "Iteration 228, loss = 0.22066780\n",
      "Iteration 229, loss = 0.22036900\n",
      "Iteration 230, loss = 0.22029290\n",
      "Iteration 231, loss = 0.22000121\n",
      "Iteration 232, loss = 0.21958631\n",
      "Iteration 233, loss = 0.21929237\n",
      "Iteration 234, loss = 0.21903860\n",
      "Iteration 235, loss = 0.21907955\n",
      "Iteration 236, loss = 0.21891042\n",
      "Iteration 237, loss = 0.21883708\n",
      "Iteration 238, loss = 0.21814796\n",
      "Iteration 239, loss = 0.21833041\n",
      "Iteration 240, loss = 0.21764399\n",
      "Iteration 241, loss = 0.21742288\n",
      "Iteration 242, loss = 0.21727685\n",
      "Iteration 243, loss = 0.21726895\n",
      "Iteration 244, loss = 0.21740799\n",
      "Iteration 245, loss = 0.21735829\n",
      "Iteration 246, loss = 0.21737250\n",
      "Iteration 247, loss = 0.21652465\n",
      "Iteration 248, loss = 0.21667123\n",
      "Iteration 249, loss = 0.21618664\n",
      "Iteration 250, loss = 0.21550037\n",
      "Iteration 251, loss = 0.21618461\n",
      "Iteration 252, loss = 0.21557495\n",
      "Iteration 253, loss = 0.21511731\n",
      "Iteration 254, loss = 0.21509982\n",
      "Iteration 255, loss = 0.21471096\n",
      "Iteration 256, loss = 0.21464416\n",
      "Iteration 257, loss = 0.21434398\n",
      "Iteration 258, loss = 0.21408269\n",
      "Iteration 259, loss = 0.21415916\n",
      "Iteration 260, loss = 0.21407573\n",
      "Iteration 261, loss = 0.21368774\n",
      "Iteration 262, loss = 0.21409308\n",
      "Iteration 263, loss = 0.21361162\n",
      "Iteration 264, loss = 0.21356200\n",
      "Iteration 265, loss = 0.21349996\n",
      "Iteration 266, loss = 0.21338004\n",
      "Iteration 267, loss = 0.21291425\n",
      "Iteration 268, loss = 0.21299152\n",
      "Iteration 269, loss = 0.21240898\n",
      "Iteration 270, loss = 0.21289348\n",
      "Iteration 271, loss = 0.21238616\n",
      "Iteration 272, loss = 0.21202436\n",
      "Iteration 273, loss = 0.21217890\n",
      "Iteration 274, loss = 0.21255537\n",
      "Iteration 275, loss = 0.21184808\n",
      "Iteration 276, loss = 0.21140940\n",
      "Iteration 277, loss = 0.21128185\n",
      "Iteration 278, loss = 0.21085945\n",
      "Iteration 279, loss = 0.21096660\n",
      "Iteration 280, loss = 0.21088573\n",
      "Iteration 281, loss = 0.21119458\n",
      "Iteration 282, loss = 0.21068607\n",
      "Iteration 283, loss = 0.21051418\n",
      "Iteration 284, loss = 0.21018013\n",
      "Iteration 285, loss = 0.21020924\n",
      "Iteration 286, loss = 0.20984333\n",
      "Iteration 287, loss = 0.20986652\n",
      "Iteration 288, loss = 0.21078619\n",
      "Iteration 289, loss = 0.20974010\n",
      "Iteration 290, loss = 0.20984892\n",
      "Iteration 291, loss = 0.20938699\n",
      "Iteration 292, loss = 0.20944148\n",
      "Iteration 293, loss = 0.20938702\n",
      "Iteration 294, loss = 0.20899793\n",
      "Iteration 295, loss = 0.20847066\n",
      "Iteration 296, loss = 0.20854645\n",
      "Iteration 297, loss = 0.20869097\n",
      "Iteration 298, loss = 0.20845084\n",
      "Iteration 299, loss = 0.20855329\n",
      "Iteration 300, loss = 0.20780555\n",
      "Iteration 301, loss = 0.20773306\n",
      "Iteration 302, loss = 0.20764522\n",
      "Iteration 303, loss = 0.20742809\n",
      "Iteration 304, loss = 0.20732401\n",
      "Iteration 305, loss = 0.20756288\n",
      "Iteration 306, loss = 0.20690398\n",
      "Iteration 307, loss = 0.20807156\n",
      "Iteration 308, loss = 0.20663337\n",
      "Iteration 309, loss = 0.20691296\n",
      "Iteration 310, loss = 0.20608819\n",
      "Iteration 311, loss = 0.20636692\n",
      "Iteration 312, loss = 0.20607246\n",
      "Iteration 313, loss = 0.20610149\n",
      "Iteration 314, loss = 0.20607401\n",
      "Iteration 315, loss = 0.20584263\n",
      "Iteration 316, loss = 0.20548647\n",
      "Iteration 317, loss = 0.20535193\n",
      "Iteration 318, loss = 0.20539911\n",
      "Iteration 319, loss = 0.20543172\n",
      "Iteration 320, loss = 0.20497429\n",
      "Iteration 321, loss = 0.20487084\n",
      "Iteration 322, loss = 0.20443629\n",
      "Iteration 323, loss = 0.20474247\n",
      "Iteration 324, loss = 0.20411104\n",
      "Iteration 325, loss = 0.20459372\n",
      "Iteration 326, loss = 0.20411131\n",
      "Iteration 327, loss = 0.20436979\n",
      "Iteration 328, loss = 0.20376380\n",
      "Iteration 329, loss = 0.20368080\n",
      "Iteration 330, loss = 0.20367495\n",
      "Iteration 331, loss = 0.20335299\n",
      "Iteration 332, loss = 0.20335396\n",
      "Iteration 333, loss = 0.20316494\n",
      "Iteration 334, loss = 0.20437707\n",
      "Iteration 335, loss = 0.20256054\n",
      "Iteration 336, loss = 0.20254851\n",
      "Iteration 337, loss = 0.20272807\n",
      "Iteration 338, loss = 0.20218989\n",
      "Iteration 339, loss = 0.20201598\n",
      "Iteration 340, loss = 0.20197213\n",
      "Iteration 341, loss = 0.20201442\n",
      "Iteration 342, loss = 0.20132129\n",
      "Iteration 343, loss = 0.20123138\n",
      "Iteration 344, loss = 0.20096947\n",
      "Iteration 345, loss = 0.20093805\n",
      "Iteration 346, loss = 0.20109437\n",
      "Iteration 347, loss = 0.20045846\n",
      "Iteration 348, loss = 0.20060956\n",
      "Iteration 349, loss = 0.20045113\n",
      "Iteration 350, loss = 0.20030482\n",
      "Iteration 351, loss = 0.19998919\n",
      "Iteration 352, loss = 0.20031087\n",
      "Iteration 353, loss = 0.19995796\n",
      "Iteration 354, loss = 0.19995720\n",
      "Iteration 355, loss = 0.19951528\n",
      "Iteration 356, loss = 0.19920358\n",
      "Iteration 357, loss = 0.19900603\n",
      "Iteration 358, loss = 0.19931583\n",
      "Iteration 359, loss = 0.19904553\n",
      "Iteration 360, loss = 0.19881584\n",
      "Iteration 361, loss = 0.19903865\n",
      "Iteration 362, loss = 0.19820483\n",
      "Iteration 363, loss = 0.19812807\n",
      "Iteration 364, loss = 0.19816554\n",
      "Iteration 365, loss = 0.19792927\n",
      "Iteration 366, loss = 0.19802442\n",
      "Iteration 367, loss = 0.19912684\n",
      "Iteration 368, loss = 0.19779661\n",
      "Iteration 369, loss = 0.19709562\n",
      "Iteration 370, loss = 0.19759028\n",
      "Iteration 371, loss = 0.19727452\n",
      "Iteration 372, loss = 0.19678172\n",
      "Iteration 373, loss = 0.19724852\n",
      "Iteration 374, loss = 0.19654746\n",
      "Iteration 375, loss = 0.19718057\n",
      "Iteration 376, loss = 0.19641918\n",
      "Iteration 377, loss = 0.19620627\n",
      "Iteration 378, loss = 0.19623109\n",
      "Iteration 379, loss = 0.19596593\n",
      "Iteration 380, loss = 0.19572342\n",
      "Iteration 381, loss = 0.19546982\n",
      "Iteration 382, loss = 0.19509300\n",
      "Iteration 383, loss = 0.19505699\n",
      "Iteration 384, loss = 0.19521935\n",
      "Iteration 385, loss = 0.19515345\n",
      "Iteration 386, loss = 0.19489530\n",
      "Iteration 387, loss = 0.19447226\n",
      "Iteration 388, loss = 0.19446404\n",
      "Iteration 389, loss = 0.19456770\n",
      "Iteration 390, loss = 0.19420228\n",
      "Iteration 391, loss = 0.19426800\n",
      "Iteration 392, loss = 0.19511639\n",
      "Iteration 393, loss = 0.19406394\n",
      "Iteration 394, loss = 0.19392746\n",
      "Iteration 395, loss = 0.19375545\n",
      "Iteration 396, loss = 0.19352550\n",
      "Iteration 397, loss = 0.19345936\n",
      "Iteration 398, loss = 0.19331677\n",
      "Iteration 399, loss = 0.19338192\n",
      "Iteration 400, loss = 0.19279786\n",
      "Iteration 401, loss = 0.19297117\n",
      "Iteration 402, loss = 0.19268162\n",
      "Iteration 403, loss = 0.19285657\n",
      "Iteration 404, loss = 0.19283970\n",
      "Iteration 405, loss = 0.19267927\n",
      "Iteration 406, loss = 0.19247198\n",
      "Iteration 407, loss = 0.19293739\n",
      "Iteration 408, loss = 0.19278354\n",
      "Iteration 409, loss = 0.19219966\n",
      "Iteration 410, loss = 0.19191637\n",
      "Iteration 411, loss = 0.19170710\n",
      "Iteration 412, loss = 0.19208731\n",
      "Iteration 413, loss = 0.19160162\n",
      "Iteration 414, loss = 0.19179234\n",
      "Iteration 415, loss = 0.19162476\n",
      "Iteration 416, loss = 0.19178588\n",
      "Iteration 417, loss = 0.19165917\n",
      "Iteration 418, loss = 0.19137251\n",
      "Iteration 419, loss = 0.19163656\n",
      "Iteration 420, loss = 0.19149364\n",
      "Iteration 421, loss = 0.19084204\n",
      "Iteration 422, loss = 0.19116088\n",
      "Iteration 423, loss = 0.19096970\n",
      "Iteration 424, loss = 0.19096012\n",
      "Iteration 425, loss = 0.19113021\n",
      "Iteration 426, loss = 0.19114426\n",
      "Iteration 427, loss = 0.19066419\n",
      "Iteration 428, loss = 0.19023274\n",
      "Iteration 429, loss = 0.19057560\n",
      "Iteration 430, loss = 0.19020109\n",
      "Iteration 431, loss = 0.19017770\n",
      "Iteration 432, loss = 0.19018133\n",
      "Iteration 433, loss = 0.19047400\n",
      "Iteration 434, loss = 0.18972750\n",
      "Iteration 435, loss = 0.19005847\n",
      "Iteration 436, loss = 0.19009209\n",
      "Iteration 437, loss = 0.18991818\n",
      "Iteration 438, loss = 0.19001354\n",
      "Iteration 439, loss = 0.18910798\n",
      "Iteration 440, loss = 0.18992206\n",
      "Iteration 441, loss = 0.18945187\n",
      "Iteration 442, loss = 0.18905536\n",
      "Iteration 443, loss = 0.18919225\n",
      "Iteration 444, loss = 0.18942594\n",
      "Iteration 445, loss = 0.18855422\n",
      "Iteration 446, loss = 0.18895823\n",
      "Iteration 447, loss = 0.18882912\n",
      "Iteration 448, loss = 0.18885977\n",
      "Iteration 449, loss = 0.18832991\n",
      "Iteration 450, loss = 0.18835223\n",
      "Iteration 451, loss = 0.18810340\n",
      "Iteration 452, loss = 0.18805205\n",
      "Iteration 453, loss = 0.18755144\n",
      "Iteration 454, loss = 0.18754775\n",
      "Iteration 455, loss = 0.18769726\n",
      "Iteration 456, loss = 0.18776799\n",
      "Iteration 457, loss = 0.18773739\n",
      "Iteration 458, loss = 0.18764287\n",
      "Iteration 459, loss = 0.18730924\n",
      "Iteration 460, loss = 0.18682335\n",
      "Iteration 461, loss = 0.18712603\n",
      "Iteration 462, loss = 0.18705255\n",
      "Iteration 463, loss = 0.18681301\n",
      "Iteration 464, loss = 0.18703448\n",
      "Iteration 465, loss = 0.18664471\n",
      "Iteration 466, loss = 0.18659311\n",
      "Iteration 467, loss = 0.18680096\n",
      "Iteration 468, loss = 0.18666816\n",
      "Iteration 469, loss = 0.18622797\n",
      "Iteration 470, loss = 0.18629864\n",
      "Iteration 471, loss = 0.18631477\n",
      "Iteration 472, loss = 0.18640280\n",
      "Iteration 473, loss = 0.18606044\n",
      "Iteration 474, loss = 0.18715137\n",
      "Iteration 475, loss = 0.18617283\n",
      "Iteration 476, loss = 0.18600139\n",
      "Iteration 477, loss = 0.18559633\n",
      "Iteration 478, loss = 0.18575985\n",
      "Iteration 479, loss = 0.18613694\n",
      "Iteration 480, loss = 0.18543638\n",
      "Iteration 481, loss = 0.18543505\n",
      "Iteration 482, loss = 0.18535315\n",
      "Iteration 483, loss = 0.18532455\n",
      "Iteration 484, loss = 0.18550286\n",
      "Iteration 485, loss = 0.18509511\n",
      "Iteration 486, loss = 0.18673113\n",
      "Iteration 487, loss = 0.18482657\n",
      "Iteration 488, loss = 0.18492709\n",
      "Iteration 489, loss = 0.18489873\n",
      "Iteration 490, loss = 0.18465037\n",
      "Iteration 491, loss = 0.18430051\n",
      "Iteration 492, loss = 0.18429237\n",
      "Iteration 493, loss = 0.18443413\n",
      "Iteration 494, loss = 0.18398692\n",
      "Iteration 495, loss = 0.18382337\n",
      "Iteration 496, loss = 0.18393740\n",
      "Iteration 497, loss = 0.18382860\n",
      "Iteration 498, loss = 0.18383369\n",
      "Iteration 499, loss = 0.18540329\n",
      "Iteration 500, loss = 0.18336662\n",
      "Iteration 501, loss = 0.18335512\n",
      "Iteration 502, loss = 0.18360639\n",
      "Iteration 503, loss = 0.18416719\n",
      "Iteration 504, loss = 0.18349132\n",
      "Iteration 505, loss = 0.18347178\n",
      "Iteration 506, loss = 0.18328318\n",
      "Iteration 507, loss = 0.18369621\n",
      "Iteration 508, loss = 0.18266985\n",
      "Iteration 509, loss = 0.18267927\n",
      "Iteration 510, loss = 0.18247549\n",
      "Iteration 511, loss = 0.18223263\n",
      "Iteration 512, loss = 0.18268012\n",
      "Iteration 513, loss = 0.18235565\n",
      "Iteration 514, loss = 0.18238579\n",
      "Iteration 515, loss = 0.18371950\n",
      "Iteration 516, loss = 0.18220828\n",
      "Iteration 517, loss = 0.18236100\n",
      "Iteration 518, loss = 0.18179521\n",
      "Iteration 519, loss = 0.18170581\n",
      "Iteration 520, loss = 0.18155647\n",
      "Iteration 521, loss = 0.18123452\n",
      "Iteration 522, loss = 0.18173565\n",
      "Iteration 523, loss = 0.18125210\n",
      "Iteration 524, loss = 0.18141952\n",
      "Iteration 525, loss = 0.18138157\n",
      "Iteration 526, loss = 0.18084655\n",
      "Iteration 527, loss = 0.18077818\n",
      "Iteration 528, loss = 0.18119555\n",
      "Iteration 529, loss = 0.18104709\n",
      "Iteration 530, loss = 0.18074035\n",
      "Iteration 531, loss = 0.18063063\n",
      "Iteration 532, loss = 0.18118977\n",
      "Iteration 533, loss = 0.18075226\n",
      "Iteration 534, loss = 0.18017711\n",
      "Iteration 535, loss = 0.18030922\n",
      "Iteration 536, loss = 0.18004820\n",
      "Iteration 537, loss = 0.17989436\n",
      "Iteration 538, loss = 0.18012457\n",
      "Iteration 539, loss = 0.17961305\n",
      "Iteration 540, loss = 0.17949478\n",
      "Iteration 541, loss = 0.17973811\n",
      "Iteration 542, loss = 0.17950318\n",
      "Iteration 543, loss = 0.17917133\n",
      "Iteration 544, loss = 0.17966352\n",
      "Iteration 545, loss = 0.17912624\n",
      "Iteration 546, loss = 0.17898702\n",
      "Iteration 547, loss = 0.17885555\n",
      "Iteration 548, loss = 0.17874326\n",
      "Iteration 549, loss = 0.17914880\n",
      "Iteration 550, loss = 0.17906104\n",
      "Iteration 551, loss = 0.17936413\n",
      "Iteration 552, loss = 0.17886992\n",
      "Iteration 553, loss = 0.17858370\n",
      "Iteration 554, loss = 0.17851924\n",
      "Iteration 555, loss = 0.17819527\n",
      "Iteration 556, loss = 0.17910265\n",
      "Iteration 557, loss = 0.17854126\n",
      "Iteration 558, loss = 0.17841394\n",
      "Iteration 559, loss = 0.17924819\n",
      "Iteration 560, loss = 0.17879060\n",
      "Iteration 561, loss = 0.17852149\n",
      "Iteration 562, loss = 0.17801643\n",
      "Iteration 563, loss = 0.17833948\n",
      "Iteration 564, loss = 0.17759720\n",
      "Iteration 565, loss = 0.17833312\n",
      "Iteration 566, loss = 0.17779490\n",
      "Iteration 567, loss = 0.17776904\n",
      "Iteration 568, loss = 0.17800269\n",
      "Iteration 569, loss = 0.17735629\n",
      "Iteration 570, loss = 0.17739294\n",
      "Iteration 571, loss = 0.17758022\n",
      "Iteration 572, loss = 0.17723618\n",
      "Iteration 573, loss = 0.17737206\n",
      "Iteration 574, loss = 0.17711619\n",
      "Iteration 575, loss = 0.17706395\n",
      "Iteration 576, loss = 0.17732176\n",
      "Iteration 577, loss = 0.17716360\n",
      "Iteration 578, loss = 0.17680326\n",
      "Iteration 579, loss = 0.17711471\n",
      "Iteration 580, loss = 0.17688925\n",
      "Iteration 581, loss = 0.17813943\n",
      "Iteration 582, loss = 0.17642372\n",
      "Iteration 583, loss = 0.17656387\n",
      "Iteration 584, loss = 0.17686554\n",
      "Iteration 585, loss = 0.17647676\n",
      "Iteration 586, loss = 0.17675237\n",
      "Iteration 587, loss = 0.17670250\n",
      "Iteration 588, loss = 0.17673387\n",
      "Iteration 589, loss = 0.17663125\n",
      "Iteration 590, loss = 0.17640962\n",
      "Iteration 591, loss = 0.17756194\n",
      "Iteration 592, loss = 0.17721670\n",
      "Iteration 593, loss = 0.17665288\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77022089\n",
      "Iteration 2, loss = 0.74384567\n",
      "Iteration 3, loss = 0.72208596\n",
      "Iteration 4, loss = 0.70516028\n",
      "Iteration 5, loss = 0.69043535\n",
      "Iteration 6, loss = 0.67739958\n",
      "Iteration 7, loss = 0.66404531\n",
      "Iteration 8, loss = 0.64947909\n",
      "Iteration 9, loss = 0.63208255\n",
      "Iteration 10, loss = 0.61337235\n",
      "Iteration 11, loss = 0.59327677\n",
      "Iteration 12, loss = 0.57451072\n",
      "Iteration 13, loss = 0.55810306\n",
      "Iteration 14, loss = 0.54456199\n",
      "Iteration 15, loss = 0.53234379\n",
      "Iteration 16, loss = 0.52199199\n",
      "Iteration 17, loss = 0.51229652\n",
      "Iteration 18, loss = 0.50314102\n",
      "Iteration 19, loss = 0.49546704\n",
      "Iteration 20, loss = 0.48852932\n",
      "Iteration 21, loss = 0.48204592\n",
      "Iteration 22, loss = 0.47603646\n",
      "Iteration 23, loss = 0.47060635\n",
      "Iteration 24, loss = 0.46531310\n",
      "Iteration 25, loss = 0.46019515\n",
      "Iteration 26, loss = 0.45540602\n",
      "Iteration 27, loss = 0.45120284\n",
      "Iteration 28, loss = 0.44693611\n",
      "Iteration 29, loss = 0.44295567\n",
      "Iteration 30, loss = 0.43897667\n",
      "Iteration 31, loss = 0.43510143\n",
      "Iteration 32, loss = 0.43120037\n",
      "Iteration 33, loss = 0.42743705\n",
      "Iteration 34, loss = 0.42414320\n",
      "Iteration 35, loss = 0.42072938\n",
      "Iteration 36, loss = 0.41678069\n",
      "Iteration 37, loss = 0.41353553\n",
      "Iteration 38, loss = 0.41026391\n",
      "Iteration 39, loss = 0.40690078\n",
      "Iteration 40, loss = 0.40360763\n",
      "Iteration 41, loss = 0.40066492\n",
      "Iteration 42, loss = 0.39766637\n",
      "Iteration 43, loss = 0.39475542\n",
      "Iteration 44, loss = 0.39193411\n",
      "Iteration 45, loss = 0.38912753\n",
      "Iteration 46, loss = 0.38623319\n",
      "Iteration 47, loss = 0.38372200\n",
      "Iteration 48, loss = 0.38107898\n",
      "Iteration 49, loss = 0.37853443\n",
      "Iteration 50, loss = 0.37617028\n",
      "Iteration 51, loss = 0.37402712\n",
      "Iteration 52, loss = 0.37169242\n",
      "Iteration 53, loss = 0.36916165\n",
      "Iteration 54, loss = 0.36668149\n",
      "Iteration 55, loss = 0.36468214\n",
      "Iteration 56, loss = 0.36255955\n",
      "Iteration 57, loss = 0.36028840\n",
      "Iteration 58, loss = 0.35814897\n",
      "Iteration 59, loss = 0.35616424\n",
      "Iteration 60, loss = 0.35391292\n",
      "Iteration 61, loss = 0.35169029\n",
      "Iteration 62, loss = 0.34979073\n",
      "Iteration 63, loss = 0.34772920\n",
      "Iteration 64, loss = 0.34592306\n",
      "Iteration 65, loss = 0.34401144\n",
      "Iteration 66, loss = 0.34207818\n",
      "Iteration 67, loss = 0.34022156\n",
      "Iteration 68, loss = 0.33813963\n",
      "Iteration 69, loss = 0.33612750\n",
      "Iteration 70, loss = 0.33436463\n",
      "Iteration 71, loss = 0.33243560\n",
      "Iteration 72, loss = 0.33080434\n",
      "Iteration 73, loss = 0.32878318\n",
      "Iteration 74, loss = 0.32715681\n",
      "Iteration 75, loss = 0.32535535\n",
      "Iteration 76, loss = 0.32382835\n",
      "Iteration 77, loss = 0.32262659\n",
      "Iteration 78, loss = 0.32050908\n",
      "Iteration 79, loss = 0.31894881\n",
      "Iteration 80, loss = 0.31743652\n",
      "Iteration 81, loss = 0.31569105\n",
      "Iteration 82, loss = 0.31462038\n",
      "Iteration 83, loss = 0.31287140\n",
      "Iteration 84, loss = 0.31139818\n",
      "Iteration 85, loss = 0.30999394\n",
      "Iteration 86, loss = 0.30866488\n",
      "Iteration 87, loss = 0.30731293\n",
      "Iteration 88, loss = 0.30589088\n",
      "Iteration 89, loss = 0.30446990\n",
      "Iteration 90, loss = 0.30366125\n",
      "Iteration 91, loss = 0.30230827\n",
      "Iteration 92, loss = 0.30062765\n",
      "Iteration 93, loss = 0.29951731\n",
      "Iteration 94, loss = 0.29797069\n",
      "Iteration 95, loss = 0.29740033\n",
      "Iteration 96, loss = 0.29559178\n",
      "Iteration 97, loss = 0.29503879\n",
      "Iteration 98, loss = 0.29341970\n",
      "Iteration 99, loss = 0.29249655\n",
      "Iteration 100, loss = 0.29127575\n",
      "Iteration 101, loss = 0.29013629\n",
      "Iteration 102, loss = 0.28903578\n",
      "Iteration 103, loss = 0.28798513\n",
      "Iteration 104, loss = 0.28689771\n",
      "Iteration 105, loss = 0.28605163\n",
      "Iteration 106, loss = 0.28498405\n",
      "Iteration 107, loss = 0.28396249\n",
      "Iteration 108, loss = 0.28309189\n",
      "Iteration 109, loss = 0.28228063\n",
      "Iteration 110, loss = 0.28130400\n",
      "Iteration 111, loss = 0.28041095\n",
      "Iteration 112, loss = 0.27948044\n",
      "Iteration 113, loss = 0.27833593\n",
      "Iteration 114, loss = 0.27746080\n",
      "Iteration 115, loss = 0.27690642\n",
      "Iteration 116, loss = 0.27589232\n",
      "Iteration 117, loss = 0.27487312\n",
      "Iteration 118, loss = 0.27405709\n",
      "Iteration 119, loss = 0.27347185\n",
      "Iteration 120, loss = 0.27278548\n",
      "Iteration 121, loss = 0.27152860\n",
      "Iteration 122, loss = 0.27086130\n",
      "Iteration 123, loss = 0.27022803\n",
      "Iteration 124, loss = 0.26979408\n",
      "Iteration 125, loss = 0.26881714\n",
      "Iteration 126, loss = 0.26780722\n",
      "Iteration 127, loss = 0.26715702\n",
      "Iteration 128, loss = 0.26631013\n",
      "Iteration 129, loss = 0.26552087\n",
      "Iteration 130, loss = 0.26477246\n",
      "Iteration 131, loss = 0.26419232\n",
      "Iteration 132, loss = 0.26370392\n",
      "Iteration 133, loss = 0.26304071\n",
      "Iteration 134, loss = 0.26242697\n",
      "Iteration 135, loss = 0.26141727\n",
      "Iteration 136, loss = 0.26063671\n",
      "Iteration 137, loss = 0.26020527\n",
      "Iteration 138, loss = 0.25957370\n",
      "Iteration 139, loss = 0.25877193\n",
      "Iteration 140, loss = 0.25831043\n",
      "Iteration 141, loss = 0.25810048\n",
      "Iteration 142, loss = 0.25713496\n",
      "Iteration 143, loss = 0.25656791\n",
      "Iteration 144, loss = 0.25594760\n",
      "Iteration 145, loss = 0.25517403\n",
      "Iteration 146, loss = 0.25500316\n",
      "Iteration 147, loss = 0.25409996\n",
      "Iteration 148, loss = 0.25354267\n",
      "Iteration 149, loss = 0.25341194\n",
      "Iteration 150, loss = 0.25238416\n",
      "Iteration 151, loss = 0.25175972\n",
      "Iteration 152, loss = 0.25123836\n",
      "Iteration 153, loss = 0.25070264\n",
      "Iteration 154, loss = 0.25014984\n",
      "Iteration 155, loss = 0.24992908\n",
      "Iteration 156, loss = 0.24978939\n",
      "Iteration 157, loss = 0.24856818\n",
      "Iteration 158, loss = 0.24809466\n",
      "Iteration 159, loss = 0.24755701\n",
      "Iteration 160, loss = 0.24701027\n",
      "Iteration 161, loss = 0.24645484\n",
      "Iteration 162, loss = 0.24590689\n",
      "Iteration 163, loss = 0.24550593\n",
      "Iteration 164, loss = 0.24500901\n",
      "Iteration 165, loss = 0.24480594\n",
      "Iteration 166, loss = 0.24412896\n",
      "Iteration 167, loss = 0.24375702\n",
      "Iteration 168, loss = 0.24327888\n",
      "Iteration 169, loss = 0.24264966\n",
      "Iteration 170, loss = 0.24233429\n",
      "Iteration 171, loss = 0.24166847\n",
      "Iteration 172, loss = 0.24153796\n",
      "Iteration 173, loss = 0.24107234\n",
      "Iteration 174, loss = 0.24074696\n",
      "Iteration 175, loss = 0.24069567\n",
      "Iteration 176, loss = 0.23952908\n",
      "Iteration 177, loss = 0.23958378\n",
      "Iteration 178, loss = 0.23921408\n",
      "Iteration 179, loss = 0.23876350\n",
      "Iteration 180, loss = 0.23818136\n",
      "Iteration 181, loss = 0.23772200\n",
      "Iteration 182, loss = 0.23722617\n",
      "Iteration 183, loss = 0.23711870\n",
      "Iteration 184, loss = 0.23650056\n",
      "Iteration 185, loss = 0.23596470\n",
      "Iteration 186, loss = 0.23555702\n",
      "Iteration 187, loss = 0.23506976\n",
      "Iteration 188, loss = 0.23493413\n",
      "Iteration 189, loss = 0.23467461\n",
      "Iteration 190, loss = 0.23412096\n",
      "Iteration 191, loss = 0.23358663\n",
      "Iteration 192, loss = 0.23338658\n",
      "Iteration 193, loss = 0.23294154\n",
      "Iteration 194, loss = 0.23259189\n",
      "Iteration 195, loss = 0.23248992\n",
      "Iteration 196, loss = 0.23184449\n",
      "Iteration 197, loss = 0.23196406\n",
      "Iteration 198, loss = 0.23122256\n",
      "Iteration 199, loss = 0.23071366\n",
      "Iteration 200, loss = 0.23053091\n",
      "Iteration 201, loss = 0.22994582\n",
      "Iteration 202, loss = 0.22962141\n",
      "Iteration 203, loss = 0.22932088\n",
      "Iteration 204, loss = 0.22934593\n",
      "Iteration 205, loss = 0.22918121\n",
      "Iteration 206, loss = 0.22860147\n",
      "Iteration 207, loss = 0.22781476\n",
      "Iteration 208, loss = 0.22750910\n",
      "Iteration 209, loss = 0.22717861\n",
      "Iteration 210, loss = 0.22663630\n",
      "Iteration 211, loss = 0.22629415\n",
      "Iteration 212, loss = 0.22599592\n",
      "Iteration 213, loss = 0.22565271\n",
      "Iteration 214, loss = 0.22538730\n",
      "Iteration 215, loss = 0.22510524\n",
      "Iteration 216, loss = 0.22466926\n",
      "Iteration 217, loss = 0.22427781\n",
      "Iteration 218, loss = 0.22386159\n",
      "Iteration 219, loss = 0.22370287\n",
      "Iteration 220, loss = 0.22302459\n",
      "Iteration 221, loss = 0.22383222\n",
      "Iteration 222, loss = 0.22244606\n",
      "Iteration 223, loss = 0.22207083\n",
      "Iteration 224, loss = 0.22209683\n",
      "Iteration 225, loss = 0.22173497\n",
      "Iteration 226, loss = 0.22094128\n",
      "Iteration 227, loss = 0.22115242\n",
      "Iteration 228, loss = 0.22023491\n",
      "Iteration 229, loss = 0.22014370\n",
      "Iteration 230, loss = 0.21982323\n",
      "Iteration 231, loss = 0.21962130\n",
      "Iteration 232, loss = 0.21907659\n",
      "Iteration 233, loss = 0.21873099\n",
      "Iteration 234, loss = 0.21854851\n",
      "Iteration 235, loss = 0.21859976\n",
      "Iteration 236, loss = 0.21809138\n",
      "Iteration 237, loss = 0.21801645\n",
      "Iteration 238, loss = 0.21731495\n",
      "Iteration 239, loss = 0.21694950\n",
      "Iteration 240, loss = 0.21676666\n",
      "Iteration 241, loss = 0.21668731\n",
      "Iteration 242, loss = 0.21640793\n",
      "Iteration 243, loss = 0.21701780\n",
      "Iteration 244, loss = 0.21588925\n",
      "Iteration 245, loss = 0.21544659\n",
      "Iteration 246, loss = 0.21488451\n",
      "Iteration 247, loss = 0.21482680\n",
      "Iteration 248, loss = 0.21429734\n",
      "Iteration 249, loss = 0.21453786\n",
      "Iteration 250, loss = 0.21384669\n",
      "Iteration 251, loss = 0.21388580\n",
      "Iteration 252, loss = 0.21359579\n",
      "Iteration 253, loss = 0.21368363\n",
      "Iteration 254, loss = 0.21308498\n",
      "Iteration 255, loss = 0.21304149\n",
      "Iteration 256, loss = 0.21264749\n",
      "Iteration 257, loss = 0.21250423\n",
      "Iteration 258, loss = 0.21232246\n",
      "Iteration 259, loss = 0.21210274\n",
      "Iteration 260, loss = 0.21203322\n",
      "Iteration 261, loss = 0.21184865\n",
      "Iteration 262, loss = 0.21153524\n",
      "Iteration 263, loss = 0.21124276\n",
      "Iteration 264, loss = 0.21120027\n",
      "Iteration 265, loss = 0.21086818\n",
      "Iteration 266, loss = 0.21071303\n",
      "Iteration 267, loss = 0.21021849\n",
      "Iteration 268, loss = 0.21031419\n",
      "Iteration 269, loss = 0.21017503\n",
      "Iteration 270, loss = 0.21047472\n",
      "Iteration 271, loss = 0.21007988\n",
      "Iteration 272, loss = 0.20938694\n",
      "Iteration 273, loss = 0.20944709\n",
      "Iteration 274, loss = 0.20990795\n",
      "Iteration 275, loss = 0.20928145\n",
      "Iteration 276, loss = 0.20931440\n",
      "Iteration 277, loss = 0.20866506\n",
      "Iteration 278, loss = 0.20843918\n",
      "Iteration 279, loss = 0.20854160\n",
      "Iteration 280, loss = 0.20806257\n",
      "Iteration 281, loss = 0.20788217\n",
      "Iteration 282, loss = 0.20777471\n",
      "Iteration 283, loss = 0.20763303\n",
      "Iteration 284, loss = 0.20754306\n",
      "Iteration 285, loss = 0.20711741\n",
      "Iteration 286, loss = 0.20746746\n",
      "Iteration 287, loss = 0.20712707\n",
      "Iteration 288, loss = 0.20686410\n",
      "Iteration 289, loss = 0.20658020\n",
      "Iteration 290, loss = 0.20645527\n",
      "Iteration 291, loss = 0.20596865\n",
      "Iteration 292, loss = 0.20574628\n",
      "Iteration 293, loss = 0.20614782\n",
      "Iteration 294, loss = 0.20573934\n",
      "Iteration 295, loss = 0.20581543\n",
      "Iteration 296, loss = 0.20551852\n",
      "Iteration 297, loss = 0.20599996\n",
      "Iteration 298, loss = 0.20480036\n",
      "Iteration 299, loss = 0.20515152\n",
      "Iteration 300, loss = 0.20578724\n",
      "Iteration 301, loss = 0.20429963\n",
      "Iteration 302, loss = 0.20513344\n",
      "Iteration 303, loss = 0.20429896\n",
      "Iteration 304, loss = 0.20447098\n",
      "Iteration 305, loss = 0.20395940\n",
      "Iteration 306, loss = 0.20380715\n",
      "Iteration 307, loss = 0.20421113\n",
      "Iteration 308, loss = 0.20401267\n",
      "Iteration 309, loss = 0.20423899\n",
      "Iteration 310, loss = 0.20380009\n",
      "Iteration 311, loss = 0.20322595\n",
      "Iteration 312, loss = 0.20320256\n",
      "Iteration 313, loss = 0.20332528\n",
      "Iteration 314, loss = 0.20266233\n",
      "Iteration 315, loss = 0.20261471\n",
      "Iteration 316, loss = 0.20262345\n",
      "Iteration 317, loss = 0.20260945\n",
      "Iteration 318, loss = 0.20222098\n",
      "Iteration 319, loss = 0.20232657\n",
      "Iteration 320, loss = 0.20210957\n",
      "Iteration 321, loss = 0.20174123\n",
      "Iteration 322, loss = 0.20230232\n",
      "Iteration 323, loss = 0.20133846\n",
      "Iteration 324, loss = 0.20168653\n",
      "Iteration 325, loss = 0.20171362\n",
      "Iteration 326, loss = 0.20090875\n",
      "Iteration 327, loss = 0.20132251\n",
      "Iteration 328, loss = 0.20083333\n",
      "Iteration 329, loss = 0.20141822\n",
      "Iteration 330, loss = 0.20036124\n",
      "Iteration 331, loss = 0.20045161\n",
      "Iteration 332, loss = 0.19999132\n",
      "Iteration 333, loss = 0.19993029\n",
      "Iteration 334, loss = 0.19970574\n",
      "Iteration 335, loss = 0.20018118\n",
      "Iteration 336, loss = 0.19940686\n",
      "Iteration 337, loss = 0.19915228\n",
      "Iteration 338, loss = 0.19879121\n",
      "Iteration 339, loss = 0.19850310\n",
      "Iteration 340, loss = 0.19860932\n",
      "Iteration 341, loss = 0.19838836\n",
      "Iteration 342, loss = 0.19799532\n",
      "Iteration 343, loss = 0.19798772\n",
      "Iteration 344, loss = 0.19813554\n",
      "Iteration 345, loss = 0.19844093\n",
      "Iteration 346, loss = 0.19760236\n",
      "Iteration 347, loss = 0.19699395\n",
      "Iteration 348, loss = 0.19728853\n",
      "Iteration 349, loss = 0.19718644\n",
      "Iteration 350, loss = 0.19695275\n",
      "Iteration 351, loss = 0.19692721\n",
      "Iteration 352, loss = 0.19636319\n",
      "Iteration 353, loss = 0.19685241\n",
      "Iteration 354, loss = 0.19643864\n",
      "Iteration 355, loss = 0.19648974\n",
      "Iteration 356, loss = 0.19590304\n",
      "Iteration 357, loss = 0.19553815\n",
      "Iteration 358, loss = 0.19569705\n",
      "Iteration 359, loss = 0.19692241\n",
      "Iteration 360, loss = 0.19527354\n",
      "Iteration 361, loss = 0.19507506\n",
      "Iteration 362, loss = 0.19475263\n",
      "Iteration 363, loss = 0.19468349\n",
      "Iteration 364, loss = 0.19493151\n",
      "Iteration 365, loss = 0.19454316\n",
      "Iteration 366, loss = 0.19454696\n",
      "Iteration 367, loss = 0.19383823\n",
      "Iteration 368, loss = 0.19404437\n",
      "Iteration 369, loss = 0.19407963\n",
      "Iteration 370, loss = 0.19359344\n",
      "Iteration 371, loss = 0.19331285\n",
      "Iteration 372, loss = 0.19335684\n",
      "Iteration 373, loss = 0.19311606\n",
      "Iteration 374, loss = 0.19312820\n",
      "Iteration 375, loss = 0.19321248\n",
      "Iteration 376, loss = 0.19376214\n",
      "Iteration 377, loss = 0.19288451\n",
      "Iteration 378, loss = 0.19243135\n",
      "Iteration 379, loss = 0.19219463\n",
      "Iteration 380, loss = 0.19275388\n",
      "Iteration 381, loss = 0.19210645\n",
      "Iteration 382, loss = 0.19246498\n",
      "Iteration 383, loss = 0.19242965\n",
      "Iteration 384, loss = 0.19215395\n",
      "Iteration 385, loss = 0.19154841\n",
      "Iteration 386, loss = 0.19208553\n",
      "Iteration 387, loss = 0.19117008\n",
      "Iteration 388, loss = 0.19135267\n",
      "Iteration 389, loss = 0.19111670\n",
      "Iteration 390, loss = 0.19195705\n",
      "Iteration 391, loss = 0.19061176\n",
      "Iteration 392, loss = 0.19059396\n",
      "Iteration 393, loss = 0.19064007\n",
      "Iteration 394, loss = 0.19042455\n",
      "Iteration 395, loss = 0.19037590\n",
      "Iteration 396, loss = 0.19022370\n",
      "Iteration 397, loss = 0.19064095\n",
      "Iteration 398, loss = 0.18984626\n",
      "Iteration 399, loss = 0.19092807\n",
      "Iteration 400, loss = 0.18975181\n",
      "Iteration 401, loss = 0.18934034\n",
      "Iteration 402, loss = 0.18952195\n",
      "Iteration 403, loss = 0.18910249\n",
      "Iteration 404, loss = 0.18904381\n",
      "Iteration 405, loss = 0.18893247\n",
      "Iteration 406, loss = 0.18891036\n",
      "Iteration 407, loss = 0.18888684\n",
      "Iteration 408, loss = 0.18880304\n",
      "Iteration 409, loss = 0.18851616\n",
      "Iteration 410, loss = 0.18859664\n",
      "Iteration 411, loss = 0.18844295\n",
      "Iteration 412, loss = 0.18850347\n",
      "Iteration 413, loss = 0.18821588\n",
      "Iteration 414, loss = 0.18855940\n",
      "Iteration 415, loss = 0.18785928\n",
      "Iteration 416, loss = 0.18782539\n",
      "Iteration 417, loss = 0.18778367\n",
      "Iteration 418, loss = 0.18781813\n",
      "Iteration 419, loss = 0.18781099\n",
      "Iteration 420, loss = 0.18758016\n",
      "Iteration 421, loss = 0.18742761\n",
      "Iteration 422, loss = 0.18877151\n",
      "Iteration 423, loss = 0.18769224\n",
      "Iteration 424, loss = 0.18717265\n",
      "Iteration 425, loss = 0.18748349\n",
      "Iteration 426, loss = 0.18691063\n",
      "Iteration 427, loss = 0.18720477\n",
      "Iteration 428, loss = 0.18653730\n",
      "Iteration 429, loss = 0.18665362\n",
      "Iteration 430, loss = 0.18626531\n",
      "Iteration 431, loss = 0.18638147\n",
      "Iteration 432, loss = 0.18626332\n",
      "Iteration 433, loss = 0.18587787\n",
      "Iteration 434, loss = 0.18597733\n",
      "Iteration 435, loss = 0.18566589\n",
      "Iteration 436, loss = 0.18572891\n",
      "Iteration 437, loss = 0.18554593\n",
      "Iteration 438, loss = 0.18567247\n",
      "Iteration 439, loss = 0.18549011\n",
      "Iteration 440, loss = 0.18531061\n",
      "Iteration 441, loss = 0.18526050\n",
      "Iteration 442, loss = 0.18544181\n",
      "Iteration 443, loss = 0.18508646\n",
      "Iteration 444, loss = 0.18463082\n",
      "Iteration 445, loss = 0.18483039\n",
      "Iteration 446, loss = 0.18490181\n",
      "Iteration 447, loss = 0.18516794\n",
      "Iteration 448, loss = 0.18448921\n",
      "Iteration 449, loss = 0.18550088\n",
      "Iteration 450, loss = 0.18460677\n",
      "Iteration 451, loss = 0.18482553\n",
      "Iteration 452, loss = 0.18434372\n",
      "Iteration 453, loss = 0.18440739\n",
      "Iteration 454, loss = 0.18434837\n",
      "Iteration 455, loss = 0.18479037\n",
      "Iteration 456, loss = 0.18414187\n",
      "Iteration 457, loss = 0.18383928\n",
      "Iteration 458, loss = 0.18361942\n",
      "Iteration 459, loss = 0.18350599\n",
      "Iteration 460, loss = 0.18338967\n",
      "Iteration 461, loss = 0.18421224\n",
      "Iteration 462, loss = 0.18374329\n",
      "Iteration 463, loss = 0.18348205\n",
      "Iteration 464, loss = 0.18346891\n",
      "Iteration 465, loss = 0.18367238\n",
      "Iteration 466, loss = 0.18432752\n",
      "Iteration 467, loss = 0.18361508\n",
      "Iteration 468, loss = 0.18308993\n",
      "Iteration 469, loss = 0.18304040\n",
      "Iteration 470, loss = 0.18293159\n",
      "Iteration 471, loss = 0.18264167\n",
      "Iteration 472, loss = 0.18284662\n",
      "Iteration 473, loss = 0.18247736\n",
      "Iteration 474, loss = 0.18250695\n",
      "Iteration 475, loss = 0.18243810\n",
      "Iteration 476, loss = 0.18236832\n",
      "Iteration 477, loss = 0.18231046\n",
      "Iteration 478, loss = 0.18251860\n",
      "Iteration 479, loss = 0.18284029\n",
      "Iteration 480, loss = 0.18236821\n",
      "Iteration 481, loss = 0.18252117\n",
      "Iteration 482, loss = 0.18243384\n",
      "Iteration 483, loss = 0.18252922\n",
      "Iteration 484, loss = 0.18236955\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76837230\n",
      "Iteration 2, loss = 0.74226628\n",
      "Iteration 3, loss = 0.72076600\n",
      "Iteration 4, loss = 0.70406166\n",
      "Iteration 5, loss = 0.69043518\n",
      "Iteration 6, loss = 0.67723122\n",
      "Iteration 7, loss = 0.66305892\n",
      "Iteration 8, loss = 0.64763039\n",
      "Iteration 9, loss = 0.62942783\n",
      "Iteration 10, loss = 0.61005545\n",
      "Iteration 11, loss = 0.59096659\n",
      "Iteration 12, loss = 0.57260032\n",
      "Iteration 13, loss = 0.55590530\n",
      "Iteration 14, loss = 0.54191579\n",
      "Iteration 15, loss = 0.52947245\n",
      "Iteration 16, loss = 0.51904230\n",
      "Iteration 17, loss = 0.50952498\n",
      "Iteration 18, loss = 0.50152418\n",
      "Iteration 19, loss = 0.49421317\n",
      "Iteration 20, loss = 0.48758611\n",
      "Iteration 21, loss = 0.48137396\n",
      "Iteration 22, loss = 0.47560249\n",
      "Iteration 23, loss = 0.47004274\n",
      "Iteration 24, loss = 0.46484944\n",
      "Iteration 25, loss = 0.46004296\n",
      "Iteration 26, loss = 0.45535568\n",
      "Iteration 27, loss = 0.45093753\n",
      "Iteration 28, loss = 0.44673657\n",
      "Iteration 29, loss = 0.44272196\n",
      "Iteration 30, loss = 0.43876119\n",
      "Iteration 31, loss = 0.43494835\n",
      "Iteration 32, loss = 0.43115940\n",
      "Iteration 33, loss = 0.42780904\n",
      "Iteration 34, loss = 0.42431486\n",
      "Iteration 35, loss = 0.42124245\n",
      "Iteration 36, loss = 0.41792540\n",
      "Iteration 37, loss = 0.41461820\n",
      "Iteration 38, loss = 0.41139013\n",
      "Iteration 39, loss = 0.40831154\n",
      "Iteration 40, loss = 0.40525963\n",
      "Iteration 41, loss = 0.40235470\n",
      "Iteration 42, loss = 0.39978575\n",
      "Iteration 43, loss = 0.39680859\n",
      "Iteration 44, loss = 0.39400227\n",
      "Iteration 45, loss = 0.39148032\n",
      "Iteration 46, loss = 0.38896714\n",
      "Iteration 47, loss = 0.38630289\n",
      "Iteration 48, loss = 0.38391060\n",
      "Iteration 49, loss = 0.38129802\n",
      "Iteration 50, loss = 0.37887648\n",
      "Iteration 51, loss = 0.37657856\n",
      "Iteration 52, loss = 0.37464426\n",
      "Iteration 53, loss = 0.37223967\n",
      "Iteration 54, loss = 0.36978771\n",
      "Iteration 55, loss = 0.36788302\n",
      "Iteration 56, loss = 0.36545426\n",
      "Iteration 57, loss = 0.36359795\n",
      "Iteration 58, loss = 0.36090270\n",
      "Iteration 59, loss = 0.35927558\n",
      "Iteration 60, loss = 0.35689821\n",
      "Iteration 61, loss = 0.35507720\n",
      "Iteration 62, loss = 0.35324929\n",
      "Iteration 63, loss = 0.35128039\n",
      "Iteration 64, loss = 0.34937933\n",
      "Iteration 65, loss = 0.34762001\n",
      "Iteration 66, loss = 0.34585947\n",
      "Iteration 67, loss = 0.34395197\n",
      "Iteration 68, loss = 0.34222238\n",
      "Iteration 69, loss = 0.34063386\n",
      "Iteration 70, loss = 0.33894682\n",
      "Iteration 71, loss = 0.33743093\n",
      "Iteration 72, loss = 0.33573691\n",
      "Iteration 73, loss = 0.33383673\n",
      "Iteration 74, loss = 0.33259250\n",
      "Iteration 75, loss = 0.33036195\n",
      "Iteration 76, loss = 0.32878471\n",
      "Iteration 77, loss = 0.32707870\n",
      "Iteration 78, loss = 0.32580599\n",
      "Iteration 79, loss = 0.32408311\n",
      "Iteration 80, loss = 0.32250704\n",
      "Iteration 81, loss = 0.32093592\n",
      "Iteration 82, loss = 0.31925680\n",
      "Iteration 83, loss = 0.31776126\n",
      "Iteration 84, loss = 0.31674509\n",
      "Iteration 85, loss = 0.31513847\n",
      "Iteration 86, loss = 0.31379170\n",
      "Iteration 87, loss = 0.31234966\n",
      "Iteration 88, loss = 0.31098275\n",
      "Iteration 89, loss = 0.30949062\n",
      "Iteration 90, loss = 0.30842670\n",
      "Iteration 91, loss = 0.30780104\n",
      "Iteration 92, loss = 0.30570208\n",
      "Iteration 93, loss = 0.30456646\n",
      "Iteration 94, loss = 0.30332673\n",
      "Iteration 95, loss = 0.30202511\n",
      "Iteration 96, loss = 0.30076051\n",
      "Iteration 97, loss = 0.29973059\n",
      "Iteration 98, loss = 0.29863186\n",
      "Iteration 99, loss = 0.29721746\n",
      "Iteration 100, loss = 0.29629990\n",
      "Iteration 101, loss = 0.29514311\n",
      "Iteration 102, loss = 0.29393242\n",
      "Iteration 103, loss = 0.29250205\n",
      "Iteration 104, loss = 0.29156946\n",
      "Iteration 105, loss = 0.29005046\n",
      "Iteration 106, loss = 0.28925099\n",
      "Iteration 107, loss = 0.28831737\n",
      "Iteration 108, loss = 0.28715144\n",
      "Iteration 109, loss = 0.28609738\n",
      "Iteration 110, loss = 0.28543385\n",
      "Iteration 111, loss = 0.28396847\n",
      "Iteration 112, loss = 0.28304176\n",
      "Iteration 113, loss = 0.28180439\n",
      "Iteration 114, loss = 0.28055355\n",
      "Iteration 115, loss = 0.28017833\n",
      "Iteration 116, loss = 0.27866638\n",
      "Iteration 117, loss = 0.27802849\n",
      "Iteration 118, loss = 0.27711120\n",
      "Iteration 119, loss = 0.27578628\n",
      "Iteration 120, loss = 0.27549494\n",
      "Iteration 121, loss = 0.27437854\n",
      "Iteration 122, loss = 0.27322425\n",
      "Iteration 123, loss = 0.27198131\n",
      "Iteration 124, loss = 0.27095349\n",
      "Iteration 125, loss = 0.27016173\n",
      "Iteration 126, loss = 0.26950322\n",
      "Iteration 127, loss = 0.26819648\n",
      "Iteration 128, loss = 0.26754070\n",
      "Iteration 129, loss = 0.26671771\n",
      "Iteration 130, loss = 0.26588149\n",
      "Iteration 131, loss = 0.26448295\n",
      "Iteration 132, loss = 0.26408452\n",
      "Iteration 133, loss = 0.26313594\n",
      "Iteration 134, loss = 0.26247607\n",
      "Iteration 135, loss = 0.26177588\n",
      "Iteration 136, loss = 0.26102478\n",
      "Iteration 137, loss = 0.26032553\n",
      "Iteration 138, loss = 0.25908251\n",
      "Iteration 139, loss = 0.25845795\n",
      "Iteration 140, loss = 0.25816032\n",
      "Iteration 141, loss = 0.25698740\n",
      "Iteration 142, loss = 0.25597402\n",
      "Iteration 143, loss = 0.25537412\n",
      "Iteration 144, loss = 0.25463425\n",
      "Iteration 145, loss = 0.25391899\n",
      "Iteration 146, loss = 0.25315053\n",
      "Iteration 147, loss = 0.25292590\n",
      "Iteration 148, loss = 0.25216502\n",
      "Iteration 149, loss = 0.25089796\n",
      "Iteration 150, loss = 0.25048662\n",
      "Iteration 151, loss = 0.24977459\n",
      "Iteration 152, loss = 0.24902147\n",
      "Iteration 153, loss = 0.24851534\n",
      "Iteration 154, loss = 0.24753648\n",
      "Iteration 155, loss = 0.24756655\n",
      "Iteration 156, loss = 0.24630780\n",
      "Iteration 157, loss = 0.24592342\n",
      "Iteration 158, loss = 0.24516183\n",
      "Iteration 159, loss = 0.24446513\n",
      "Iteration 160, loss = 0.24398306\n",
      "Iteration 161, loss = 0.24355611\n",
      "Iteration 162, loss = 0.24283187\n",
      "Iteration 163, loss = 0.24222184\n",
      "Iteration 164, loss = 0.24191494\n",
      "Iteration 165, loss = 0.24151415\n",
      "Iteration 166, loss = 0.24041256\n",
      "Iteration 167, loss = 0.23992305\n",
      "Iteration 168, loss = 0.23938440\n",
      "Iteration 169, loss = 0.23955815\n",
      "Iteration 170, loss = 0.23855142\n",
      "Iteration 171, loss = 0.23771020\n",
      "Iteration 172, loss = 0.23723491\n",
      "Iteration 173, loss = 0.23802149\n",
      "Iteration 174, loss = 0.23658640\n",
      "Iteration 175, loss = 0.23577745\n",
      "Iteration 176, loss = 0.23534840\n",
      "Iteration 177, loss = 0.23490007\n",
      "Iteration 178, loss = 0.23506056\n",
      "Iteration 179, loss = 0.23361486\n",
      "Iteration 180, loss = 0.23375239\n",
      "Iteration 181, loss = 0.23307177\n",
      "Iteration 182, loss = 0.23277111\n",
      "Iteration 183, loss = 0.23219364\n",
      "Iteration 184, loss = 0.23150548\n",
      "Iteration 185, loss = 0.23141009\n",
      "Iteration 186, loss = 0.23081025\n",
      "Iteration 187, loss = 0.23046084\n",
      "Iteration 188, loss = 0.23000951\n",
      "Iteration 189, loss = 0.23025173\n",
      "Iteration 190, loss = 0.22916924\n",
      "Iteration 191, loss = 0.22900943\n",
      "Iteration 192, loss = 0.22869042\n",
      "Iteration 193, loss = 0.22793544\n",
      "Iteration 194, loss = 0.22767728\n",
      "Iteration 195, loss = 0.22724046\n",
      "Iteration 196, loss = 0.22741590\n",
      "Iteration 197, loss = 0.22662479\n",
      "Iteration 198, loss = 0.22648680\n",
      "Iteration 199, loss = 0.22582365\n",
      "Iteration 200, loss = 0.22553243\n",
      "Iteration 201, loss = 0.22500820\n",
      "Iteration 202, loss = 0.22458414\n",
      "Iteration 203, loss = 0.22431834\n",
      "Iteration 204, loss = 0.22405644\n",
      "Iteration 205, loss = 0.22400295\n",
      "Iteration 206, loss = 0.22353268\n",
      "Iteration 207, loss = 0.22304867\n",
      "Iteration 208, loss = 0.22319951\n",
      "Iteration 209, loss = 0.22292253\n",
      "Iteration 210, loss = 0.22255779\n",
      "Iteration 211, loss = 0.22222224\n",
      "Iteration 212, loss = 0.22161193\n",
      "Iteration 213, loss = 0.22120300\n",
      "Iteration 214, loss = 0.22168116\n",
      "Iteration 215, loss = 0.22112519\n",
      "Iteration 216, loss = 0.22039932\n",
      "Iteration 217, loss = 0.22082412\n",
      "Iteration 218, loss = 0.22028973\n",
      "Iteration 219, loss = 0.21951795\n",
      "Iteration 220, loss = 0.21933439\n",
      "Iteration 221, loss = 0.21894797\n",
      "Iteration 222, loss = 0.21898844\n",
      "Iteration 223, loss = 0.21829992\n",
      "Iteration 224, loss = 0.21854981\n",
      "Iteration 225, loss = 0.21798741\n",
      "Iteration 226, loss = 0.21796752\n",
      "Iteration 227, loss = 0.21739754\n",
      "Iteration 228, loss = 0.21722092\n",
      "Iteration 229, loss = 0.21729261\n",
      "Iteration 230, loss = 0.21652261\n",
      "Iteration 231, loss = 0.21718127\n",
      "Iteration 232, loss = 0.21763629\n",
      "Iteration 233, loss = 0.21676887\n",
      "Iteration 234, loss = 0.21557554\n",
      "Iteration 235, loss = 0.21584698\n",
      "Iteration 236, loss = 0.21567311\n",
      "Iteration 237, loss = 0.21512772\n",
      "Iteration 238, loss = 0.21538516\n",
      "Iteration 239, loss = 0.21468100\n",
      "Iteration 240, loss = 0.21449126\n",
      "Iteration 241, loss = 0.21438270\n",
      "Iteration 242, loss = 0.21432381\n",
      "Iteration 243, loss = 0.21393307\n",
      "Iteration 244, loss = 0.21400614\n",
      "Iteration 245, loss = 0.21370062\n",
      "Iteration 246, loss = 0.21365564\n",
      "Iteration 247, loss = 0.21369467\n",
      "Iteration 248, loss = 0.21325215\n",
      "Iteration 249, loss = 0.21330267\n",
      "Iteration 250, loss = 0.21363741\n",
      "Iteration 251, loss = 0.21247399\n",
      "Iteration 252, loss = 0.21272975\n",
      "Iteration 253, loss = 0.21239102\n",
      "Iteration 254, loss = 0.21204533\n",
      "Iteration 255, loss = 0.21183303\n",
      "Iteration 256, loss = 0.21236269\n",
      "Iteration 257, loss = 0.21180086\n",
      "Iteration 258, loss = 0.21116189\n",
      "Iteration 259, loss = 0.21078742\n",
      "Iteration 260, loss = 0.21051006\n",
      "Iteration 261, loss = 0.21036308\n",
      "Iteration 262, loss = 0.21004202\n",
      "Iteration 263, loss = 0.20976290\n",
      "Iteration 264, loss = 0.20971737\n",
      "Iteration 265, loss = 0.20932481\n",
      "Iteration 266, loss = 0.20956805\n",
      "Iteration 267, loss = 0.20879112\n",
      "Iteration 268, loss = 0.20878695\n",
      "Iteration 269, loss = 0.20877104\n",
      "Iteration 270, loss = 0.20830566\n",
      "Iteration 271, loss = 0.20816601\n",
      "Iteration 272, loss = 0.20834932\n",
      "Iteration 273, loss = 0.20793806\n",
      "Iteration 274, loss = 0.20887803\n",
      "Iteration 275, loss = 0.20794348\n",
      "Iteration 276, loss = 0.20803083\n",
      "Iteration 277, loss = 0.20822888\n",
      "Iteration 278, loss = 0.20717624\n",
      "Iteration 279, loss = 0.20702208\n",
      "Iteration 280, loss = 0.20696220\n",
      "Iteration 281, loss = 0.20738621\n",
      "Iteration 282, loss = 0.20637716\n",
      "Iteration 283, loss = 0.20668148\n",
      "Iteration 284, loss = 0.20592087\n",
      "Iteration 285, loss = 0.20600320\n",
      "Iteration 286, loss = 0.20578038\n",
      "Iteration 287, loss = 0.20591337\n",
      "Iteration 288, loss = 0.20549509\n",
      "Iteration 289, loss = 0.20522559\n",
      "Iteration 290, loss = 0.20486634\n",
      "Iteration 291, loss = 0.20500588\n",
      "Iteration 292, loss = 0.20582940\n",
      "Iteration 293, loss = 0.20559636\n",
      "Iteration 294, loss = 0.20480165\n",
      "Iteration 295, loss = 0.20428241\n",
      "Iteration 296, loss = 0.20485066\n",
      "Iteration 297, loss = 0.20442955\n",
      "Iteration 298, loss = 0.20383730\n",
      "Iteration 299, loss = 0.20358355\n",
      "Iteration 300, loss = 0.20376990\n",
      "Iteration 301, loss = 0.20392947\n",
      "Iteration 302, loss = 0.20319693\n",
      "Iteration 303, loss = 0.20260913\n",
      "Iteration 304, loss = 0.20311096\n",
      "Iteration 305, loss = 0.20292614\n",
      "Iteration 306, loss = 0.20286286\n",
      "Iteration 307, loss = 0.20216023\n",
      "Iteration 308, loss = 0.20232780\n",
      "Iteration 309, loss = 0.20221581\n",
      "Iteration 310, loss = 0.20170542\n",
      "Iteration 311, loss = 0.20245144\n",
      "Iteration 312, loss = 0.20159845\n",
      "Iteration 313, loss = 0.20199774\n",
      "Iteration 314, loss = 0.20160111\n",
      "Iteration 315, loss = 0.20143123\n",
      "Iteration 316, loss = 0.20125174\n",
      "Iteration 317, loss = 0.20132005\n",
      "Iteration 318, loss = 0.20117452\n",
      "Iteration 319, loss = 0.20082159\n",
      "Iteration 320, loss = 0.20109132\n",
      "Iteration 321, loss = 0.20023679\n",
      "Iteration 322, loss = 0.20056329\n",
      "Iteration 323, loss = 0.19966697\n",
      "Iteration 324, loss = 0.20065275\n",
      "Iteration 325, loss = 0.20003545\n",
      "Iteration 326, loss = 0.19994237\n",
      "Iteration 327, loss = 0.19932514\n",
      "Iteration 328, loss = 0.19892905\n",
      "Iteration 329, loss = 0.19924305\n",
      "Iteration 330, loss = 0.19888925\n",
      "Iteration 331, loss = 0.19903790\n",
      "Iteration 332, loss = 0.19856859\n",
      "Iteration 333, loss = 0.19832595\n",
      "Iteration 334, loss = 0.19872071\n",
      "Iteration 335, loss = 0.19849087\n",
      "Iteration 336, loss = 0.19968437\n",
      "Iteration 337, loss = 0.19882051\n",
      "Iteration 338, loss = 0.19747249\n",
      "Iteration 339, loss = 0.19799872\n",
      "Iteration 340, loss = 0.19768517\n",
      "Iteration 341, loss = 0.19721393\n",
      "Iteration 342, loss = 0.19727848\n",
      "Iteration 343, loss = 0.19684994\n",
      "Iteration 344, loss = 0.19661392\n",
      "Iteration 345, loss = 0.19756768\n",
      "Iteration 346, loss = 0.19744441\n",
      "Iteration 347, loss = 0.19621086\n",
      "Iteration 348, loss = 0.19625154\n",
      "Iteration 349, loss = 0.19624010\n",
      "Iteration 350, loss = 0.19560770\n",
      "Iteration 351, loss = 0.19566421\n",
      "Iteration 352, loss = 0.19662337\n",
      "Iteration 353, loss = 0.19602465\n",
      "Iteration 354, loss = 0.19532325\n",
      "Iteration 355, loss = 0.19523098\n",
      "Iteration 356, loss = 0.19593640\n",
      "Iteration 357, loss = 0.19546910\n",
      "Iteration 358, loss = 0.19641408\n",
      "Iteration 359, loss = 0.19470315\n",
      "Iteration 360, loss = 0.19495520\n",
      "Iteration 361, loss = 0.19492495\n",
      "Iteration 362, loss = 0.19472160\n",
      "Iteration 363, loss = 0.19484190\n",
      "Iteration 364, loss = 0.19440253\n",
      "Iteration 365, loss = 0.19427434\n",
      "Iteration 366, loss = 0.19419385\n",
      "Iteration 367, loss = 0.19434821\n",
      "Iteration 368, loss = 0.19404911\n",
      "Iteration 369, loss = 0.19418749\n",
      "Iteration 370, loss = 0.19358012\n",
      "Iteration 371, loss = 0.19360332\n",
      "Iteration 372, loss = 0.19393437\n",
      "Iteration 373, loss = 0.19312413\n",
      "Iteration 374, loss = 0.19370305\n",
      "Iteration 375, loss = 0.19317668\n",
      "Iteration 376, loss = 0.19432171\n",
      "Iteration 377, loss = 0.19469023\n",
      "Iteration 378, loss = 0.19319304\n",
      "Iteration 379, loss = 0.19284775\n",
      "Iteration 380, loss = 0.19296373\n",
      "Iteration 381, loss = 0.19272017\n",
      "Iteration 382, loss = 0.19241643\n",
      "Iteration 383, loss = 0.19215149\n",
      "Iteration 384, loss = 0.19223697\n",
      "Iteration 385, loss = 0.19249813\n",
      "Iteration 386, loss = 0.19262265\n",
      "Iteration 387, loss = 0.19201204\n",
      "Iteration 388, loss = 0.19228868\n",
      "Iteration 389, loss = 0.19154139\n",
      "Iteration 390, loss = 0.19208587\n",
      "Iteration 391, loss = 0.19224310\n",
      "Iteration 392, loss = 0.19178334\n",
      "Iteration 393, loss = 0.19166400\n",
      "Iteration 394, loss = 0.19155835\n",
      "Iteration 395, loss = 0.19150931\n",
      "Iteration 396, loss = 0.19126393\n",
      "Iteration 397, loss = 0.19130613\n",
      "Iteration 398, loss = 0.19166627\n",
      "Iteration 399, loss = 0.19156980\n",
      "Iteration 400, loss = 0.19076926\n",
      "Iteration 401, loss = 0.19108624\n",
      "Iteration 402, loss = 0.19070659\n",
      "Iteration 403, loss = 0.19075976\n",
      "Iteration 404, loss = 0.19030665\n",
      "Iteration 405, loss = 0.19106569\n",
      "Iteration 406, loss = 0.19065992\n",
      "Iteration 407, loss = 0.18984930\n",
      "Iteration 408, loss = 0.19023096\n",
      "Iteration 409, loss = 0.18985544\n",
      "Iteration 410, loss = 0.19042809\n",
      "Iteration 411, loss = 0.19017003\n",
      "Iteration 412, loss = 0.18949882\n",
      "Iteration 413, loss = 0.19035610\n",
      "Iteration 414, loss = 0.19059606\n",
      "Iteration 415, loss = 0.18952850\n",
      "Iteration 416, loss = 0.18992111\n",
      "Iteration 417, loss = 0.18930032\n",
      "Iteration 418, loss = 0.18956257\n",
      "Iteration 419, loss = 0.18901446\n",
      "Iteration 420, loss = 0.18909598\n",
      "Iteration 421, loss = 0.18914163\n",
      "Iteration 422, loss = 0.18921046\n",
      "Iteration 423, loss = 0.19018871\n",
      "Iteration 424, loss = 0.18919399\n",
      "Iteration 425, loss = 0.18865639\n",
      "Iteration 426, loss = 0.18916010\n",
      "Iteration 427, loss = 0.18847290\n",
      "Iteration 428, loss = 0.18925268\n",
      "Iteration 429, loss = 0.18897377\n",
      "Iteration 430, loss = 0.18915724\n",
      "Iteration 431, loss = 0.18844432\n",
      "Iteration 432, loss = 0.18874707\n",
      "Iteration 433, loss = 0.18843931\n",
      "Iteration 434, loss = 0.18942231\n",
      "Iteration 435, loss = 0.18901527\n",
      "Iteration 436, loss = 0.18816283\n",
      "Iteration 437, loss = 0.18781785\n",
      "Iteration 438, loss = 0.18799766\n",
      "Iteration 439, loss = 0.18800134\n",
      "Iteration 440, loss = 0.18775661\n",
      "Iteration 441, loss = 0.18817400\n",
      "Iteration 442, loss = 0.18799625\n",
      "Iteration 443, loss = 0.18792511\n",
      "Iteration 444, loss = 0.18767579\n",
      "Iteration 445, loss = 0.18812048\n",
      "Iteration 446, loss = 0.18741043\n",
      "Iteration 447, loss = 0.18750835\n",
      "Iteration 448, loss = 0.18730022\n",
      "Iteration 449, loss = 0.18718179\n",
      "Iteration 450, loss = 0.18733362\n",
      "Iteration 451, loss = 0.18693180\n",
      "Iteration 452, loss = 0.18733855\n",
      "Iteration 453, loss = 0.18727716\n",
      "Iteration 454, loss = 0.18690059\n",
      "Iteration 455, loss = 0.18695635\n",
      "Iteration 456, loss = 0.18651287\n",
      "Iteration 457, loss = 0.18744846\n",
      "Iteration 458, loss = 0.18685623\n",
      "Iteration 459, loss = 0.18692282\n",
      "Iteration 460, loss = 0.18716913\n",
      "Iteration 461, loss = 0.18688347\n",
      "Iteration 462, loss = 0.18708281\n",
      "Iteration 463, loss = 0.18679476\n",
      "Iteration 464, loss = 0.18664837\n",
      "Iteration 465, loss = 0.18653868\n",
      "Iteration 466, loss = 0.18671963\n",
      "Iteration 467, loss = 0.18666526\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77209852\n",
      "Iteration 2, loss = 0.74545128\n",
      "Iteration 3, loss = 0.72379122\n",
      "Iteration 4, loss = 0.70628992\n",
      "Iteration 5, loss = 0.69151830\n",
      "Iteration 6, loss = 0.67875897\n",
      "Iteration 7, loss = 0.66612712\n",
      "Iteration 8, loss = 0.65173853\n",
      "Iteration 9, loss = 0.63466151\n",
      "Iteration 10, loss = 0.61590905\n",
      "Iteration 11, loss = 0.59646730\n",
      "Iteration 12, loss = 0.57786901\n",
      "Iteration 13, loss = 0.56127316\n",
      "Iteration 14, loss = 0.54689604\n",
      "Iteration 15, loss = 0.53487880\n",
      "Iteration 16, loss = 0.52453545\n",
      "Iteration 17, loss = 0.51507695\n",
      "Iteration 18, loss = 0.50687581\n",
      "Iteration 19, loss = 0.49897664\n",
      "Iteration 20, loss = 0.49222102\n",
      "Iteration 21, loss = 0.48576061\n",
      "Iteration 22, loss = 0.48006151\n",
      "Iteration 23, loss = 0.47454871\n",
      "Iteration 24, loss = 0.46943139\n",
      "Iteration 25, loss = 0.46461550\n",
      "Iteration 26, loss = 0.45982203\n",
      "Iteration 27, loss = 0.45529762\n",
      "Iteration 28, loss = 0.45105836\n",
      "Iteration 29, loss = 0.44680209\n",
      "Iteration 30, loss = 0.44289633\n",
      "Iteration 31, loss = 0.43910830\n",
      "Iteration 32, loss = 0.43524999\n",
      "Iteration 33, loss = 0.43152838\n",
      "Iteration 34, loss = 0.42803748\n",
      "Iteration 35, loss = 0.42438720\n",
      "Iteration 36, loss = 0.42071461\n",
      "Iteration 37, loss = 0.41728058\n",
      "Iteration 38, loss = 0.41393384\n",
      "Iteration 39, loss = 0.41068134\n",
      "Iteration 40, loss = 0.40732173\n",
      "Iteration 41, loss = 0.40412076\n",
      "Iteration 42, loss = 0.40110683\n",
      "Iteration 43, loss = 0.39800795\n",
      "Iteration 44, loss = 0.39514348\n",
      "Iteration 45, loss = 0.39261434\n",
      "Iteration 46, loss = 0.38947498\n",
      "Iteration 47, loss = 0.38693006\n",
      "Iteration 48, loss = 0.38442177\n",
      "Iteration 49, loss = 0.38200278\n",
      "Iteration 50, loss = 0.37937785\n",
      "Iteration 51, loss = 0.37702637\n",
      "Iteration 52, loss = 0.37466411\n",
      "Iteration 53, loss = 0.37259062\n",
      "Iteration 54, loss = 0.37033307\n",
      "Iteration 55, loss = 0.36790615\n",
      "Iteration 56, loss = 0.36584585\n",
      "Iteration 57, loss = 0.36392735\n",
      "Iteration 58, loss = 0.36180843\n",
      "Iteration 59, loss = 0.35962634\n",
      "Iteration 60, loss = 0.35742249\n",
      "Iteration 61, loss = 0.35551929\n",
      "Iteration 62, loss = 0.35349395\n",
      "Iteration 63, loss = 0.35142424\n",
      "Iteration 64, loss = 0.34985235\n",
      "Iteration 65, loss = 0.34841065\n",
      "Iteration 66, loss = 0.34591915\n",
      "Iteration 67, loss = 0.34390320\n",
      "Iteration 68, loss = 0.34198130\n",
      "Iteration 69, loss = 0.34024361\n",
      "Iteration 70, loss = 0.33860076\n",
      "Iteration 71, loss = 0.33647775\n",
      "Iteration 72, loss = 0.33475448\n",
      "Iteration 73, loss = 0.33295975\n",
      "Iteration 74, loss = 0.33112784\n",
      "Iteration 75, loss = 0.32962385\n",
      "Iteration 76, loss = 0.32833658\n",
      "Iteration 77, loss = 0.32626558\n",
      "Iteration 78, loss = 0.32474680\n",
      "Iteration 79, loss = 0.32379637\n",
      "Iteration 80, loss = 0.32162908\n",
      "Iteration 81, loss = 0.32001417\n",
      "Iteration 82, loss = 0.31898801\n",
      "Iteration 83, loss = 0.31698243\n",
      "Iteration 84, loss = 0.31567344\n",
      "Iteration 85, loss = 0.31401337\n",
      "Iteration 86, loss = 0.31251562\n",
      "Iteration 87, loss = 0.31103029\n",
      "Iteration 88, loss = 0.30955456\n",
      "Iteration 89, loss = 0.30862853\n",
      "Iteration 90, loss = 0.30669196\n",
      "Iteration 91, loss = 0.30556441\n",
      "Iteration 92, loss = 0.30401704\n",
      "Iteration 93, loss = 0.30261799\n",
      "Iteration 94, loss = 0.30139873\n",
      "Iteration 95, loss = 0.29992808\n",
      "Iteration 96, loss = 0.29860270\n",
      "Iteration 97, loss = 0.29742751\n",
      "Iteration 98, loss = 0.29602470\n",
      "Iteration 99, loss = 0.29478236\n",
      "Iteration 100, loss = 0.29360634\n",
      "Iteration 101, loss = 0.29242719\n",
      "Iteration 102, loss = 0.29134466\n",
      "Iteration 103, loss = 0.29012948\n",
      "Iteration 104, loss = 0.28889672\n",
      "Iteration 105, loss = 0.28771740\n",
      "Iteration 106, loss = 0.28672014\n",
      "Iteration 107, loss = 0.28595066\n",
      "Iteration 108, loss = 0.28465824\n",
      "Iteration 109, loss = 0.28360920\n",
      "Iteration 110, loss = 0.28225162\n",
      "Iteration 111, loss = 0.28146735\n",
      "Iteration 112, loss = 0.28045774\n",
      "Iteration 113, loss = 0.27976105\n",
      "Iteration 114, loss = 0.27825231\n",
      "Iteration 115, loss = 0.27779736\n",
      "Iteration 116, loss = 0.27715644\n",
      "Iteration 117, loss = 0.27534782\n",
      "Iteration 118, loss = 0.27478348\n",
      "Iteration 119, loss = 0.27377420\n",
      "Iteration 120, loss = 0.27259072\n",
      "Iteration 121, loss = 0.27185524\n",
      "Iteration 122, loss = 0.27084741\n",
      "Iteration 123, loss = 0.27011509\n",
      "Iteration 124, loss = 0.26895500\n",
      "Iteration 125, loss = 0.26810726\n",
      "Iteration 126, loss = 0.26717688\n",
      "Iteration 127, loss = 0.26631525\n",
      "Iteration 128, loss = 0.26545078\n",
      "Iteration 129, loss = 0.26458707\n",
      "Iteration 130, loss = 0.26397307\n",
      "Iteration 131, loss = 0.26277325\n",
      "Iteration 132, loss = 0.26189900\n",
      "Iteration 133, loss = 0.26164685\n",
      "Iteration 134, loss = 0.26065413\n",
      "Iteration 135, loss = 0.25963178\n",
      "Iteration 136, loss = 0.25908738\n",
      "Iteration 137, loss = 0.25873825\n",
      "Iteration 138, loss = 0.25765876\n",
      "Iteration 139, loss = 0.25650630\n",
      "Iteration 140, loss = 0.25619270\n",
      "Iteration 141, loss = 0.25524509\n",
      "Iteration 142, loss = 0.25450160\n",
      "Iteration 143, loss = 0.25374403\n",
      "Iteration 144, loss = 0.25324550\n",
      "Iteration 145, loss = 0.25308123\n",
      "Iteration 146, loss = 0.25210283\n",
      "Iteration 147, loss = 0.25103809\n",
      "Iteration 148, loss = 0.25048826\n",
      "Iteration 149, loss = 0.24999140\n",
      "Iteration 150, loss = 0.24936894\n",
      "Iteration 151, loss = 0.24877294\n",
      "Iteration 152, loss = 0.24843214\n",
      "Iteration 153, loss = 0.24778803\n",
      "Iteration 154, loss = 0.24717997\n",
      "Iteration 155, loss = 0.24652321\n",
      "Iteration 156, loss = 0.24606139\n",
      "Iteration 157, loss = 0.24550414\n",
      "Iteration 158, loss = 0.24543588\n",
      "Iteration 159, loss = 0.24432205\n",
      "Iteration 160, loss = 0.24372328\n",
      "Iteration 161, loss = 0.24343091\n",
      "Iteration 162, loss = 0.24241067\n",
      "Iteration 163, loss = 0.24198415\n",
      "Iteration 164, loss = 0.24125965\n",
      "Iteration 165, loss = 0.24078686\n",
      "Iteration 166, loss = 0.24047162\n",
      "Iteration 167, loss = 0.24007189\n",
      "Iteration 168, loss = 0.23963487\n",
      "Iteration 169, loss = 0.23896037\n",
      "Iteration 170, loss = 0.23844702\n",
      "Iteration 171, loss = 0.23840203\n",
      "Iteration 172, loss = 0.23760010\n",
      "Iteration 173, loss = 0.23713813\n",
      "Iteration 174, loss = 0.23655138\n",
      "Iteration 175, loss = 0.23632668\n",
      "Iteration 176, loss = 0.23546370\n",
      "Iteration 177, loss = 0.23519286\n",
      "Iteration 178, loss = 0.23434717\n",
      "Iteration 179, loss = 0.23420332\n",
      "Iteration 180, loss = 0.23332243\n",
      "Iteration 181, loss = 0.23296383\n",
      "Iteration 182, loss = 0.23243245\n",
      "Iteration 183, loss = 0.23250088\n",
      "Iteration 184, loss = 0.23132069\n",
      "Iteration 185, loss = 0.23113531\n",
      "Iteration 186, loss = 0.23049877\n",
      "Iteration 187, loss = 0.22993850\n",
      "Iteration 188, loss = 0.22963093\n",
      "Iteration 189, loss = 0.22934727\n",
      "Iteration 190, loss = 0.22876565\n",
      "Iteration 191, loss = 0.22830162\n",
      "Iteration 192, loss = 0.22814309\n",
      "Iteration 193, loss = 0.22723235\n",
      "Iteration 194, loss = 0.22706313\n",
      "Iteration 195, loss = 0.22645457\n",
      "Iteration 196, loss = 0.22640500\n",
      "Iteration 197, loss = 0.22603672\n",
      "Iteration 198, loss = 0.22541162\n",
      "Iteration 199, loss = 0.22499586\n",
      "Iteration 200, loss = 0.22459347\n",
      "Iteration 201, loss = 0.22444853\n",
      "Iteration 202, loss = 0.22409346\n",
      "Iteration 203, loss = 0.22370330\n",
      "Iteration 204, loss = 0.22314345\n",
      "Iteration 205, loss = 0.22310914\n",
      "Iteration 206, loss = 0.22245298\n",
      "Iteration 207, loss = 0.22236951\n",
      "Iteration 208, loss = 0.22189950\n",
      "Iteration 209, loss = 0.22164649\n",
      "Iteration 210, loss = 0.22156722\n",
      "Iteration 211, loss = 0.22102575\n",
      "Iteration 212, loss = 0.22059741\n",
      "Iteration 213, loss = 0.22073147\n",
      "Iteration 214, loss = 0.21990383\n",
      "Iteration 215, loss = 0.21990719\n",
      "Iteration 216, loss = 0.21963034\n",
      "Iteration 217, loss = 0.21910851\n",
      "Iteration 218, loss = 0.21909034\n",
      "Iteration 219, loss = 0.21895730\n",
      "Iteration 220, loss = 0.21827568\n",
      "Iteration 221, loss = 0.21802612\n",
      "Iteration 222, loss = 0.21783408\n",
      "Iteration 223, loss = 0.21746834\n",
      "Iteration 224, loss = 0.21737855\n",
      "Iteration 225, loss = 0.21708434\n",
      "Iteration 226, loss = 0.21683960\n",
      "Iteration 227, loss = 0.21612025\n",
      "Iteration 228, loss = 0.21632733\n",
      "Iteration 229, loss = 0.21605119\n",
      "Iteration 230, loss = 0.21573185\n",
      "Iteration 231, loss = 0.21557511\n",
      "Iteration 232, loss = 0.21559737\n",
      "Iteration 233, loss = 0.21482010\n",
      "Iteration 234, loss = 0.21547645\n",
      "Iteration 235, loss = 0.21473413\n",
      "Iteration 236, loss = 0.21463852\n",
      "Iteration 237, loss = 0.21409737\n",
      "Iteration 238, loss = 0.21400437\n",
      "Iteration 239, loss = 0.21364358\n",
      "Iteration 240, loss = 0.21308808\n",
      "Iteration 241, loss = 0.21332246\n",
      "Iteration 242, loss = 0.21290223\n",
      "Iteration 243, loss = 0.21224114\n",
      "Iteration 244, loss = 0.21233388\n",
      "Iteration 245, loss = 0.21229211\n",
      "Iteration 246, loss = 0.21147058\n",
      "Iteration 247, loss = 0.21187405\n",
      "Iteration 248, loss = 0.21121197\n",
      "Iteration 249, loss = 0.21088039\n",
      "Iteration 250, loss = 0.21107626\n",
      "Iteration 251, loss = 0.21191668\n",
      "Iteration 252, loss = 0.21006198\n",
      "Iteration 253, loss = 0.21059383\n",
      "Iteration 254, loss = 0.20996457\n",
      "Iteration 255, loss = 0.20988993\n",
      "Iteration 256, loss = 0.21006173\n",
      "Iteration 257, loss = 0.20923471\n",
      "Iteration 258, loss = 0.20908790\n",
      "Iteration 259, loss = 0.20883814\n",
      "Iteration 260, loss = 0.20947964\n",
      "Iteration 261, loss = 0.20834830\n",
      "Iteration 262, loss = 0.20844621\n",
      "Iteration 263, loss = 0.20813912\n",
      "Iteration 264, loss = 0.20796797\n",
      "Iteration 265, loss = 0.20811482\n",
      "Iteration 266, loss = 0.20811665\n",
      "Iteration 267, loss = 0.20845277\n",
      "Iteration 268, loss = 0.20764574\n",
      "Iteration 269, loss = 0.20724254\n",
      "Iteration 270, loss = 0.20691053\n",
      "Iteration 271, loss = 0.20668231\n",
      "Iteration 272, loss = 0.20680212\n",
      "Iteration 273, loss = 0.20612306\n",
      "Iteration 274, loss = 0.20631211\n",
      "Iteration 275, loss = 0.20635290\n",
      "Iteration 276, loss = 0.20625422\n",
      "Iteration 277, loss = 0.20567215\n",
      "Iteration 278, loss = 0.20592156\n",
      "Iteration 279, loss = 0.20569622\n",
      "Iteration 280, loss = 0.20620095\n",
      "Iteration 281, loss = 0.20485373\n",
      "Iteration 282, loss = 0.20549439\n",
      "Iteration 283, loss = 0.20495670\n",
      "Iteration 284, loss = 0.20491235\n",
      "Iteration 285, loss = 0.20474213\n",
      "Iteration 286, loss = 0.20469074\n",
      "Iteration 287, loss = 0.20447243\n",
      "Iteration 288, loss = 0.20408500\n",
      "Iteration 289, loss = 0.20386336\n",
      "Iteration 290, loss = 0.20379252\n",
      "Iteration 291, loss = 0.20393337\n",
      "Iteration 292, loss = 0.20321048\n",
      "Iteration 293, loss = 0.20391064\n",
      "Iteration 294, loss = 0.20328860\n",
      "Iteration 295, loss = 0.20437335\n",
      "Iteration 296, loss = 0.20389395\n",
      "Iteration 297, loss = 0.20277599\n",
      "Iteration 298, loss = 0.20262794\n",
      "Iteration 299, loss = 0.20261203\n",
      "Iteration 300, loss = 0.20212570\n",
      "Iteration 301, loss = 0.20246090\n",
      "Iteration 302, loss = 0.20207697\n",
      "Iteration 303, loss = 0.20199092\n",
      "Iteration 304, loss = 0.20183327\n",
      "Iteration 305, loss = 0.20206083\n",
      "Iteration 306, loss = 0.20119122\n",
      "Iteration 307, loss = 0.20127082\n",
      "Iteration 308, loss = 0.20144699\n",
      "Iteration 309, loss = 0.20157891\n",
      "Iteration 310, loss = 0.20083313\n",
      "Iteration 311, loss = 0.20107753\n",
      "Iteration 312, loss = 0.20070820\n",
      "Iteration 313, loss = 0.20052812\n",
      "Iteration 314, loss = 0.20065712\n",
      "Iteration 315, loss = 0.20035729\n",
      "Iteration 316, loss = 0.20011068\n",
      "Iteration 317, loss = 0.20028235\n",
      "Iteration 318, loss = 0.20004856\n",
      "Iteration 319, loss = 0.19979956\n",
      "Iteration 320, loss = 0.19976103\n",
      "Iteration 321, loss = 0.19960052\n",
      "Iteration 322, loss = 0.19965929\n",
      "Iteration 323, loss = 0.19923271\n",
      "Iteration 324, loss = 0.19889183\n",
      "Iteration 325, loss = 0.19892924\n",
      "Iteration 326, loss = 0.19901732\n",
      "Iteration 327, loss = 0.19844850\n",
      "Iteration 328, loss = 0.19842436\n",
      "Iteration 329, loss = 0.19828532\n",
      "Iteration 330, loss = 0.19841257\n",
      "Iteration 331, loss = 0.19802038\n",
      "Iteration 332, loss = 0.19813906\n",
      "Iteration 333, loss = 0.19786926\n",
      "Iteration 334, loss = 0.19784113\n",
      "Iteration 335, loss = 0.19729640\n",
      "Iteration 336, loss = 0.19774822\n",
      "Iteration 337, loss = 0.19769947\n",
      "Iteration 338, loss = 0.19740284\n",
      "Iteration 339, loss = 0.19728024\n",
      "Iteration 340, loss = 0.19707484\n",
      "Iteration 341, loss = 0.19704378\n",
      "Iteration 342, loss = 0.19689821\n",
      "Iteration 343, loss = 0.19666517\n",
      "Iteration 344, loss = 0.19704370\n",
      "Iteration 345, loss = 0.19665446\n",
      "Iteration 346, loss = 0.19621807\n",
      "Iteration 347, loss = 0.19629003\n",
      "Iteration 348, loss = 0.19607149\n",
      "Iteration 349, loss = 0.19602611\n",
      "Iteration 350, loss = 0.19631579\n",
      "Iteration 351, loss = 0.19580589\n",
      "Iteration 352, loss = 0.19597896\n",
      "Iteration 353, loss = 0.19548252\n",
      "Iteration 354, loss = 0.19569214\n",
      "Iteration 355, loss = 0.19507421\n",
      "Iteration 356, loss = 0.19513917\n",
      "Iteration 357, loss = 0.19492111\n",
      "Iteration 358, loss = 0.19486623\n",
      "Iteration 359, loss = 0.19477091\n",
      "Iteration 360, loss = 0.19478710\n",
      "Iteration 361, loss = 0.19489000\n",
      "Iteration 362, loss = 0.19476905\n",
      "Iteration 363, loss = 0.19446154\n",
      "Iteration 364, loss = 0.19430067\n",
      "Iteration 365, loss = 0.19416019\n",
      "Iteration 366, loss = 0.19415637\n",
      "Iteration 367, loss = 0.19405896\n",
      "Iteration 368, loss = 0.19410056\n",
      "Iteration 369, loss = 0.19409588\n",
      "Iteration 370, loss = 0.19353154\n",
      "Iteration 371, loss = 0.19350683\n",
      "Iteration 372, loss = 0.19336749\n",
      "Iteration 373, loss = 0.19385408\n",
      "Iteration 374, loss = 0.19326258\n",
      "Iteration 375, loss = 0.19318091\n",
      "Iteration 376, loss = 0.19307848\n",
      "Iteration 377, loss = 0.19304781\n",
      "Iteration 378, loss = 0.19263315\n",
      "Iteration 379, loss = 0.19295271\n",
      "Iteration 380, loss = 0.19318530\n",
      "Iteration 381, loss = 0.19288077\n",
      "Iteration 382, loss = 0.19251050\n",
      "Iteration 383, loss = 0.19271297\n",
      "Iteration 384, loss = 0.19252883\n",
      "Iteration 385, loss = 0.19223761\n",
      "Iteration 386, loss = 0.19218542\n",
      "Iteration 387, loss = 0.19222161\n",
      "Iteration 388, loss = 0.19226636\n",
      "Iteration 389, loss = 0.19209022\n",
      "Iteration 390, loss = 0.19196693\n",
      "Iteration 391, loss = 0.19216525\n",
      "Iteration 392, loss = 0.19210302\n",
      "Iteration 393, loss = 0.19176381\n",
      "Iteration 394, loss = 0.19153532\n",
      "Iteration 395, loss = 0.19153796\n",
      "Iteration 396, loss = 0.19142039\n",
      "Iteration 397, loss = 0.19192134\n",
      "Iteration 398, loss = 0.19178842\n",
      "Iteration 399, loss = 0.19135592\n",
      "Iteration 400, loss = 0.19163739\n",
      "Iteration 401, loss = 0.19133335\n",
      "Iteration 402, loss = 0.19155898\n",
      "Iteration 403, loss = 0.19107176\n",
      "Iteration 404, loss = 0.19071350\n",
      "Iteration 405, loss = 0.19092254\n",
      "Iteration 406, loss = 0.19056634\n",
      "Iteration 407, loss = 0.19089975\n",
      "Iteration 408, loss = 0.19080412\n",
      "Iteration 409, loss = 0.19062415\n",
      "Iteration 410, loss = 0.19056132\n",
      "Iteration 411, loss = 0.19079606\n",
      "Iteration 412, loss = 0.19115636\n",
      "Iteration 413, loss = 0.19001231\n",
      "Iteration 414, loss = 0.18984387\n",
      "Iteration 415, loss = 0.18995016\n",
      "Iteration 416, loss = 0.18930995\n",
      "Iteration 417, loss = 0.18958327\n",
      "Iteration 418, loss = 0.18937686\n",
      "Iteration 419, loss = 0.18930243\n",
      "Iteration 420, loss = 0.18976542\n",
      "Iteration 421, loss = 0.18908814\n",
      "Iteration 422, loss = 0.18907978\n",
      "Iteration 423, loss = 0.18882547\n",
      "Iteration 424, loss = 0.18936359\n",
      "Iteration 425, loss = 0.18900926\n",
      "Iteration 426, loss = 0.18891913\n",
      "Iteration 427, loss = 0.18877867\n",
      "Iteration 428, loss = 0.18872694\n",
      "Iteration 429, loss = 0.18936792\n",
      "Iteration 430, loss = 0.18851229\n",
      "Iteration 431, loss = 0.18843734\n",
      "Iteration 432, loss = 0.18839848\n",
      "Iteration 433, loss = 0.18897407\n",
      "Iteration 434, loss = 0.18837432\n",
      "Iteration 435, loss = 0.18909768\n",
      "Iteration 436, loss = 0.18823059\n",
      "Iteration 437, loss = 0.18886691\n",
      "Iteration 438, loss = 0.18809511\n",
      "Iteration 439, loss = 0.18902336\n",
      "Iteration 440, loss = 0.18926378\n",
      "Iteration 441, loss = 0.19011134\n",
      "Iteration 442, loss = 0.18772350\n",
      "Iteration 443, loss = 0.18791757\n",
      "Iteration 444, loss = 0.18846904\n",
      "Iteration 445, loss = 0.18797442\n",
      "Iteration 446, loss = 0.18765706\n",
      "Iteration 447, loss = 0.18732103\n",
      "Iteration 448, loss = 0.18758718\n",
      "Iteration 449, loss = 0.18710939\n",
      "Iteration 450, loss = 0.18773104\n",
      "Iteration 451, loss = 0.18754309\n",
      "Iteration 452, loss = 0.18782943\n",
      "Iteration 453, loss = 0.18706566\n",
      "Iteration 454, loss = 0.18717488\n",
      "Iteration 455, loss = 0.18708517\n",
      "Iteration 456, loss = 0.18692984\n",
      "Iteration 457, loss = 0.18668138\n",
      "Iteration 458, loss = 0.18667489\n",
      "Iteration 459, loss = 0.18665967\n",
      "Iteration 460, loss = 0.18680230\n",
      "Iteration 461, loss = 0.18636954\n",
      "Iteration 462, loss = 0.18664703\n",
      "Iteration 463, loss = 0.18675386\n",
      "Iteration 464, loss = 0.18669890\n",
      "Iteration 465, loss = 0.18605442\n",
      "Iteration 466, loss = 0.18650370\n",
      "Iteration 467, loss = 0.18699220\n",
      "Iteration 468, loss = 0.18678019\n",
      "Iteration 469, loss = 0.18632719\n",
      "Iteration 470, loss = 0.18630547\n",
      "Iteration 471, loss = 0.18631804\n",
      "Iteration 472, loss = 0.18606182\n",
      "Iteration 473, loss = 0.18552805\n",
      "Iteration 474, loss = 0.18551079\n",
      "Iteration 475, loss = 0.18556788\n",
      "Iteration 476, loss = 0.18556129\n",
      "Iteration 477, loss = 0.18531335\n",
      "Iteration 478, loss = 0.18541685\n",
      "Iteration 479, loss = 0.18522042\n",
      "Iteration 480, loss = 0.18553672\n",
      "Iteration 481, loss = 0.18512316\n",
      "Iteration 482, loss = 0.18543264\n",
      "Iteration 483, loss = 0.18512685\n",
      "Iteration 484, loss = 0.18473154\n",
      "Iteration 485, loss = 0.18507563\n",
      "Iteration 486, loss = 0.18465473\n",
      "Iteration 487, loss = 0.18444631\n",
      "Iteration 488, loss = 0.18502226\n",
      "Iteration 489, loss = 0.18456148\n",
      "Iteration 490, loss = 0.18421837\n",
      "Iteration 491, loss = 0.18428082\n",
      "Iteration 492, loss = 0.18451254\n",
      "Iteration 493, loss = 0.18465367\n",
      "Iteration 494, loss = 0.18444842\n",
      "Iteration 495, loss = 0.18361095\n",
      "Iteration 496, loss = 0.18415303\n",
      "Iteration 497, loss = 0.18371988\n",
      "Iteration 498, loss = 0.18400440\n",
      "Iteration 499, loss = 0.18364084\n",
      "Iteration 500, loss = 0.18407519\n",
      "Iteration 501, loss = 0.18416376\n",
      "Iteration 502, loss = 0.18392514\n",
      "Iteration 503, loss = 0.18331791\n",
      "Iteration 504, loss = 0.18314918\n",
      "Iteration 505, loss = 0.18290518\n",
      "Iteration 506, loss = 0.18296753\n",
      "Iteration 507, loss = 0.18313489\n",
      "Iteration 508, loss = 0.18263107\n",
      "Iteration 509, loss = 0.18284014\n",
      "Iteration 510, loss = 0.18267506\n",
      "Iteration 511, loss = 0.18253100\n",
      "Iteration 512, loss = 0.18247784\n",
      "Iteration 513, loss = 0.18296793\n",
      "Iteration 514, loss = 0.18254002\n",
      "Iteration 515, loss = 0.18235291\n",
      "Iteration 516, loss = 0.18247409\n",
      "Iteration 517, loss = 0.18253075\n",
      "Iteration 518, loss = 0.18295144\n",
      "Iteration 519, loss = 0.18206648\n",
      "Iteration 520, loss = 0.18240674\n",
      "Iteration 521, loss = 0.18207686\n",
      "Iteration 522, loss = 0.18196697\n",
      "Iteration 523, loss = 0.18222100\n",
      "Iteration 524, loss = 0.18177869\n",
      "Iteration 525, loss = 0.18199190\n",
      "Iteration 526, loss = 0.18149731\n",
      "Iteration 527, loss = 0.18162406\n",
      "Iteration 528, loss = 0.18178972\n",
      "Iteration 529, loss = 0.18197363\n",
      "Iteration 530, loss = 0.18169444\n",
      "Iteration 531, loss = 0.18168225\n",
      "Iteration 532, loss = 0.18171256\n",
      "Iteration 533, loss = 0.18163880\n",
      "Iteration 534, loss = 0.18156132\n",
      "Iteration 535, loss = 0.18129786\n",
      "Iteration 536, loss = 0.18076355\n",
      "Iteration 537, loss = 0.18089220\n",
      "Iteration 538, loss = 0.18070277\n",
      "Iteration 539, loss = 0.18093489\n",
      "Iteration 540, loss = 0.18089423\n",
      "Iteration 541, loss = 0.18098526\n",
      "Iteration 542, loss = 0.18016230\n",
      "Iteration 543, loss = 0.18184241\n",
      "Iteration 544, loss = 0.18162342\n",
      "Iteration 545, loss = 0.18088571\n",
      "Iteration 546, loss = 0.18078151\n",
      "Iteration 547, loss = 0.18021655\n",
      "Iteration 548, loss = 0.18056977\n",
      "Iteration 549, loss = 0.18039648\n",
      "Iteration 550, loss = 0.18031007\n",
      "Iteration 551, loss = 0.17997160\n",
      "Iteration 552, loss = 0.17971115\n",
      "Iteration 553, loss = 0.17996638\n",
      "Iteration 554, loss = 0.18023065\n",
      "Iteration 555, loss = 0.17986361\n",
      "Iteration 556, loss = 0.18020417\n",
      "Iteration 557, loss = 0.18003274\n",
      "Iteration 558, loss = 0.17946508\n",
      "Iteration 559, loss = 0.18116389\n",
      "Iteration 560, loss = 0.17953230\n",
      "Iteration 561, loss = 0.17997933\n",
      "Iteration 562, loss = 0.17965422\n",
      "Iteration 563, loss = 0.17993488\n",
      "Iteration 564, loss = 0.18039883\n",
      "Iteration 565, loss = 0.17988644\n",
      "Iteration 566, loss = 0.17904311\n",
      "Iteration 567, loss = 0.17920305\n",
      "Iteration 568, loss = 0.17973383\n",
      "Iteration 569, loss = 0.17913806\n",
      "Iteration 570, loss = 0.17902495\n",
      "Iteration 571, loss = 0.17961218\n",
      "Iteration 572, loss = 0.17908521\n",
      "Iteration 573, loss = 0.17922729\n",
      "Iteration 574, loss = 0.17880236\n",
      "Iteration 575, loss = 0.17917658\n",
      "Iteration 576, loss = 0.17853576\n",
      "Iteration 577, loss = 0.17912829\n",
      "Iteration 578, loss = 0.17883796\n",
      "Iteration 579, loss = 0.17902909\n",
      "Iteration 580, loss = 0.17887798\n",
      "Iteration 581, loss = 0.17894860\n",
      "Iteration 582, loss = 0.17934822\n",
      "Iteration 583, loss = 0.17920994\n",
      "Iteration 584, loss = 0.17851215\n",
      "Iteration 585, loss = 0.17844365\n",
      "Iteration 586, loss = 0.17891708\n",
      "Iteration 587, loss = 0.17895397\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76976572\n",
      "Iteration 2, loss = 0.74347967\n",
      "Iteration 3, loss = 0.72190306\n",
      "Iteration 4, loss = 0.70455299\n",
      "Iteration 5, loss = 0.69041816\n",
      "Iteration 6, loss = 0.67763651\n",
      "Iteration 7, loss = 0.66427324\n",
      "Iteration 8, loss = 0.64925393\n",
      "Iteration 9, loss = 0.63142665\n",
      "Iteration 10, loss = 0.61206915\n",
      "Iteration 11, loss = 0.59246675\n",
      "Iteration 12, loss = 0.57408535\n",
      "Iteration 13, loss = 0.55784287\n",
      "Iteration 14, loss = 0.54307215\n",
      "Iteration 15, loss = 0.53086904\n",
      "Iteration 16, loss = 0.52042177\n",
      "Iteration 17, loss = 0.51063238\n",
      "Iteration 18, loss = 0.50215337\n",
      "Iteration 19, loss = 0.49465696\n",
      "Iteration 20, loss = 0.48772078\n",
      "Iteration 21, loss = 0.48122622\n",
      "Iteration 22, loss = 0.47520443\n",
      "Iteration 23, loss = 0.46946541\n",
      "Iteration 24, loss = 0.46397855\n",
      "Iteration 25, loss = 0.45882553\n",
      "Iteration 26, loss = 0.45403135\n",
      "Iteration 27, loss = 0.44950745\n",
      "Iteration 28, loss = 0.44532352\n",
      "Iteration 29, loss = 0.44143354\n",
      "Iteration 30, loss = 0.43727274\n",
      "Iteration 31, loss = 0.43352467\n",
      "Iteration 32, loss = 0.42975395\n",
      "Iteration 33, loss = 0.42609400\n",
      "Iteration 34, loss = 0.42267085\n",
      "Iteration 35, loss = 0.41913151\n",
      "Iteration 36, loss = 0.41602332\n",
      "Iteration 37, loss = 0.41256166\n",
      "Iteration 38, loss = 0.40930288\n",
      "Iteration 39, loss = 0.40622050\n",
      "Iteration 40, loss = 0.40315122\n",
      "Iteration 41, loss = 0.40015930\n",
      "Iteration 42, loss = 0.39720413\n",
      "Iteration 43, loss = 0.39403193\n",
      "Iteration 44, loss = 0.39119905\n",
      "Iteration 45, loss = 0.38841750\n",
      "Iteration 46, loss = 0.38568721\n",
      "Iteration 47, loss = 0.38317479\n",
      "Iteration 48, loss = 0.38054350\n",
      "Iteration 49, loss = 0.37799650\n",
      "Iteration 50, loss = 0.37563305\n",
      "Iteration 51, loss = 0.37315101\n",
      "Iteration 52, loss = 0.37097072\n",
      "Iteration 53, loss = 0.36831759\n",
      "Iteration 54, loss = 0.36630738\n",
      "Iteration 55, loss = 0.36406459\n",
      "Iteration 56, loss = 0.36177310\n",
      "Iteration 57, loss = 0.35977743\n",
      "Iteration 58, loss = 0.35766421\n",
      "Iteration 59, loss = 0.35563642\n",
      "Iteration 60, loss = 0.35349043\n",
      "Iteration 61, loss = 0.35163134\n",
      "Iteration 62, loss = 0.34971930\n",
      "Iteration 63, loss = 0.34789645\n",
      "Iteration 64, loss = 0.34591030\n",
      "Iteration 65, loss = 0.34409077\n",
      "Iteration 66, loss = 0.34240143\n",
      "Iteration 67, loss = 0.34040652\n",
      "Iteration 68, loss = 0.33890242\n",
      "Iteration 69, loss = 0.33682603\n",
      "Iteration 70, loss = 0.33537371\n",
      "Iteration 71, loss = 0.33386076\n",
      "Iteration 72, loss = 0.33180181\n",
      "Iteration 73, loss = 0.33007750\n",
      "Iteration 74, loss = 0.32870227\n",
      "Iteration 75, loss = 0.32689338\n",
      "Iteration 76, loss = 0.32528623\n",
      "Iteration 77, loss = 0.32374259\n",
      "Iteration 78, loss = 0.32221217\n",
      "Iteration 79, loss = 0.32073122\n",
      "Iteration 80, loss = 0.31911410\n",
      "Iteration 81, loss = 0.31768191\n",
      "Iteration 82, loss = 0.31621112\n",
      "Iteration 83, loss = 0.31498386\n",
      "Iteration 84, loss = 0.31326255\n",
      "Iteration 85, loss = 0.31192341\n",
      "Iteration 86, loss = 0.31080614\n",
      "Iteration 87, loss = 0.30951186\n",
      "Iteration 88, loss = 0.30797525\n",
      "Iteration 89, loss = 0.30672419\n",
      "Iteration 90, loss = 0.30554129\n",
      "Iteration 91, loss = 0.30441578\n",
      "Iteration 92, loss = 0.30342057\n",
      "Iteration 93, loss = 0.30193016\n",
      "Iteration 94, loss = 0.30092513\n",
      "Iteration 95, loss = 0.29967447\n",
      "Iteration 96, loss = 0.29813857\n",
      "Iteration 97, loss = 0.29713599\n",
      "Iteration 98, loss = 0.29580812\n",
      "Iteration 99, loss = 0.29466205\n",
      "Iteration 100, loss = 0.29388192\n",
      "Iteration 101, loss = 0.29255304\n",
      "Iteration 102, loss = 0.29154206\n",
      "Iteration 103, loss = 0.29060961\n",
      "Iteration 104, loss = 0.28947691\n",
      "Iteration 105, loss = 0.28850529\n",
      "Iteration 106, loss = 0.28744759\n",
      "Iteration 107, loss = 0.28658081\n",
      "Iteration 108, loss = 0.28531745\n",
      "Iteration 109, loss = 0.28472961\n",
      "Iteration 110, loss = 0.28398641\n",
      "Iteration 111, loss = 0.28237311\n",
      "Iteration 112, loss = 0.28179930\n",
      "Iteration 113, loss = 0.28136821\n",
      "Iteration 114, loss = 0.28023534\n",
      "Iteration 115, loss = 0.27899228\n",
      "Iteration 116, loss = 0.27780433\n",
      "Iteration 117, loss = 0.27722869\n",
      "Iteration 118, loss = 0.27622330\n",
      "Iteration 119, loss = 0.27640621\n",
      "Iteration 120, loss = 0.27401108\n",
      "Iteration 121, loss = 0.27410135\n",
      "Iteration 122, loss = 0.27299138\n",
      "Iteration 123, loss = 0.27182088\n",
      "Iteration 124, loss = 0.27117327\n",
      "Iteration 125, loss = 0.27043051\n",
      "Iteration 126, loss = 0.26969726\n",
      "Iteration 127, loss = 0.26885413\n",
      "Iteration 128, loss = 0.26776035\n",
      "Iteration 129, loss = 0.26704864\n",
      "Iteration 130, loss = 0.26611699\n",
      "Iteration 131, loss = 0.26547703\n",
      "Iteration 132, loss = 0.26424826\n",
      "Iteration 133, loss = 0.26351996\n",
      "Iteration 134, loss = 0.26271284\n",
      "Iteration 135, loss = 0.26210229\n",
      "Iteration 136, loss = 0.26137043\n",
      "Iteration 137, loss = 0.26041143\n",
      "Iteration 138, loss = 0.26040073\n",
      "Iteration 139, loss = 0.25907024\n",
      "Iteration 140, loss = 0.25843251\n",
      "Iteration 141, loss = 0.25766334\n",
      "Iteration 142, loss = 0.25716835\n",
      "Iteration 143, loss = 0.25614264\n",
      "Iteration 144, loss = 0.25531439\n",
      "Iteration 145, loss = 0.25488190\n",
      "Iteration 146, loss = 0.25408831\n",
      "Iteration 147, loss = 0.25356041\n",
      "Iteration 148, loss = 0.25277975\n",
      "Iteration 149, loss = 0.25186187\n",
      "Iteration 150, loss = 0.25120114\n",
      "Iteration 151, loss = 0.25113366\n",
      "Iteration 152, loss = 0.24981385\n",
      "Iteration 153, loss = 0.24910184\n",
      "Iteration 154, loss = 0.24855295\n",
      "Iteration 155, loss = 0.24804888\n",
      "Iteration 156, loss = 0.24728169\n",
      "Iteration 157, loss = 0.24638680\n",
      "Iteration 158, loss = 0.24607280\n",
      "Iteration 159, loss = 0.24543543\n",
      "Iteration 160, loss = 0.24462717\n",
      "Iteration 161, loss = 0.24388363\n",
      "Iteration 162, loss = 0.24358575\n",
      "Iteration 163, loss = 0.24269374\n",
      "Iteration 164, loss = 0.24217080\n",
      "Iteration 165, loss = 0.24169948\n",
      "Iteration 166, loss = 0.24110953\n",
      "Iteration 167, loss = 0.24064953\n",
      "Iteration 168, loss = 0.23994797\n",
      "Iteration 169, loss = 0.23928001\n",
      "Iteration 170, loss = 0.23890031\n",
      "Iteration 171, loss = 0.23900687\n",
      "Iteration 172, loss = 0.23811483\n",
      "Iteration 173, loss = 0.23817999\n",
      "Iteration 174, loss = 0.23720025\n",
      "Iteration 175, loss = 0.23608501\n",
      "Iteration 176, loss = 0.23545378\n",
      "Iteration 177, loss = 0.23444915\n",
      "Iteration 178, loss = 0.23454779\n",
      "Iteration 179, loss = 0.23331349\n",
      "Iteration 180, loss = 0.23298064\n",
      "Iteration 181, loss = 0.23183457\n",
      "Iteration 182, loss = 0.23157319\n",
      "Iteration 183, loss = 0.23086608\n",
      "Iteration 184, loss = 0.23056695\n",
      "Iteration 185, loss = 0.22992030\n",
      "Iteration 186, loss = 0.22965397\n",
      "Iteration 187, loss = 0.22826810\n",
      "Iteration 188, loss = 0.22847495\n",
      "Iteration 189, loss = 0.22736408\n",
      "Iteration 190, loss = 0.22733587\n",
      "Iteration 191, loss = 0.22635239\n",
      "Iteration 192, loss = 0.22599360\n",
      "Iteration 193, loss = 0.22544230\n",
      "Iteration 194, loss = 0.22495578\n",
      "Iteration 195, loss = 0.22440231\n",
      "Iteration 196, loss = 0.22371810\n",
      "Iteration 197, loss = 0.22384716\n",
      "Iteration 198, loss = 0.22332681\n",
      "Iteration 199, loss = 0.22233779\n",
      "Iteration 200, loss = 0.22184802\n",
      "Iteration 201, loss = 0.22150915\n",
      "Iteration 202, loss = 0.22058558\n",
      "Iteration 203, loss = 0.22012900\n",
      "Iteration 204, loss = 0.21976388\n",
      "Iteration 205, loss = 0.21939466\n",
      "Iteration 206, loss = 0.21843534\n",
      "Iteration 207, loss = 0.21900981\n",
      "Iteration 208, loss = 0.21748015\n",
      "Iteration 209, loss = 0.21737001\n",
      "Iteration 210, loss = 0.21678039\n",
      "Iteration 211, loss = 0.21605560\n",
      "Iteration 212, loss = 0.21563089\n",
      "Iteration 213, loss = 0.21565258\n",
      "Iteration 214, loss = 0.21463538\n",
      "Iteration 215, loss = 0.21574639\n",
      "Iteration 216, loss = 0.21375304\n",
      "Iteration 217, loss = 0.21344645\n",
      "Iteration 218, loss = 0.21368232\n",
      "Iteration 219, loss = 0.21254810\n",
      "Iteration 220, loss = 0.21208813\n",
      "Iteration 221, loss = 0.21155618\n",
      "Iteration 222, loss = 0.21150888\n",
      "Iteration 223, loss = 0.21090264\n",
      "Iteration 224, loss = 0.21030257\n",
      "Iteration 225, loss = 0.21012333\n",
      "Iteration 226, loss = 0.20942440\n",
      "Iteration 227, loss = 0.20873989\n",
      "Iteration 228, loss = 0.20972357\n",
      "Iteration 229, loss = 0.20923782\n",
      "Iteration 230, loss = 0.20949923\n",
      "Iteration 231, loss = 0.20780043\n",
      "Iteration 232, loss = 0.20714003\n",
      "Iteration 233, loss = 0.20736234\n",
      "Iteration 234, loss = 0.20607236\n",
      "Iteration 235, loss = 0.20657794\n",
      "Iteration 236, loss = 0.20567747\n",
      "Iteration 237, loss = 0.20635700\n",
      "Iteration 238, loss = 0.20492471\n",
      "Iteration 239, loss = 0.20578575\n",
      "Iteration 240, loss = 0.20485624\n",
      "Iteration 241, loss = 0.20390325\n",
      "Iteration 242, loss = 0.20357211\n",
      "Iteration 243, loss = 0.20375070\n",
      "Iteration 244, loss = 0.20379488\n",
      "Iteration 245, loss = 0.20280079\n",
      "Iteration 246, loss = 0.20225446\n",
      "Iteration 247, loss = 0.20178431\n",
      "Iteration 248, loss = 0.20196738\n",
      "Iteration 249, loss = 0.20184380\n",
      "Iteration 250, loss = 0.20149531\n",
      "Iteration 251, loss = 0.20100547\n",
      "Iteration 252, loss = 0.20073683\n",
      "Iteration 253, loss = 0.20025713\n",
      "Iteration 254, loss = 0.20000164\n",
      "Iteration 255, loss = 0.19988370\n",
      "Iteration 256, loss = 0.19938286\n",
      "Iteration 257, loss = 0.19975008\n",
      "Iteration 258, loss = 0.19899791\n",
      "Iteration 259, loss = 0.19880567\n",
      "Iteration 260, loss = 0.19807146\n",
      "Iteration 261, loss = 0.19799230\n",
      "Iteration 262, loss = 0.19762320\n",
      "Iteration 263, loss = 0.19812896\n",
      "Iteration 264, loss = 0.19789764\n",
      "Iteration 265, loss = 0.19691948\n",
      "Iteration 266, loss = 0.19697738\n",
      "Iteration 267, loss = 0.19686309\n",
      "Iteration 268, loss = 0.19640306\n",
      "Iteration 269, loss = 0.19608032\n",
      "Iteration 270, loss = 0.19580458\n",
      "Iteration 271, loss = 0.19580044\n",
      "Iteration 272, loss = 0.19515005\n",
      "Iteration 273, loss = 0.19480976\n",
      "Iteration 274, loss = 0.19509787\n",
      "Iteration 275, loss = 0.19463854\n",
      "Iteration 276, loss = 0.19439397\n",
      "Iteration 277, loss = 0.19597242\n",
      "Iteration 278, loss = 0.19463290\n",
      "Iteration 279, loss = 0.19483537\n",
      "Iteration 280, loss = 0.19296982\n",
      "Iteration 281, loss = 0.19348714\n",
      "Iteration 282, loss = 0.19303257\n",
      "Iteration 283, loss = 0.19271859\n",
      "Iteration 284, loss = 0.19272932\n",
      "Iteration 285, loss = 0.19260008\n",
      "Iteration 286, loss = 0.19245350\n",
      "Iteration 287, loss = 0.19177743\n",
      "Iteration 288, loss = 0.19242583\n",
      "Iteration 289, loss = 0.19166614\n",
      "Iteration 290, loss = 0.19241516\n",
      "Iteration 291, loss = 0.19092789\n",
      "Iteration 292, loss = 0.19092356\n",
      "Iteration 293, loss = 0.19067061\n",
      "Iteration 294, loss = 0.19036766\n",
      "Iteration 295, loss = 0.19099251\n",
      "Iteration 296, loss = 0.18944679\n",
      "Iteration 297, loss = 0.18946346\n",
      "Iteration 298, loss = 0.18921204\n",
      "Iteration 299, loss = 0.18942106\n",
      "Iteration 300, loss = 0.18863130\n",
      "Iteration 301, loss = 0.18884626\n",
      "Iteration 302, loss = 0.18824043\n",
      "Iteration 303, loss = 0.18823741\n",
      "Iteration 304, loss = 0.18878914\n",
      "Iteration 305, loss = 0.18830619\n",
      "Iteration 306, loss = 0.18751229\n",
      "Iteration 307, loss = 0.18766969\n",
      "Iteration 308, loss = 0.18782261\n",
      "Iteration 309, loss = 0.18722849\n",
      "Iteration 310, loss = 0.18720685\n",
      "Iteration 311, loss = 0.18654408\n",
      "Iteration 312, loss = 0.18624650\n",
      "Iteration 313, loss = 0.18639410\n",
      "Iteration 314, loss = 0.18628297\n",
      "Iteration 315, loss = 0.18618711\n",
      "Iteration 316, loss = 0.18580659\n",
      "Iteration 317, loss = 0.18693907\n",
      "Iteration 318, loss = 0.18799043\n",
      "Iteration 319, loss = 0.18793485\n",
      "Iteration 320, loss = 0.18582819\n",
      "Iteration 321, loss = 0.18508865\n",
      "Iteration 322, loss = 0.18465818\n",
      "Iteration 323, loss = 0.18465513\n",
      "Iteration 324, loss = 0.18429216\n",
      "Iteration 325, loss = 0.18414658\n",
      "Iteration 326, loss = 0.18385415\n",
      "Iteration 327, loss = 0.18461127\n",
      "Iteration 328, loss = 0.18391390\n",
      "Iteration 329, loss = 0.18381765\n",
      "Iteration 330, loss = 0.18354737\n",
      "Iteration 331, loss = 0.18343610\n",
      "Iteration 332, loss = 0.18352732\n",
      "Iteration 333, loss = 0.18333820\n",
      "Iteration 334, loss = 0.18298861\n",
      "Iteration 335, loss = 0.18269653\n",
      "Iteration 336, loss = 0.18279770\n",
      "Iteration 337, loss = 0.18259263\n",
      "Iteration 338, loss = 0.18298290\n",
      "Iteration 339, loss = 0.18210824\n",
      "Iteration 340, loss = 0.18236994\n",
      "Iteration 341, loss = 0.18176053\n",
      "Iteration 342, loss = 0.18251394\n",
      "Iteration 343, loss = 0.18142382\n",
      "Iteration 344, loss = 0.18197777\n",
      "Iteration 345, loss = 0.18125865\n",
      "Iteration 346, loss = 0.18133604\n",
      "Iteration 347, loss = 0.18110100\n",
      "Iteration 348, loss = 0.18152291\n",
      "Iteration 349, loss = 0.18186708\n",
      "Iteration 350, loss = 0.18144024\n",
      "Iteration 351, loss = 0.18044874\n",
      "Iteration 352, loss = 0.18106055\n",
      "Iteration 353, loss = 0.17997474\n",
      "Iteration 354, loss = 0.18018188\n",
      "Iteration 355, loss = 0.17993738\n",
      "Iteration 356, loss = 0.17996519\n",
      "Iteration 357, loss = 0.17974179\n",
      "Iteration 358, loss = 0.17983088\n",
      "Iteration 359, loss = 0.17998657\n",
      "Iteration 360, loss = 0.17953164\n",
      "Iteration 361, loss = 0.17932423\n",
      "Iteration 362, loss = 0.17938464\n",
      "Iteration 363, loss = 0.17987090\n",
      "Iteration 364, loss = 0.17939771\n",
      "Iteration 365, loss = 0.17953922\n",
      "Iteration 366, loss = 0.17930263\n",
      "Iteration 367, loss = 0.17867918\n",
      "Iteration 368, loss = 0.17892263\n",
      "Iteration 369, loss = 0.17848382\n",
      "Iteration 370, loss = 0.17853760\n",
      "Iteration 371, loss = 0.17829028\n",
      "Iteration 372, loss = 0.17815215\n",
      "Iteration 373, loss = 0.17811022\n",
      "Iteration 374, loss = 0.17900028\n",
      "Iteration 375, loss = 0.17855451\n",
      "Iteration 376, loss = 0.17776604\n",
      "Iteration 377, loss = 0.17738027\n",
      "Iteration 378, loss = 0.17748168\n",
      "Iteration 379, loss = 0.17712742\n",
      "Iteration 380, loss = 0.17783142\n",
      "Iteration 381, loss = 0.17718625\n",
      "Iteration 382, loss = 0.17728119\n",
      "Iteration 383, loss = 0.17715152\n",
      "Iteration 384, loss = 0.17722060\n",
      "Iteration 385, loss = 0.17774226\n",
      "Iteration 386, loss = 0.17659404\n",
      "Iteration 387, loss = 0.17731717\n",
      "Iteration 388, loss = 0.17635548\n",
      "Iteration 389, loss = 0.17619645\n",
      "Iteration 390, loss = 0.17657021\n",
      "Iteration 391, loss = 0.17650295\n",
      "Iteration 392, loss = 0.17597993\n",
      "Iteration 393, loss = 0.17594606\n",
      "Iteration 394, loss = 0.17626713\n",
      "Iteration 395, loss = 0.17686104\n",
      "Iteration 396, loss = 0.17569419\n",
      "Iteration 397, loss = 0.17598982\n",
      "Iteration 398, loss = 0.17576434\n",
      "Iteration 399, loss = 0.17570757\n",
      "Iteration 400, loss = 0.17548333\n",
      "Iteration 401, loss = 0.17531970\n",
      "Iteration 402, loss = 0.17519918\n",
      "Iteration 403, loss = 0.17583441\n",
      "Iteration 404, loss = 0.17519233\n",
      "Iteration 405, loss = 0.17522202\n",
      "Iteration 406, loss = 0.17471025\n",
      "Iteration 407, loss = 0.17466499\n",
      "Iteration 408, loss = 0.17448015\n",
      "Iteration 409, loss = 0.17476397\n",
      "Iteration 410, loss = 0.17500725\n",
      "Iteration 411, loss = 0.17429313\n",
      "Iteration 412, loss = 0.17414890\n",
      "Iteration 413, loss = 0.17425150\n",
      "Iteration 414, loss = 0.17409805\n",
      "Iteration 415, loss = 0.17374347\n",
      "Iteration 416, loss = 0.17431700\n",
      "Iteration 417, loss = 0.17407564\n",
      "Iteration 418, loss = 0.17389847\n",
      "Iteration 419, loss = 0.17361008\n",
      "Iteration 420, loss = 0.17371132\n",
      "Iteration 421, loss = 0.17364173\n",
      "Iteration 422, loss = 0.17345949\n",
      "Iteration 423, loss = 0.17361704\n",
      "Iteration 424, loss = 0.17373042\n",
      "Iteration 425, loss = 0.17341451\n",
      "Iteration 426, loss = 0.17357910\n",
      "Iteration 427, loss = 0.17357806\n",
      "Iteration 428, loss = 0.17268672\n",
      "Iteration 429, loss = 0.17415088\n",
      "Iteration 430, loss = 0.17516184\n",
      "Iteration 431, loss = 0.17265188\n",
      "Iteration 432, loss = 0.17334992\n",
      "Iteration 433, loss = 0.17270753\n",
      "Iteration 434, loss = 0.17295619\n",
      "Iteration 435, loss = 0.17252109\n",
      "Iteration 436, loss = 0.17238718\n",
      "Iteration 437, loss = 0.17256372\n",
      "Iteration 438, loss = 0.17231864\n",
      "Iteration 439, loss = 0.17226769\n",
      "Iteration 440, loss = 0.17220819\n",
      "Iteration 441, loss = 0.17249930\n",
      "Iteration 442, loss = 0.17198015\n",
      "Iteration 443, loss = 0.17223071\n",
      "Iteration 444, loss = 0.17253702\n",
      "Iteration 445, loss = 0.17224961\n",
      "Iteration 446, loss = 0.17151088\n",
      "Iteration 447, loss = 0.17178711\n",
      "Iteration 448, loss = 0.17194098\n",
      "Iteration 449, loss = 0.17148275\n",
      "Iteration 450, loss = 0.17194373\n",
      "Iteration 451, loss = 0.17138890\n",
      "Iteration 452, loss = 0.17143184\n",
      "Iteration 453, loss = 0.17129564\n",
      "Iteration 454, loss = 0.17128485\n",
      "Iteration 455, loss = 0.17168351\n",
      "Iteration 456, loss = 0.17146762\n",
      "Iteration 457, loss = 0.17168698\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77046005\n",
      "Iteration 2, loss = 0.74490933\n",
      "Iteration 3, loss = 0.72332516\n",
      "Iteration 4, loss = 0.70570475\n",
      "Iteration 5, loss = 0.69100670\n",
      "Iteration 6, loss = 0.67904761\n",
      "Iteration 7, loss = 0.66681904\n",
      "Iteration 8, loss = 0.65288145\n",
      "Iteration 9, loss = 0.63643379\n",
      "Iteration 10, loss = 0.61791721\n",
      "Iteration 11, loss = 0.59880461\n",
      "Iteration 12, loss = 0.57974585\n",
      "Iteration 13, loss = 0.56314733\n",
      "Iteration 14, loss = 0.54947301\n",
      "Iteration 15, loss = 0.53763567\n",
      "Iteration 16, loss = 0.52718706\n",
      "Iteration 17, loss = 0.51798249\n",
      "Iteration 18, loss = 0.50976702\n",
      "Iteration 19, loss = 0.50147373\n",
      "Iteration 20, loss = 0.49426624\n",
      "Iteration 21, loss = 0.48741857\n",
      "Iteration 22, loss = 0.48118332\n",
      "Iteration 23, loss = 0.47606135\n",
      "Iteration 24, loss = 0.47055377\n",
      "Iteration 25, loss = 0.46551175\n",
      "Iteration 26, loss = 0.46048807\n",
      "Iteration 27, loss = 0.45607485\n",
      "Iteration 28, loss = 0.45154226\n",
      "Iteration 29, loss = 0.44739851\n",
      "Iteration 30, loss = 0.44323247\n",
      "Iteration 31, loss = 0.43950563\n",
      "Iteration 32, loss = 0.43579052\n",
      "Iteration 33, loss = 0.43217675\n",
      "Iteration 34, loss = 0.42862623\n",
      "Iteration 35, loss = 0.42508057\n",
      "Iteration 36, loss = 0.42171952\n",
      "Iteration 37, loss = 0.41848028\n",
      "Iteration 38, loss = 0.41510522\n",
      "Iteration 39, loss = 0.41207961\n",
      "Iteration 40, loss = 0.40895255\n",
      "Iteration 41, loss = 0.40591672\n",
      "Iteration 42, loss = 0.40328641\n",
      "Iteration 43, loss = 0.39997648\n",
      "Iteration 44, loss = 0.39744266\n",
      "Iteration 45, loss = 0.39484543\n",
      "Iteration 46, loss = 0.39178124\n",
      "Iteration 47, loss = 0.38944032\n",
      "Iteration 48, loss = 0.38726879\n",
      "Iteration 49, loss = 0.38442234\n",
      "Iteration 50, loss = 0.38212727\n",
      "Iteration 51, loss = 0.37967292\n",
      "Iteration 52, loss = 0.37733963\n",
      "Iteration 53, loss = 0.37538973\n",
      "Iteration 54, loss = 0.37303087\n",
      "Iteration 55, loss = 0.37107705\n",
      "Iteration 56, loss = 0.36882932\n",
      "Iteration 57, loss = 0.36659742\n",
      "Iteration 58, loss = 0.36448979\n",
      "Iteration 59, loss = 0.36235608\n",
      "Iteration 60, loss = 0.36048168\n",
      "Iteration 61, loss = 0.35846890\n",
      "Iteration 62, loss = 0.35659067\n",
      "Iteration 63, loss = 0.35450965\n",
      "Iteration 64, loss = 0.35275511\n",
      "Iteration 65, loss = 0.35068007\n",
      "Iteration 66, loss = 0.34893578\n",
      "Iteration 67, loss = 0.34721867\n",
      "Iteration 68, loss = 0.34554133\n",
      "Iteration 69, loss = 0.34370366\n",
      "Iteration 70, loss = 0.34202482\n",
      "Iteration 71, loss = 0.34031325\n",
      "Iteration 72, loss = 0.33857204\n",
      "Iteration 73, loss = 0.33678136\n",
      "Iteration 74, loss = 0.33502779\n",
      "Iteration 75, loss = 0.33337845\n",
      "Iteration 76, loss = 0.33171739\n",
      "Iteration 77, loss = 0.33001825\n",
      "Iteration 78, loss = 0.32882940\n",
      "Iteration 79, loss = 0.32698466\n",
      "Iteration 80, loss = 0.32582781\n",
      "Iteration 81, loss = 0.32396194\n",
      "Iteration 82, loss = 0.32234335\n",
      "Iteration 83, loss = 0.32127011\n",
      "Iteration 84, loss = 0.31984868\n",
      "Iteration 85, loss = 0.31781584\n",
      "Iteration 86, loss = 0.31667986\n",
      "Iteration 87, loss = 0.31495472\n",
      "Iteration 88, loss = 0.31361268\n",
      "Iteration 89, loss = 0.31229860\n",
      "Iteration 90, loss = 0.31099608\n",
      "Iteration 91, loss = 0.30959472\n",
      "Iteration 92, loss = 0.30823076\n",
      "Iteration 93, loss = 0.30696028\n",
      "Iteration 94, loss = 0.30575376\n",
      "Iteration 95, loss = 0.30432062\n",
      "Iteration 96, loss = 0.30332323\n",
      "Iteration 97, loss = 0.30216642\n",
      "Iteration 98, loss = 0.30064243\n",
      "Iteration 99, loss = 0.29959071\n",
      "Iteration 100, loss = 0.29840356\n",
      "Iteration 101, loss = 0.29719782\n",
      "Iteration 102, loss = 0.29595502\n",
      "Iteration 103, loss = 0.29497370\n",
      "Iteration 104, loss = 0.29404376\n",
      "Iteration 105, loss = 0.29257241\n",
      "Iteration 106, loss = 0.29196113\n",
      "Iteration 107, loss = 0.29036601\n",
      "Iteration 108, loss = 0.28934621\n",
      "Iteration 109, loss = 0.28844091\n",
      "Iteration 110, loss = 0.28730828\n",
      "Iteration 111, loss = 0.28637333\n",
      "Iteration 112, loss = 0.28522855\n",
      "Iteration 113, loss = 0.28439426\n",
      "Iteration 114, loss = 0.28304137\n",
      "Iteration 115, loss = 0.28217143\n",
      "Iteration 116, loss = 0.28081927\n",
      "Iteration 117, loss = 0.28040355\n",
      "Iteration 118, loss = 0.27899520\n",
      "Iteration 119, loss = 0.27788667\n",
      "Iteration 120, loss = 0.27674423\n",
      "Iteration 121, loss = 0.27581830\n",
      "Iteration 122, loss = 0.27479195\n",
      "Iteration 123, loss = 0.27414190\n",
      "Iteration 124, loss = 0.27289419\n",
      "Iteration 125, loss = 0.27222498\n",
      "Iteration 126, loss = 0.27128146\n",
      "Iteration 127, loss = 0.27027944\n",
      "Iteration 128, loss = 0.26934738\n",
      "Iteration 129, loss = 0.26859573\n",
      "Iteration 130, loss = 0.26773713\n",
      "Iteration 131, loss = 0.26695346\n",
      "Iteration 132, loss = 0.26643881\n",
      "Iteration 133, loss = 0.26571980\n",
      "Iteration 134, loss = 0.26486746\n",
      "Iteration 135, loss = 0.26397543\n",
      "Iteration 136, loss = 0.26333865\n",
      "Iteration 137, loss = 0.26240096\n",
      "Iteration 138, loss = 0.26189031\n",
      "Iteration 139, loss = 0.26113776\n",
      "Iteration 140, loss = 0.26030097\n",
      "Iteration 141, loss = 0.25968322\n",
      "Iteration 142, loss = 0.25885925\n",
      "Iteration 143, loss = 0.25851272\n",
      "Iteration 144, loss = 0.25768111\n",
      "Iteration 145, loss = 0.25688654\n",
      "Iteration 146, loss = 0.25630330\n",
      "Iteration 147, loss = 0.25618576\n",
      "Iteration 148, loss = 0.25510591\n",
      "Iteration 149, loss = 0.25438384\n",
      "Iteration 150, loss = 0.25386012\n",
      "Iteration 151, loss = 0.25319441\n",
      "Iteration 152, loss = 0.25293080\n",
      "Iteration 153, loss = 0.25212714\n",
      "Iteration 154, loss = 0.25163765\n",
      "Iteration 155, loss = 0.25089832\n",
      "Iteration 156, loss = 0.25015864\n",
      "Iteration 157, loss = 0.24961964\n",
      "Iteration 158, loss = 0.24893283\n",
      "Iteration 159, loss = 0.24846883\n",
      "Iteration 160, loss = 0.24815090\n",
      "Iteration 161, loss = 0.24721829\n",
      "Iteration 162, loss = 0.24709610\n",
      "Iteration 163, loss = 0.24631766\n",
      "Iteration 164, loss = 0.24593971\n",
      "Iteration 165, loss = 0.24540677\n",
      "Iteration 166, loss = 0.24456968\n",
      "Iteration 167, loss = 0.24474624\n",
      "Iteration 168, loss = 0.24368220\n",
      "Iteration 169, loss = 0.24297079\n",
      "Iteration 170, loss = 0.24294027\n",
      "Iteration 171, loss = 0.24199878\n",
      "Iteration 172, loss = 0.24202561\n",
      "Iteration 173, loss = 0.24208774\n",
      "Iteration 174, loss = 0.24062763\n",
      "Iteration 175, loss = 0.24048431\n",
      "Iteration 176, loss = 0.23963334\n",
      "Iteration 177, loss = 0.23934861\n",
      "Iteration 178, loss = 0.23874504\n",
      "Iteration 179, loss = 0.23809500\n",
      "Iteration 180, loss = 0.23759931\n",
      "Iteration 181, loss = 0.23729430\n",
      "Iteration 182, loss = 0.23668734\n",
      "Iteration 183, loss = 0.23624825\n",
      "Iteration 184, loss = 0.23576331\n",
      "Iteration 185, loss = 0.23502429\n",
      "Iteration 186, loss = 0.23478531\n",
      "Iteration 187, loss = 0.23428410\n",
      "Iteration 188, loss = 0.23376977\n",
      "Iteration 189, loss = 0.23317415\n",
      "Iteration 190, loss = 0.23288031\n",
      "Iteration 191, loss = 0.23250704\n",
      "Iteration 192, loss = 0.23250493\n",
      "Iteration 193, loss = 0.23155622\n",
      "Iteration 194, loss = 0.23123145\n",
      "Iteration 195, loss = 0.23080667\n",
      "Iteration 196, loss = 0.23059305\n",
      "Iteration 197, loss = 0.23004797\n",
      "Iteration 198, loss = 0.22967204\n",
      "Iteration 199, loss = 0.22937692\n",
      "Iteration 200, loss = 0.22909411\n",
      "Iteration 201, loss = 0.22856259\n",
      "Iteration 202, loss = 0.22805851\n",
      "Iteration 203, loss = 0.22789790\n",
      "Iteration 204, loss = 0.22766845\n",
      "Iteration 205, loss = 0.22696331\n",
      "Iteration 206, loss = 0.22639603\n",
      "Iteration 207, loss = 0.22618372\n",
      "Iteration 208, loss = 0.22595786\n",
      "Iteration 209, loss = 0.22555205\n",
      "Iteration 210, loss = 0.22545060\n",
      "Iteration 211, loss = 0.22546600\n",
      "Iteration 212, loss = 0.22461205\n",
      "Iteration 213, loss = 0.22412524\n",
      "Iteration 214, loss = 0.22421882\n",
      "Iteration 215, loss = 0.22401460\n",
      "Iteration 216, loss = 0.22323124\n",
      "Iteration 217, loss = 0.22283994\n",
      "Iteration 218, loss = 0.22283130\n",
      "Iteration 219, loss = 0.22333350\n",
      "Iteration 220, loss = 0.22230793\n",
      "Iteration 221, loss = 0.22187425\n",
      "Iteration 222, loss = 0.22132259\n",
      "Iteration 223, loss = 0.22136155\n",
      "Iteration 224, loss = 0.22115542\n",
      "Iteration 225, loss = 0.22042947\n",
      "Iteration 226, loss = 0.22039447\n",
      "Iteration 227, loss = 0.22003351\n",
      "Iteration 228, loss = 0.21996286\n",
      "Iteration 229, loss = 0.21989790\n",
      "Iteration 230, loss = 0.21931503\n",
      "Iteration 231, loss = 0.21897435\n",
      "Iteration 232, loss = 0.21912451\n",
      "Iteration 233, loss = 0.21831582\n",
      "Iteration 234, loss = 0.21855106\n",
      "Iteration 235, loss = 0.21825334\n",
      "Iteration 236, loss = 0.21823217\n",
      "Iteration 237, loss = 0.21751109\n",
      "Iteration 238, loss = 0.21743577\n",
      "Iteration 239, loss = 0.21720424\n",
      "Iteration 240, loss = 0.21677099\n",
      "Iteration 241, loss = 0.21628219\n",
      "Iteration 242, loss = 0.21618560\n",
      "Iteration 243, loss = 0.21639311\n",
      "Iteration 244, loss = 0.21572479\n",
      "Iteration 245, loss = 0.21554570\n",
      "Iteration 246, loss = 0.21525281\n",
      "Iteration 247, loss = 0.21490727\n",
      "Iteration 248, loss = 0.21515462\n",
      "Iteration 249, loss = 0.21507944\n",
      "Iteration 250, loss = 0.21567132\n",
      "Iteration 251, loss = 0.21420615\n",
      "Iteration 252, loss = 0.21401185\n",
      "Iteration 253, loss = 0.21414074\n",
      "Iteration 254, loss = 0.21365785\n",
      "Iteration 255, loss = 0.21315217\n",
      "Iteration 256, loss = 0.21306263\n",
      "Iteration 257, loss = 0.21285126\n",
      "Iteration 258, loss = 0.21404582\n",
      "Iteration 259, loss = 0.21275422\n",
      "Iteration 260, loss = 0.21229717\n",
      "Iteration 261, loss = 0.21213370\n",
      "Iteration 262, loss = 0.21226358\n",
      "Iteration 263, loss = 0.21149100\n",
      "Iteration 264, loss = 0.21114649\n",
      "Iteration 265, loss = 0.21124215\n",
      "Iteration 266, loss = 0.21131857\n",
      "Iteration 267, loss = 0.21064505\n",
      "Iteration 268, loss = 0.21085766\n",
      "Iteration 269, loss = 0.21053987\n",
      "Iteration 270, loss = 0.21031234\n",
      "Iteration 271, loss = 0.21035267\n",
      "Iteration 272, loss = 0.21062461\n",
      "Iteration 273, loss = 0.20945250\n",
      "Iteration 274, loss = 0.20957937\n",
      "Iteration 275, loss = 0.20948081\n",
      "Iteration 276, loss = 0.20937228\n",
      "Iteration 277, loss = 0.20954520\n",
      "Iteration 278, loss = 0.20954154\n",
      "Iteration 279, loss = 0.20848814\n",
      "Iteration 280, loss = 0.20868093\n",
      "Iteration 281, loss = 0.20812435\n",
      "Iteration 282, loss = 0.20804990\n",
      "Iteration 283, loss = 0.20800266\n",
      "Iteration 284, loss = 0.20759918\n",
      "Iteration 285, loss = 0.20749664\n",
      "Iteration 286, loss = 0.20733339\n",
      "Iteration 287, loss = 0.20783348\n",
      "Iteration 288, loss = 0.20715388\n",
      "Iteration 289, loss = 0.20680825\n",
      "Iteration 290, loss = 0.20681080\n",
      "Iteration 291, loss = 0.20688086\n",
      "Iteration 292, loss = 0.20664130\n",
      "Iteration 293, loss = 0.20624911\n",
      "Iteration 294, loss = 0.20608199\n",
      "Iteration 295, loss = 0.20598753\n",
      "Iteration 296, loss = 0.20576805\n",
      "Iteration 297, loss = 0.20596455\n",
      "Iteration 298, loss = 0.20543703\n",
      "Iteration 299, loss = 0.20507865\n",
      "Iteration 300, loss = 0.20528509\n",
      "Iteration 301, loss = 0.20515533\n",
      "Iteration 302, loss = 0.20487703\n",
      "Iteration 303, loss = 0.20480951\n",
      "Iteration 304, loss = 0.20502477\n",
      "Iteration 305, loss = 0.20504370\n",
      "Iteration 306, loss = 0.20425877\n",
      "Iteration 307, loss = 0.20446745\n",
      "Iteration 308, loss = 0.20439961\n",
      "Iteration 309, loss = 0.20391409\n",
      "Iteration 310, loss = 0.20402187\n",
      "Iteration 311, loss = 0.20359868\n",
      "Iteration 312, loss = 0.20424379\n",
      "Iteration 313, loss = 0.20401294\n",
      "Iteration 314, loss = 0.20392036\n",
      "Iteration 315, loss = 0.20398514\n",
      "Iteration 316, loss = 0.20327482\n",
      "Iteration 317, loss = 0.20360273\n",
      "Iteration 318, loss = 0.20301298\n",
      "Iteration 319, loss = 0.20315983\n",
      "Iteration 320, loss = 0.20302724\n",
      "Iteration 321, loss = 0.20261628\n",
      "Iteration 322, loss = 0.20268263\n",
      "Iteration 323, loss = 0.20260258\n",
      "Iteration 324, loss = 0.20214692\n",
      "Iteration 325, loss = 0.20292469\n",
      "Iteration 326, loss = 0.20208910\n",
      "Iteration 327, loss = 0.20238212\n",
      "Iteration 328, loss = 0.20270741\n",
      "Iteration 329, loss = 0.20203639\n",
      "Iteration 330, loss = 0.20287595\n",
      "Iteration 331, loss = 0.20185178\n",
      "Iteration 332, loss = 0.20145380\n",
      "Iteration 333, loss = 0.20153793\n",
      "Iteration 334, loss = 0.20145605\n",
      "Iteration 335, loss = 0.20126223\n",
      "Iteration 336, loss = 0.20108540\n",
      "Iteration 337, loss = 0.20082124\n",
      "Iteration 338, loss = 0.20144228\n",
      "Iteration 339, loss = 0.20108505\n",
      "Iteration 340, loss = 0.20049219\n",
      "Iteration 341, loss = 0.20069898\n",
      "Iteration 342, loss = 0.20066564\n",
      "Iteration 343, loss = 0.20047745\n",
      "Iteration 344, loss = 0.20095316\n",
      "Iteration 345, loss = 0.20020526\n",
      "Iteration 346, loss = 0.20019495\n",
      "Iteration 347, loss = 0.19999210\n",
      "Iteration 348, loss = 0.20000834\n",
      "Iteration 349, loss = 0.19982408\n",
      "Iteration 350, loss = 0.20032417\n",
      "Iteration 351, loss = 0.19998755\n",
      "Iteration 352, loss = 0.19978720\n",
      "Iteration 353, loss = 0.19989467\n",
      "Iteration 354, loss = 0.19934172\n",
      "Iteration 355, loss = 0.19962669\n",
      "Iteration 356, loss = 0.19944026\n",
      "Iteration 357, loss = 0.19930951\n",
      "Iteration 358, loss = 0.19946803\n",
      "Iteration 359, loss = 0.19900431\n",
      "Iteration 360, loss = 0.19914984\n",
      "Iteration 361, loss = 0.19915697\n",
      "Iteration 362, loss = 0.19873259\n",
      "Iteration 363, loss = 0.19897249\n",
      "Iteration 364, loss = 0.19865536\n",
      "Iteration 365, loss = 0.19860712\n",
      "Iteration 366, loss = 0.19854994\n",
      "Iteration 367, loss = 0.19814224\n",
      "Iteration 368, loss = 0.19843241\n",
      "Iteration 369, loss = 0.19820638\n",
      "Iteration 370, loss = 0.19838559\n",
      "Iteration 371, loss = 0.19807678\n",
      "Iteration 372, loss = 0.19809248\n",
      "Iteration 373, loss = 0.19808851\n",
      "Iteration 374, loss = 0.19740675\n",
      "Iteration 375, loss = 0.19757919\n",
      "Iteration 376, loss = 0.19754279\n",
      "Iteration 377, loss = 0.19730727\n",
      "Iteration 378, loss = 0.19703759\n",
      "Iteration 379, loss = 0.19721203\n",
      "Iteration 380, loss = 0.19773386\n",
      "Iteration 381, loss = 0.19706508\n",
      "Iteration 382, loss = 0.19736912\n",
      "Iteration 383, loss = 0.19691414\n",
      "Iteration 384, loss = 0.19694412\n",
      "Iteration 385, loss = 0.19660168\n",
      "Iteration 386, loss = 0.19656120\n",
      "Iteration 387, loss = 0.19646485\n",
      "Iteration 388, loss = 0.19678369\n",
      "Iteration 389, loss = 0.19637814\n",
      "Iteration 390, loss = 0.19677428\n",
      "Iteration 391, loss = 0.19638215\n",
      "Iteration 392, loss = 0.19625854\n",
      "Iteration 393, loss = 0.19609468\n",
      "Iteration 394, loss = 0.19660987\n",
      "Iteration 395, loss = 0.19605376\n",
      "Iteration 396, loss = 0.19661747\n",
      "Iteration 397, loss = 0.19588816\n",
      "Iteration 398, loss = 0.19605920\n",
      "Iteration 399, loss = 0.19590515\n",
      "Iteration 400, loss = 0.19592583\n",
      "Iteration 401, loss = 0.19586420\n",
      "Iteration 402, loss = 0.19569410\n",
      "Iteration 403, loss = 0.19536890\n",
      "Iteration 404, loss = 0.19590944\n",
      "Iteration 405, loss = 0.19560421\n",
      "Iteration 406, loss = 0.19546872\n",
      "Iteration 407, loss = 0.19554663\n",
      "Iteration 408, loss = 0.19545067\n",
      "Iteration 409, loss = 0.19584529\n",
      "Iteration 410, loss = 0.19553899\n",
      "Iteration 411, loss = 0.19520989\n",
      "Iteration 412, loss = 0.19538346\n",
      "Iteration 413, loss = 0.19496001\n",
      "Iteration 414, loss = 0.19564489\n",
      "Iteration 415, loss = 0.19504859\n",
      "Iteration 416, loss = 0.19515771\n",
      "Iteration 417, loss = 0.19541621\n",
      "Iteration 418, loss = 0.19515013\n",
      "Iteration 419, loss = 0.19477885\n",
      "Iteration 420, loss = 0.19506090\n",
      "Iteration 421, loss = 0.19574906\n",
      "Iteration 422, loss = 0.19548210\n",
      "Iteration 423, loss = 0.19534722\n",
      "Iteration 424, loss = 0.19500411\n",
      "Iteration 425, loss = 0.19470647\n",
      "Iteration 426, loss = 0.19493788\n",
      "Iteration 427, loss = 0.19494598\n",
      "Iteration 428, loss = 0.19519074\n",
      "Iteration 429, loss = 0.19461778\n",
      "Iteration 430, loss = 0.19439020\n",
      "Iteration 431, loss = 0.19462191\n",
      "Iteration 432, loss = 0.19507164\n",
      "Iteration 433, loss = 0.19483307\n",
      "Iteration 434, loss = 0.19447841\n",
      "Iteration 435, loss = 0.19517395\n",
      "Iteration 436, loss = 0.19445885\n",
      "Iteration 437, loss = 0.19441120\n",
      "Iteration 438, loss = 0.19441312\n",
      "Iteration 439, loss = 0.19410500\n",
      "Iteration 440, loss = 0.19500663\n",
      "Iteration 441, loss = 0.19397838\n",
      "Iteration 442, loss = 0.19416110\n",
      "Iteration 443, loss = 0.19446441\n",
      "Iteration 444, loss = 0.19414061\n",
      "Iteration 445, loss = 0.19391606\n",
      "Iteration 446, loss = 0.19426860\n",
      "Iteration 447, loss = 0.19423382\n",
      "Iteration 448, loss = 0.19429622\n",
      "Iteration 449, loss = 0.19516873\n",
      "Iteration 450, loss = 0.19384755\n",
      "Iteration 451, loss = 0.19405956\n",
      "Iteration 452, loss = 0.19350324\n",
      "Iteration 453, loss = 0.19419148\n",
      "Iteration 454, loss = 0.19401151\n",
      "Iteration 455, loss = 0.19376121\n",
      "Iteration 456, loss = 0.19368067\n",
      "Iteration 457, loss = 0.19410505\n",
      "Iteration 458, loss = 0.19348402\n",
      "Iteration 459, loss = 0.19335093\n",
      "Iteration 460, loss = 0.19353867\n",
      "Iteration 461, loss = 0.19335016\n",
      "Iteration 462, loss = 0.19388642\n",
      "Iteration 463, loss = 0.19382602\n",
      "Iteration 464, loss = 0.19341410\n",
      "Iteration 465, loss = 0.19319390\n",
      "Iteration 466, loss = 0.19330637\n",
      "Iteration 467, loss = 0.19336752\n",
      "Iteration 468, loss = 0.19383217\n",
      "Iteration 469, loss = 0.19396095\n",
      "Iteration 470, loss = 0.19321136\n",
      "Iteration 471, loss = 0.19355638\n",
      "Iteration 472, loss = 0.19394090\n",
      "Iteration 473, loss = 0.19386054\n",
      "Iteration 474, loss = 0.19412209\n",
      "Iteration 475, loss = 0.19341476\n",
      "Iteration 476, loss = 0.19303457\n",
      "Iteration 477, loss = 0.19315750\n",
      "Iteration 478, loss = 0.19327899\n",
      "Iteration 479, loss = 0.19294161\n",
      "Iteration 480, loss = 0.19271455\n",
      "Iteration 481, loss = 0.19279953\n",
      "Iteration 482, loss = 0.19295406\n",
      "Iteration 483, loss = 0.19298205\n",
      "Iteration 484, loss = 0.19548326\n",
      "Iteration 485, loss = 0.19288971\n",
      "Iteration 486, loss = 0.19361017\n",
      "Iteration 487, loss = 0.19381674\n",
      "Iteration 488, loss = 0.19305855\n",
      "Iteration 489, loss = 0.19276464\n",
      "Iteration 490, loss = 0.19239719\n",
      "Iteration 491, loss = 0.19239809\n",
      "Iteration 492, loss = 0.19236821\n",
      "Iteration 493, loss = 0.19253581\n",
      "Iteration 494, loss = 0.19225734\n",
      "Iteration 495, loss = 0.19234926\n",
      "Iteration 496, loss = 0.19219409\n",
      "Iteration 497, loss = 0.19223257\n",
      "Iteration 498, loss = 0.19219448\n",
      "Iteration 499, loss = 0.19224925\n",
      "Iteration 500, loss = 0.19198234\n",
      "Iteration 501, loss = 0.19352321\n",
      "Iteration 502, loss = 0.19252964\n",
      "Iteration 503, loss = 0.19250001\n",
      "Iteration 504, loss = 0.19225691\n",
      "Iteration 505, loss = 0.19186259\n",
      "Iteration 506, loss = 0.19146106\n",
      "Iteration 507, loss = 0.19196121\n",
      "Iteration 508, loss = 0.19169541\n",
      "Iteration 509, loss = 0.19221142\n",
      "Iteration 510, loss = 0.19204400\n",
      "Iteration 511, loss = 0.19192164\n",
      "Iteration 512, loss = 0.19222578\n",
      "Iteration 513, loss = 0.19154599\n",
      "Iteration 514, loss = 0.19185015\n",
      "Iteration 515, loss = 0.19461785\n",
      "Iteration 516, loss = 0.19178009\n",
      "Iteration 517, loss = 0.19180601\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77042791\n",
      "Iteration 2, loss = 0.74423139\n",
      "Iteration 3, loss = 0.72243804\n",
      "Iteration 4, loss = 0.70409977\n",
      "Iteration 5, loss = 0.68927767\n",
      "Iteration 6, loss = 0.67713585\n",
      "Iteration 7, loss = 0.66391720\n",
      "Iteration 8, loss = 0.64852091\n",
      "Iteration 9, loss = 0.63047769\n",
      "Iteration 10, loss = 0.61174794\n",
      "Iteration 11, loss = 0.59218737\n",
      "Iteration 12, loss = 0.57407302\n",
      "Iteration 13, loss = 0.55786601\n",
      "Iteration 14, loss = 0.54394519\n",
      "Iteration 15, loss = 0.53166229\n",
      "Iteration 16, loss = 0.52108787\n",
      "Iteration 17, loss = 0.51158942\n",
      "Iteration 18, loss = 0.50297833\n",
      "Iteration 19, loss = 0.49553093\n",
      "Iteration 20, loss = 0.48849962\n",
      "Iteration 21, loss = 0.48206617\n",
      "Iteration 22, loss = 0.47619374\n",
      "Iteration 23, loss = 0.47064334\n",
      "Iteration 24, loss = 0.46527470\n",
      "Iteration 25, loss = 0.46028453\n",
      "Iteration 26, loss = 0.45535064\n",
      "Iteration 27, loss = 0.45077929\n",
      "Iteration 28, loss = 0.44660222\n",
      "Iteration 29, loss = 0.44272722\n",
      "Iteration 30, loss = 0.43837226\n",
      "Iteration 31, loss = 0.43467516\n",
      "Iteration 32, loss = 0.43078727\n",
      "Iteration 33, loss = 0.42700712\n",
      "Iteration 34, loss = 0.42344156\n",
      "Iteration 35, loss = 0.41992112\n",
      "Iteration 36, loss = 0.41659563\n",
      "Iteration 37, loss = 0.41326376\n",
      "Iteration 38, loss = 0.40985279\n",
      "Iteration 39, loss = 0.40656745\n",
      "Iteration 40, loss = 0.40362869\n",
      "Iteration 41, loss = 0.40067756\n",
      "Iteration 42, loss = 0.39767922\n",
      "Iteration 43, loss = 0.39494284\n",
      "Iteration 44, loss = 0.39194998\n",
      "Iteration 45, loss = 0.38924526\n",
      "Iteration 46, loss = 0.38639182\n",
      "Iteration 47, loss = 0.38381253\n",
      "Iteration 48, loss = 0.38184332\n",
      "Iteration 49, loss = 0.37943046\n",
      "Iteration 50, loss = 0.37664869\n",
      "Iteration 51, loss = 0.37399186\n",
      "Iteration 52, loss = 0.37197843\n",
      "Iteration 53, loss = 0.36961502\n",
      "Iteration 54, loss = 0.36712968\n",
      "Iteration 55, loss = 0.36498753\n",
      "Iteration 56, loss = 0.36310981\n",
      "Iteration 57, loss = 0.36052744\n",
      "Iteration 58, loss = 0.35868070\n",
      "Iteration 59, loss = 0.35647541\n",
      "Iteration 60, loss = 0.35441818\n",
      "Iteration 61, loss = 0.35267301\n",
      "Iteration 62, loss = 0.35065494\n",
      "Iteration 63, loss = 0.34872644\n",
      "Iteration 64, loss = 0.34719680\n",
      "Iteration 65, loss = 0.34517318\n",
      "Iteration 66, loss = 0.34336333\n",
      "Iteration 67, loss = 0.34139179\n",
      "Iteration 68, loss = 0.33978584\n",
      "Iteration 69, loss = 0.33779245\n",
      "Iteration 70, loss = 0.33589808\n",
      "Iteration 71, loss = 0.33409025\n",
      "Iteration 72, loss = 0.33233161\n",
      "Iteration 73, loss = 0.33080028\n",
      "Iteration 74, loss = 0.32919369\n",
      "Iteration 75, loss = 0.32762664\n",
      "Iteration 76, loss = 0.32585662\n",
      "Iteration 77, loss = 0.32442761\n",
      "Iteration 78, loss = 0.32319595\n",
      "Iteration 79, loss = 0.32134477\n",
      "Iteration 80, loss = 0.32003463\n",
      "Iteration 81, loss = 0.31896103\n",
      "Iteration 82, loss = 0.31712243\n",
      "Iteration 83, loss = 0.31569337\n",
      "Iteration 84, loss = 0.31437341\n",
      "Iteration 85, loss = 0.31308643\n",
      "Iteration 86, loss = 0.31173833\n",
      "Iteration 87, loss = 0.31046000\n",
      "Iteration 88, loss = 0.30928257\n",
      "Iteration 89, loss = 0.30787785\n",
      "Iteration 90, loss = 0.30659205\n",
      "Iteration 91, loss = 0.30548827\n",
      "Iteration 92, loss = 0.30441457\n",
      "Iteration 93, loss = 0.30302813\n",
      "Iteration 94, loss = 0.30183537\n",
      "Iteration 95, loss = 0.30088092\n",
      "Iteration 96, loss = 0.29953853\n",
      "Iteration 97, loss = 0.29875889\n",
      "Iteration 98, loss = 0.29754139\n",
      "Iteration 99, loss = 0.29651612\n",
      "Iteration 100, loss = 0.29558507\n",
      "Iteration 101, loss = 0.29449898\n",
      "Iteration 102, loss = 0.29422565\n",
      "Iteration 103, loss = 0.29228928\n",
      "Iteration 104, loss = 0.29145678\n",
      "Iteration 105, loss = 0.29035301\n",
      "Iteration 106, loss = 0.28935391\n",
      "Iteration 107, loss = 0.28832743\n",
      "Iteration 108, loss = 0.28757471\n",
      "Iteration 109, loss = 0.28677776\n",
      "Iteration 110, loss = 0.28561331\n",
      "Iteration 111, loss = 0.28461821\n",
      "Iteration 112, loss = 0.28364673\n",
      "Iteration 113, loss = 0.28277854\n",
      "Iteration 114, loss = 0.28179649\n",
      "Iteration 115, loss = 0.28090937\n",
      "Iteration 116, loss = 0.28026328\n",
      "Iteration 117, loss = 0.27954069\n",
      "Iteration 118, loss = 0.27859974\n",
      "Iteration 119, loss = 0.27754636\n",
      "Iteration 120, loss = 0.27709039\n",
      "Iteration 121, loss = 0.27613071\n",
      "Iteration 122, loss = 0.27510669\n",
      "Iteration 123, loss = 0.27453802\n",
      "Iteration 124, loss = 0.27349482\n",
      "Iteration 125, loss = 0.27275230\n",
      "Iteration 126, loss = 0.27189711\n",
      "Iteration 127, loss = 0.27109478\n",
      "Iteration 128, loss = 0.27032971\n",
      "Iteration 129, loss = 0.26943997\n",
      "Iteration 130, loss = 0.26899768\n",
      "Iteration 131, loss = 0.26769068\n",
      "Iteration 132, loss = 0.26742193\n",
      "Iteration 133, loss = 0.26627147\n",
      "Iteration 134, loss = 0.26565196\n",
      "Iteration 135, loss = 0.26480647\n",
      "Iteration 136, loss = 0.26434541\n",
      "Iteration 137, loss = 0.26336426\n",
      "Iteration 138, loss = 0.26313126\n",
      "Iteration 139, loss = 0.26213493\n",
      "Iteration 140, loss = 0.26118838\n",
      "Iteration 141, loss = 0.26048659\n",
      "Iteration 142, loss = 0.25993751\n",
      "Iteration 143, loss = 0.25909305\n",
      "Iteration 144, loss = 0.25823897\n",
      "Iteration 145, loss = 0.25784306\n",
      "Iteration 146, loss = 0.25685173\n",
      "Iteration 147, loss = 0.25671244\n",
      "Iteration 148, loss = 0.25550799\n",
      "Iteration 149, loss = 0.25512303\n",
      "Iteration 150, loss = 0.25431500\n",
      "Iteration 151, loss = 0.25375412\n",
      "Iteration 152, loss = 0.25313258\n",
      "Iteration 153, loss = 0.25241449\n",
      "Iteration 154, loss = 0.25201052\n",
      "Iteration 155, loss = 0.25143961\n",
      "Iteration 156, loss = 0.25071200\n",
      "Iteration 157, loss = 0.24986156\n",
      "Iteration 158, loss = 0.24950451\n",
      "Iteration 159, loss = 0.24869296\n",
      "Iteration 160, loss = 0.24838244\n",
      "Iteration 161, loss = 0.24785243\n",
      "Iteration 162, loss = 0.24689386\n",
      "Iteration 163, loss = 0.24641520\n",
      "Iteration 164, loss = 0.24604405\n",
      "Iteration 165, loss = 0.24537698\n",
      "Iteration 166, loss = 0.24465773\n",
      "Iteration 167, loss = 0.24435401\n",
      "Iteration 168, loss = 0.24373218\n",
      "Iteration 169, loss = 0.24312405\n",
      "Iteration 170, loss = 0.24266304\n",
      "Iteration 171, loss = 0.24211914\n",
      "Iteration 172, loss = 0.24147861\n",
      "Iteration 173, loss = 0.24117793\n",
      "Iteration 174, loss = 0.24049247\n",
      "Iteration 175, loss = 0.23991409\n",
      "Iteration 176, loss = 0.23906326\n",
      "Iteration 177, loss = 0.23871277\n",
      "Iteration 178, loss = 0.23814751\n",
      "Iteration 179, loss = 0.23777462\n",
      "Iteration 180, loss = 0.23785626\n",
      "Iteration 181, loss = 0.23649170\n",
      "Iteration 182, loss = 0.23628968\n",
      "Iteration 183, loss = 0.23553308\n",
      "Iteration 184, loss = 0.23548080\n",
      "Iteration 185, loss = 0.23483295\n",
      "Iteration 186, loss = 0.23436987\n",
      "Iteration 187, loss = 0.23370481\n",
      "Iteration 188, loss = 0.23393351\n",
      "Iteration 189, loss = 0.23280738\n",
      "Iteration 190, loss = 0.23219802\n",
      "Iteration 191, loss = 0.23183465\n",
      "Iteration 192, loss = 0.23095783\n",
      "Iteration 193, loss = 0.23102595\n",
      "Iteration 194, loss = 0.23019157\n",
      "Iteration 195, loss = 0.22993812\n",
      "Iteration 196, loss = 0.22972181\n",
      "Iteration 197, loss = 0.22917384\n",
      "Iteration 198, loss = 0.22883724\n",
      "Iteration 199, loss = 0.22866837\n",
      "Iteration 200, loss = 0.22775436\n",
      "Iteration 201, loss = 0.22762934\n",
      "Iteration 202, loss = 0.22734339\n",
      "Iteration 203, loss = 0.22701370\n",
      "Iteration 204, loss = 0.22643690\n",
      "Iteration 205, loss = 0.22606795\n",
      "Iteration 206, loss = 0.22537449\n",
      "Iteration 207, loss = 0.22611293\n",
      "Iteration 208, loss = 0.22500646\n",
      "Iteration 209, loss = 0.22483241\n",
      "Iteration 210, loss = 0.22419989\n",
      "Iteration 211, loss = 0.22361869\n",
      "Iteration 212, loss = 0.22358207\n",
      "Iteration 213, loss = 0.22305832\n",
      "Iteration 214, loss = 0.22255297\n",
      "Iteration 215, loss = 0.22280227\n",
      "Iteration 216, loss = 0.22191777\n",
      "Iteration 217, loss = 0.22184913\n",
      "Iteration 218, loss = 0.22169350\n",
      "Iteration 219, loss = 0.22089071\n",
      "Iteration 220, loss = 0.22070410\n",
      "Iteration 221, loss = 0.22087652\n",
      "Iteration 222, loss = 0.22000227\n",
      "Iteration 223, loss = 0.22005058\n",
      "Iteration 224, loss = 0.21915629\n",
      "Iteration 225, loss = 0.21922356\n",
      "Iteration 226, loss = 0.21847336\n",
      "Iteration 227, loss = 0.21843673\n",
      "Iteration 228, loss = 0.21792930\n",
      "Iteration 229, loss = 0.21760752\n",
      "Iteration 230, loss = 0.21713672\n",
      "Iteration 231, loss = 0.21705353\n",
      "Iteration 232, loss = 0.21662797\n",
      "Iteration 233, loss = 0.21611270\n",
      "Iteration 234, loss = 0.21641063\n",
      "Iteration 235, loss = 0.21572497\n",
      "Iteration 236, loss = 0.21529791\n",
      "Iteration 237, loss = 0.21545263\n",
      "Iteration 238, loss = 0.21512008\n",
      "Iteration 239, loss = 0.21503358\n",
      "Iteration 240, loss = 0.21416497\n",
      "Iteration 241, loss = 0.21400632\n",
      "Iteration 242, loss = 0.21367344\n",
      "Iteration 243, loss = 0.21350212\n",
      "Iteration 244, loss = 0.21337344\n",
      "Iteration 245, loss = 0.21307548\n",
      "Iteration 246, loss = 0.21270538\n",
      "Iteration 247, loss = 0.21228537\n",
      "Iteration 248, loss = 0.21250130\n",
      "Iteration 249, loss = 0.21181926\n",
      "Iteration 250, loss = 0.21140541\n",
      "Iteration 251, loss = 0.21168576\n",
      "Iteration 252, loss = 0.21063262\n",
      "Iteration 253, loss = 0.21083967\n",
      "Iteration 254, loss = 0.21088412\n",
      "Iteration 255, loss = 0.20968105\n",
      "Iteration 256, loss = 0.20921522\n",
      "Iteration 257, loss = 0.20885542\n",
      "Iteration 258, loss = 0.20854392\n",
      "Iteration 259, loss = 0.20803335\n",
      "Iteration 260, loss = 0.20824480\n",
      "Iteration 261, loss = 0.20771130\n",
      "Iteration 262, loss = 0.20704050\n",
      "Iteration 263, loss = 0.20737702\n",
      "Iteration 264, loss = 0.20671083\n",
      "Iteration 265, loss = 0.20639832\n",
      "Iteration 266, loss = 0.20598007\n",
      "Iteration 267, loss = 0.20608769\n",
      "Iteration 268, loss = 0.20596196\n",
      "Iteration 269, loss = 0.20537595\n",
      "Iteration 270, loss = 0.20527539\n",
      "Iteration 271, loss = 0.20555139\n",
      "Iteration 272, loss = 0.20526546\n",
      "Iteration 273, loss = 0.20425062\n",
      "Iteration 274, loss = 0.20422729\n",
      "Iteration 275, loss = 0.20475228\n",
      "Iteration 276, loss = 0.20358466\n",
      "Iteration 277, loss = 0.20390255\n",
      "Iteration 278, loss = 0.20355809\n",
      "Iteration 279, loss = 0.20398931\n",
      "Iteration 280, loss = 0.20288740\n",
      "Iteration 281, loss = 0.20256246\n",
      "Iteration 282, loss = 0.20231217\n",
      "Iteration 283, loss = 0.20253788\n",
      "Iteration 284, loss = 0.20179724\n",
      "Iteration 285, loss = 0.20184089\n",
      "Iteration 286, loss = 0.20186315\n",
      "Iteration 287, loss = 0.20153077\n",
      "Iteration 288, loss = 0.20130120\n",
      "Iteration 289, loss = 0.20101016\n",
      "Iteration 290, loss = 0.20073606\n",
      "Iteration 291, loss = 0.20048554\n",
      "Iteration 292, loss = 0.20040128\n",
      "Iteration 293, loss = 0.20007678\n",
      "Iteration 294, loss = 0.20008808\n",
      "Iteration 295, loss = 0.19978018\n",
      "Iteration 296, loss = 0.20007220\n",
      "Iteration 297, loss = 0.19985970\n",
      "Iteration 298, loss = 0.19966393\n",
      "Iteration 299, loss = 0.19911839\n",
      "Iteration 300, loss = 0.19938638\n",
      "Iteration 301, loss = 0.19902170\n",
      "Iteration 302, loss = 0.19871639\n",
      "Iteration 303, loss = 0.19864064\n",
      "Iteration 304, loss = 0.19881408\n",
      "Iteration 305, loss = 0.19820618\n",
      "Iteration 306, loss = 0.19835784\n",
      "Iteration 307, loss = 0.19900132\n",
      "Iteration 308, loss = 0.19788327\n",
      "Iteration 309, loss = 0.19792137\n",
      "Iteration 310, loss = 0.19789033\n",
      "Iteration 311, loss = 0.19751950\n",
      "Iteration 312, loss = 0.19748349\n",
      "Iteration 313, loss = 0.19734497\n",
      "Iteration 314, loss = 0.19710475\n",
      "Iteration 315, loss = 0.19683190\n",
      "Iteration 316, loss = 0.19692550\n",
      "Iteration 317, loss = 0.19658171\n",
      "Iteration 318, loss = 0.19684067\n",
      "Iteration 319, loss = 0.19660315\n",
      "Iteration 320, loss = 0.19619759\n",
      "Iteration 321, loss = 0.19606240\n",
      "Iteration 322, loss = 0.19603248\n",
      "Iteration 323, loss = 0.19600383\n",
      "Iteration 324, loss = 0.19597290\n",
      "Iteration 325, loss = 0.19616196\n",
      "Iteration 326, loss = 0.19553351\n",
      "Iteration 327, loss = 0.19543477\n",
      "Iteration 328, loss = 0.19566530\n",
      "Iteration 329, loss = 0.19515357\n",
      "Iteration 330, loss = 0.19539754\n",
      "Iteration 331, loss = 0.19540076\n",
      "Iteration 332, loss = 0.19495857\n",
      "Iteration 333, loss = 0.19518706\n",
      "Iteration 334, loss = 0.19490285\n",
      "Iteration 335, loss = 0.19517286\n",
      "Iteration 336, loss = 0.19448287\n",
      "Iteration 337, loss = 0.19454703\n",
      "Iteration 338, loss = 0.19461376\n",
      "Iteration 339, loss = 0.19403218\n",
      "Iteration 340, loss = 0.19457355\n",
      "Iteration 341, loss = 0.19487638\n",
      "Iteration 342, loss = 0.19460144\n",
      "Iteration 343, loss = 0.19434664\n",
      "Iteration 344, loss = 0.19414349\n",
      "Iteration 345, loss = 0.19401746\n",
      "Iteration 346, loss = 0.19378411\n",
      "Iteration 347, loss = 0.19374234\n",
      "Iteration 348, loss = 0.19348603\n",
      "Iteration 349, loss = 0.19394788\n",
      "Iteration 350, loss = 0.19332341\n",
      "Iteration 351, loss = 0.19384970\n",
      "Iteration 352, loss = 0.19359801\n",
      "Iteration 353, loss = 0.19483949\n",
      "Iteration 354, loss = 0.19387104\n",
      "Iteration 355, loss = 0.19313206\n",
      "Iteration 356, loss = 0.19285934\n",
      "Iteration 357, loss = 0.19354213\n",
      "Iteration 358, loss = 0.19298583\n",
      "Iteration 359, loss = 0.19317650\n",
      "Iteration 360, loss = 0.19292780\n",
      "Iteration 361, loss = 0.19337192\n",
      "Iteration 362, loss = 0.19264050\n",
      "Iteration 363, loss = 0.19269069\n",
      "Iteration 364, loss = 0.19256371\n",
      "Iteration 365, loss = 0.19250136\n",
      "Iteration 366, loss = 0.19224635\n",
      "Iteration 367, loss = 0.19224546\n",
      "Iteration 368, loss = 0.19222243\n",
      "Iteration 369, loss = 0.19221383\n",
      "Iteration 370, loss = 0.19193533\n",
      "Iteration 371, loss = 0.19209553\n",
      "Iteration 372, loss = 0.19178398\n",
      "Iteration 373, loss = 0.19186049\n",
      "Iteration 374, loss = 0.19280604\n",
      "Iteration 375, loss = 0.19132534\n",
      "Iteration 376, loss = 0.19233311\n",
      "Iteration 377, loss = 0.19183450\n",
      "Iteration 378, loss = 0.19215908\n",
      "Iteration 379, loss = 0.19235464\n",
      "Iteration 380, loss = 0.19165995\n",
      "Iteration 381, loss = 0.19168070\n",
      "Iteration 382, loss = 0.19130136\n",
      "Iteration 383, loss = 0.19130136\n",
      "Iteration 384, loss = 0.19140104\n",
      "Iteration 385, loss = 0.19139859\n",
      "Iteration 386, loss = 0.19097401\n",
      "Iteration 387, loss = 0.19095141\n",
      "Iteration 388, loss = 0.19109782\n",
      "Iteration 389, loss = 0.19102487\n",
      "Iteration 390, loss = 0.19232372\n",
      "Iteration 391, loss = 0.19125192\n",
      "Iteration 392, loss = 0.19037952\n",
      "Iteration 393, loss = 0.19058039\n",
      "Iteration 394, loss = 0.19054994\n",
      "Iteration 395, loss = 0.19044282\n",
      "Iteration 396, loss = 0.19063482\n",
      "Iteration 397, loss = 0.19060424\n",
      "Iteration 398, loss = 0.19031788\n",
      "Iteration 399, loss = 0.18997769\n",
      "Iteration 400, loss = 0.19022371\n",
      "Iteration 401, loss = 0.19052044\n",
      "Iteration 402, loss = 0.19111498\n",
      "Iteration 403, loss = 0.19063500\n",
      "Iteration 404, loss = 0.19056376\n",
      "Iteration 405, loss = 0.18977577\n",
      "Iteration 406, loss = 0.19003401\n",
      "Iteration 407, loss = 0.19026092\n",
      "Iteration 408, loss = 0.18961367\n",
      "Iteration 409, loss = 0.18993592\n",
      "Iteration 410, loss = 0.18943175\n",
      "Iteration 411, loss = 0.18985643\n",
      "Iteration 412, loss = 0.18999986\n",
      "Iteration 413, loss = 0.18918569\n",
      "Iteration 414, loss = 0.18942417\n",
      "Iteration 415, loss = 0.18970448\n",
      "Iteration 416, loss = 0.18956057\n",
      "Iteration 417, loss = 0.18938149\n",
      "Iteration 418, loss = 0.18938016\n",
      "Iteration 419, loss = 0.18920166\n",
      "Iteration 420, loss = 0.18922952\n",
      "Iteration 421, loss = 0.18894904\n",
      "Iteration 422, loss = 0.18917197\n",
      "Iteration 423, loss = 0.18891395\n",
      "Iteration 424, loss = 0.18922684\n",
      "Iteration 425, loss = 0.18869503\n",
      "Iteration 426, loss = 0.18942550\n",
      "Iteration 427, loss = 0.18874692\n",
      "Iteration 428, loss = 0.18860237\n",
      "Iteration 429, loss = 0.18872765\n",
      "Iteration 430, loss = 0.18998222\n",
      "Iteration 431, loss = 0.18899216\n",
      "Iteration 432, loss = 0.18875689\n",
      "Iteration 433, loss = 0.18897154\n",
      "Iteration 434, loss = 0.18925903\n",
      "Iteration 435, loss = 0.18844542\n",
      "Iteration 436, loss = 0.18851817\n",
      "Iteration 437, loss = 0.18848211\n",
      "Iteration 438, loss = 0.18877132\n",
      "Iteration 439, loss = 0.18839266\n",
      "Iteration 440, loss = 0.18849823\n",
      "Iteration 441, loss = 0.18854659\n",
      "Iteration 442, loss = 0.18795035\n",
      "Iteration 443, loss = 0.18818840\n",
      "Iteration 444, loss = 0.18800035\n",
      "Iteration 445, loss = 0.18811627\n",
      "Iteration 446, loss = 0.18792858\n",
      "Iteration 447, loss = 0.18773351\n",
      "Iteration 448, loss = 0.18820100\n",
      "Iteration 449, loss = 0.18816456\n",
      "Iteration 450, loss = 0.18806971\n",
      "Iteration 451, loss = 0.18802989\n",
      "Iteration 452, loss = 0.18872258\n",
      "Iteration 453, loss = 0.18804139\n",
      "Iteration 454, loss = 0.18774009\n",
      "Iteration 455, loss = 0.18838053\n",
      "Iteration 456, loss = 0.18746503\n",
      "Iteration 457, loss = 0.18767268\n",
      "Iteration 458, loss = 0.18782746\n",
      "Iteration 459, loss = 0.18806899\n",
      "Iteration 460, loss = 0.18751037\n",
      "Iteration 461, loss = 0.18775157\n",
      "Iteration 462, loss = 0.18717410\n",
      "Iteration 463, loss = 0.18737361\n",
      "Iteration 464, loss = 0.18704074\n",
      "Iteration 465, loss = 0.18706816\n",
      "Iteration 466, loss = 0.18693334\n",
      "Iteration 467, loss = 0.18723487\n",
      "Iteration 468, loss = 0.18740477\n",
      "Iteration 469, loss = 0.18669145\n",
      "Iteration 470, loss = 0.18690472\n",
      "Iteration 471, loss = 0.18785800\n",
      "Iteration 472, loss = 0.18639993\n",
      "Iteration 473, loss = 0.18731480\n",
      "Iteration 474, loss = 0.18682694\n",
      "Iteration 475, loss = 0.18701042\n",
      "Iteration 476, loss = 0.18718572\n",
      "Iteration 477, loss = 0.18661973\n",
      "Iteration 478, loss = 0.18671519\n",
      "Iteration 479, loss = 0.18644044\n",
      "Iteration 480, loss = 0.18662990\n",
      "Iteration 481, loss = 0.18662860\n",
      "Iteration 482, loss = 0.18679908\n",
      "Iteration 483, loss = 0.18665661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77038063\n",
      "Iteration 2, loss = 0.74397999\n",
      "Iteration 3, loss = 0.72216955\n",
      "Iteration 4, loss = 0.70488537\n",
      "Iteration 5, loss = 0.69047327\n",
      "Iteration 6, loss = 0.67715973\n",
      "Iteration 7, loss = 0.66377320\n",
      "Iteration 8, loss = 0.64897216\n",
      "Iteration 9, loss = 0.63144692\n",
      "Iteration 10, loss = 0.61240862\n",
      "Iteration 11, loss = 0.59295363\n",
      "Iteration 12, loss = 0.57477591\n",
      "Iteration 13, loss = 0.55827892\n",
      "Iteration 14, loss = 0.54414816\n",
      "Iteration 15, loss = 0.53209103\n",
      "Iteration 16, loss = 0.52163985\n",
      "Iteration 17, loss = 0.51189055\n",
      "Iteration 18, loss = 0.50320115\n",
      "Iteration 19, loss = 0.49528181\n",
      "Iteration 20, loss = 0.48804755\n",
      "Iteration 21, loss = 0.48176994\n",
      "Iteration 22, loss = 0.47560547\n",
      "Iteration 23, loss = 0.46980157\n",
      "Iteration 24, loss = 0.46461604\n",
      "Iteration 25, loss = 0.45970587\n",
      "Iteration 26, loss = 0.45517592\n",
      "Iteration 27, loss = 0.45071534\n",
      "Iteration 28, loss = 0.44658813\n",
      "Iteration 29, loss = 0.44240539\n",
      "Iteration 30, loss = 0.43856631\n",
      "Iteration 31, loss = 0.43485894\n",
      "Iteration 32, loss = 0.43140738\n",
      "Iteration 33, loss = 0.42796069\n",
      "Iteration 34, loss = 0.42388426\n",
      "Iteration 35, loss = 0.42032116\n",
      "Iteration 36, loss = 0.41691009\n",
      "Iteration 37, loss = 0.41368463\n",
      "Iteration 38, loss = 0.41043616\n",
      "Iteration 39, loss = 0.40722196\n",
      "Iteration 40, loss = 0.40451234\n",
      "Iteration 41, loss = 0.40111055\n",
      "Iteration 42, loss = 0.39846395\n",
      "Iteration 43, loss = 0.39560100\n",
      "Iteration 44, loss = 0.39285940\n",
      "Iteration 45, loss = 0.39027689\n",
      "Iteration 46, loss = 0.38764567\n",
      "Iteration 47, loss = 0.38509460\n",
      "Iteration 48, loss = 0.38262998\n",
      "Iteration 49, loss = 0.38008550\n",
      "Iteration 50, loss = 0.37762886\n",
      "Iteration 51, loss = 0.37546814\n",
      "Iteration 52, loss = 0.37285470\n",
      "Iteration 53, loss = 0.37067090\n",
      "Iteration 54, loss = 0.36829850\n",
      "Iteration 55, loss = 0.36586316\n",
      "Iteration 56, loss = 0.36355687\n",
      "Iteration 57, loss = 0.36126757\n",
      "Iteration 58, loss = 0.35928485\n",
      "Iteration 59, loss = 0.35718884\n",
      "Iteration 60, loss = 0.35502836\n",
      "Iteration 61, loss = 0.35371600\n",
      "Iteration 62, loss = 0.35136077\n",
      "Iteration 63, loss = 0.34919670\n",
      "Iteration 64, loss = 0.34714984\n",
      "Iteration 65, loss = 0.34543612\n",
      "Iteration 66, loss = 0.34347169\n",
      "Iteration 67, loss = 0.34169192\n",
      "Iteration 68, loss = 0.34024716\n",
      "Iteration 69, loss = 0.33850059\n",
      "Iteration 70, loss = 0.33730324\n",
      "Iteration 71, loss = 0.33466134\n",
      "Iteration 72, loss = 0.33325609\n",
      "Iteration 73, loss = 0.33164145\n",
      "Iteration 74, loss = 0.32986837\n",
      "Iteration 75, loss = 0.32818576\n",
      "Iteration 76, loss = 0.32673679\n",
      "Iteration 77, loss = 0.32516631\n",
      "Iteration 78, loss = 0.32363611\n",
      "Iteration 79, loss = 0.32197658\n",
      "Iteration 80, loss = 0.32041766\n",
      "Iteration 81, loss = 0.31895733\n",
      "Iteration 82, loss = 0.31752789\n",
      "Iteration 83, loss = 0.31626900\n",
      "Iteration 84, loss = 0.31448985\n",
      "Iteration 85, loss = 0.31328583\n",
      "Iteration 86, loss = 0.31176545\n",
      "Iteration 87, loss = 0.31023157\n",
      "Iteration 88, loss = 0.30909870\n",
      "Iteration 89, loss = 0.30756131\n",
      "Iteration 90, loss = 0.30645648\n",
      "Iteration 91, loss = 0.30488703\n",
      "Iteration 92, loss = 0.30370251\n",
      "Iteration 93, loss = 0.30209094\n",
      "Iteration 94, loss = 0.30107687\n",
      "Iteration 95, loss = 0.29967605\n",
      "Iteration 96, loss = 0.29851862\n",
      "Iteration 97, loss = 0.29745419\n",
      "Iteration 98, loss = 0.29611345\n",
      "Iteration 99, loss = 0.29494662\n",
      "Iteration 100, loss = 0.29398707\n",
      "Iteration 101, loss = 0.29275694\n",
      "Iteration 102, loss = 0.29190804\n",
      "Iteration 103, loss = 0.29054363\n",
      "Iteration 104, loss = 0.28952782\n",
      "Iteration 105, loss = 0.28824231\n",
      "Iteration 106, loss = 0.28787515\n",
      "Iteration 107, loss = 0.28618845\n",
      "Iteration 108, loss = 0.28525720\n",
      "Iteration 109, loss = 0.28428654\n",
      "Iteration 110, loss = 0.28334214\n",
      "Iteration 111, loss = 0.28229072\n",
      "Iteration 112, loss = 0.28132089\n",
      "Iteration 113, loss = 0.28063842\n",
      "Iteration 114, loss = 0.27950636\n",
      "Iteration 115, loss = 0.27891477\n",
      "Iteration 116, loss = 0.27813264\n",
      "Iteration 117, loss = 0.27748633\n",
      "Iteration 118, loss = 0.27629158\n",
      "Iteration 119, loss = 0.27506175\n",
      "Iteration 120, loss = 0.27441220\n",
      "Iteration 121, loss = 0.27327876\n",
      "Iteration 122, loss = 0.27279237\n",
      "Iteration 123, loss = 0.27186957\n",
      "Iteration 124, loss = 0.27110179\n",
      "Iteration 125, loss = 0.27023615\n",
      "Iteration 126, loss = 0.26955657\n",
      "Iteration 127, loss = 0.26889664\n",
      "Iteration 128, loss = 0.26805655\n",
      "Iteration 129, loss = 0.26713835\n",
      "Iteration 130, loss = 0.26640765\n",
      "Iteration 131, loss = 0.26570273\n",
      "Iteration 132, loss = 0.26566143\n",
      "Iteration 133, loss = 0.26497250\n",
      "Iteration 134, loss = 0.26357330\n",
      "Iteration 135, loss = 0.26295596\n",
      "Iteration 136, loss = 0.26232515\n",
      "Iteration 137, loss = 0.26159636\n",
      "Iteration 138, loss = 0.26105152\n",
      "Iteration 139, loss = 0.25999823\n",
      "Iteration 140, loss = 0.26015060\n",
      "Iteration 141, loss = 0.25879612\n",
      "Iteration 142, loss = 0.25823948\n",
      "Iteration 143, loss = 0.25774927\n",
      "Iteration 144, loss = 0.25707605\n",
      "Iteration 145, loss = 0.25658048\n",
      "Iteration 146, loss = 0.25591138\n",
      "Iteration 147, loss = 0.25546676\n",
      "Iteration 148, loss = 0.25468696\n",
      "Iteration 149, loss = 0.25415827\n",
      "Iteration 150, loss = 0.25365083\n",
      "Iteration 151, loss = 0.25339658\n",
      "Iteration 152, loss = 0.25302048\n",
      "Iteration 153, loss = 0.25236076\n",
      "Iteration 154, loss = 0.25143764\n",
      "Iteration 155, loss = 0.25084286\n",
      "Iteration 156, loss = 0.25024691\n",
      "Iteration 157, loss = 0.25011623\n",
      "Iteration 158, loss = 0.24957092\n",
      "Iteration 159, loss = 0.24892396\n",
      "Iteration 160, loss = 0.24832928\n",
      "Iteration 161, loss = 0.24791320\n",
      "Iteration 162, loss = 0.24745735\n",
      "Iteration 163, loss = 0.24688984\n",
      "Iteration 164, loss = 0.24644403\n",
      "Iteration 165, loss = 0.24588521\n",
      "Iteration 166, loss = 0.24537543\n",
      "Iteration 167, loss = 0.24522740\n",
      "Iteration 168, loss = 0.24454682\n",
      "Iteration 169, loss = 0.24440712\n",
      "Iteration 170, loss = 0.24429416\n",
      "Iteration 171, loss = 0.24279005\n",
      "Iteration 172, loss = 0.24257779\n",
      "Iteration 173, loss = 0.24233181\n",
      "Iteration 174, loss = 0.24220029\n",
      "Iteration 175, loss = 0.24136077\n",
      "Iteration 176, loss = 0.24107681\n",
      "Iteration 177, loss = 0.24082241\n",
      "Iteration 178, loss = 0.24010069\n",
      "Iteration 179, loss = 0.23976076\n",
      "Iteration 180, loss = 0.23920876\n",
      "Iteration 181, loss = 0.23880379\n",
      "Iteration 182, loss = 0.23873342\n",
      "Iteration 183, loss = 0.23786058\n",
      "Iteration 184, loss = 0.23765804\n",
      "Iteration 185, loss = 0.23713557\n",
      "Iteration 186, loss = 0.23664929\n",
      "Iteration 187, loss = 0.23684684\n",
      "Iteration 188, loss = 0.23610155\n",
      "Iteration 189, loss = 0.23577270\n",
      "Iteration 190, loss = 0.23552168\n",
      "Iteration 191, loss = 0.23497643\n",
      "Iteration 192, loss = 0.23475201\n",
      "Iteration 193, loss = 0.23412978\n",
      "Iteration 194, loss = 0.23496237\n",
      "Iteration 195, loss = 0.23369332\n",
      "Iteration 196, loss = 0.23315517\n",
      "Iteration 197, loss = 0.23310596\n",
      "Iteration 198, loss = 0.23337269\n",
      "Iteration 199, loss = 0.23201505\n",
      "Iteration 200, loss = 0.23319086\n",
      "Iteration 201, loss = 0.23152233\n",
      "Iteration 202, loss = 0.23128571\n",
      "Iteration 203, loss = 0.23081532\n",
      "Iteration 204, loss = 0.23061360\n",
      "Iteration 205, loss = 0.23097362\n",
      "Iteration 206, loss = 0.22996936\n",
      "Iteration 207, loss = 0.23019003\n",
      "Iteration 208, loss = 0.23017330\n",
      "Iteration 209, loss = 0.22983337\n",
      "Iteration 210, loss = 0.22992560\n",
      "Iteration 211, loss = 0.22892257\n",
      "Iteration 212, loss = 0.22841059\n",
      "Iteration 213, loss = 0.22815784\n",
      "Iteration 214, loss = 0.22767413\n",
      "Iteration 215, loss = 0.22768705\n",
      "Iteration 216, loss = 0.22744820\n",
      "Iteration 217, loss = 0.22711885\n",
      "Iteration 218, loss = 0.22734681\n",
      "Iteration 219, loss = 0.22633339\n",
      "Iteration 220, loss = 0.22665489\n",
      "Iteration 221, loss = 0.22609788\n",
      "Iteration 222, loss = 0.22578539\n",
      "Iteration 223, loss = 0.22574662\n",
      "Iteration 224, loss = 0.22648030\n",
      "Iteration 225, loss = 0.22528548\n",
      "Iteration 226, loss = 0.22537412\n",
      "Iteration 227, loss = 0.22473099\n",
      "Iteration 228, loss = 0.22438617\n",
      "Iteration 229, loss = 0.22468615\n",
      "Iteration 230, loss = 0.22443891\n",
      "Iteration 231, loss = 0.22416800\n",
      "Iteration 232, loss = 0.22388412\n",
      "Iteration 233, loss = 0.22305578\n",
      "Iteration 234, loss = 0.22298875\n",
      "Iteration 235, loss = 0.22290761\n",
      "Iteration 236, loss = 0.22283740\n",
      "Iteration 237, loss = 0.22227024\n",
      "Iteration 238, loss = 0.22239704\n",
      "Iteration 239, loss = 0.22217452\n",
      "Iteration 240, loss = 0.22157556\n",
      "Iteration 241, loss = 0.22127685\n",
      "Iteration 242, loss = 0.22134995\n",
      "Iteration 243, loss = 0.22107764\n",
      "Iteration 244, loss = 0.22097854\n",
      "Iteration 245, loss = 0.22097549\n",
      "Iteration 246, loss = 0.22046961\n",
      "Iteration 247, loss = 0.22018704\n",
      "Iteration 248, loss = 0.21970096\n",
      "Iteration 249, loss = 0.21973764\n",
      "Iteration 250, loss = 0.21947933\n",
      "Iteration 251, loss = 0.21975993\n",
      "Iteration 252, loss = 0.21926377\n",
      "Iteration 253, loss = 0.21908147\n",
      "Iteration 254, loss = 0.21862732\n",
      "Iteration 255, loss = 0.21833885\n",
      "Iteration 256, loss = 0.21803426\n",
      "Iteration 257, loss = 0.21798264\n",
      "Iteration 258, loss = 0.21789928\n",
      "Iteration 259, loss = 0.21761113\n",
      "Iteration 260, loss = 0.21757777\n",
      "Iteration 261, loss = 0.21670989\n",
      "Iteration 262, loss = 0.21751943\n",
      "Iteration 263, loss = 0.21688414\n",
      "Iteration 264, loss = 0.21656365\n",
      "Iteration 265, loss = 0.21635109\n",
      "Iteration 266, loss = 0.21675425\n",
      "Iteration 267, loss = 0.21631072\n",
      "Iteration 268, loss = 0.21570808\n",
      "Iteration 269, loss = 0.21562355\n",
      "Iteration 270, loss = 0.21567630\n",
      "Iteration 271, loss = 0.21533565\n",
      "Iteration 272, loss = 0.21583714\n",
      "Iteration 273, loss = 0.21544885\n",
      "Iteration 274, loss = 0.21486281\n",
      "Iteration 275, loss = 0.21470489\n",
      "Iteration 276, loss = 0.21467558\n",
      "Iteration 277, loss = 0.21453022\n",
      "Iteration 278, loss = 0.21428909\n",
      "Iteration 279, loss = 0.21463976\n",
      "Iteration 280, loss = 0.21395709\n",
      "Iteration 281, loss = 0.21399625\n",
      "Iteration 282, loss = 0.21440335\n",
      "Iteration 283, loss = 0.21373680\n",
      "Iteration 284, loss = 0.21326993\n",
      "Iteration 285, loss = 0.21318623\n",
      "Iteration 286, loss = 0.21240223\n",
      "Iteration 287, loss = 0.21335963\n",
      "Iteration 288, loss = 0.21320429\n",
      "Iteration 289, loss = 0.21260337\n",
      "Iteration 290, loss = 0.21212561\n",
      "Iteration 291, loss = 0.21215566\n",
      "Iteration 292, loss = 0.21194784\n",
      "Iteration 293, loss = 0.21198554\n",
      "Iteration 294, loss = 0.21119802\n",
      "Iteration 295, loss = 0.21132458\n",
      "Iteration 296, loss = 0.21204799\n",
      "Iteration 297, loss = 0.21078726\n",
      "Iteration 298, loss = 0.21094213\n",
      "Iteration 299, loss = 0.21059434\n",
      "Iteration 300, loss = 0.21020224\n",
      "Iteration 301, loss = 0.21019695\n",
      "Iteration 302, loss = 0.20963617\n",
      "Iteration 303, loss = 0.20937434\n",
      "Iteration 304, loss = 0.21070421\n",
      "Iteration 305, loss = 0.20991174\n",
      "Iteration 306, loss = 0.20850703\n",
      "Iteration 307, loss = 0.20823214\n",
      "Iteration 308, loss = 0.20780890\n",
      "Iteration 309, loss = 0.20782082\n",
      "Iteration 310, loss = 0.20747006\n",
      "Iteration 311, loss = 0.20723100\n",
      "Iteration 312, loss = 0.20684327\n",
      "Iteration 313, loss = 0.20676803\n",
      "Iteration 314, loss = 0.20648862\n",
      "Iteration 315, loss = 0.20663115\n",
      "Iteration 316, loss = 0.20561710\n",
      "Iteration 317, loss = 0.20512607\n",
      "Iteration 318, loss = 0.20477371\n",
      "Iteration 319, loss = 0.20433384\n",
      "Iteration 320, loss = 0.20441776\n",
      "Iteration 321, loss = 0.20442138\n",
      "Iteration 322, loss = 0.20395077\n",
      "Iteration 323, loss = 0.20339423\n",
      "Iteration 324, loss = 0.20341680\n",
      "Iteration 325, loss = 0.20336667\n",
      "Iteration 326, loss = 0.20301012\n",
      "Iteration 327, loss = 0.20265382\n",
      "Iteration 328, loss = 0.20215833\n",
      "Iteration 329, loss = 0.20222547\n",
      "Iteration 330, loss = 0.20209381\n",
      "Iteration 331, loss = 0.20158521\n",
      "Iteration 332, loss = 0.20173152\n",
      "Iteration 333, loss = 0.20126919\n",
      "Iteration 334, loss = 0.20128122\n",
      "Iteration 335, loss = 0.20078676\n",
      "Iteration 336, loss = 0.20082034\n",
      "Iteration 337, loss = 0.20058902\n",
      "Iteration 338, loss = 0.20043884\n",
      "Iteration 339, loss = 0.20010044\n",
      "Iteration 340, loss = 0.20043900\n",
      "Iteration 341, loss = 0.19951602\n",
      "Iteration 342, loss = 0.19939498\n",
      "Iteration 343, loss = 0.19978427\n",
      "Iteration 344, loss = 0.19983870\n",
      "Iteration 345, loss = 0.19920664\n",
      "Iteration 346, loss = 0.19877870\n",
      "Iteration 347, loss = 0.19850790\n",
      "Iteration 348, loss = 0.19860393\n",
      "Iteration 349, loss = 0.19816224\n",
      "Iteration 350, loss = 0.19826803\n",
      "Iteration 351, loss = 0.19795299\n",
      "Iteration 352, loss = 0.19764283\n",
      "Iteration 353, loss = 0.19776886\n",
      "Iteration 354, loss = 0.19840528\n",
      "Iteration 355, loss = 0.19810253\n",
      "Iteration 356, loss = 0.19714153\n",
      "Iteration 357, loss = 0.19697419\n",
      "Iteration 358, loss = 0.19647777\n",
      "Iteration 359, loss = 0.19647703\n",
      "Iteration 360, loss = 0.19630255\n",
      "Iteration 361, loss = 0.19624681\n",
      "Iteration 362, loss = 0.19587589\n",
      "Iteration 363, loss = 0.19602011\n",
      "Iteration 364, loss = 0.19590887\n",
      "Iteration 365, loss = 0.19553096\n",
      "Iteration 366, loss = 0.19548772\n",
      "Iteration 367, loss = 0.19552962\n",
      "Iteration 368, loss = 0.19509033\n",
      "Iteration 369, loss = 0.19478082\n",
      "Iteration 370, loss = 0.19528005\n",
      "Iteration 371, loss = 0.19492484\n",
      "Iteration 372, loss = 0.19471538\n",
      "Iteration 373, loss = 0.19474351\n",
      "Iteration 374, loss = 0.19426360\n",
      "Iteration 375, loss = 0.19500620\n",
      "Iteration 376, loss = 0.19541291\n",
      "Iteration 377, loss = 0.19426902\n",
      "Iteration 378, loss = 0.19404037\n",
      "Iteration 379, loss = 0.19416834\n",
      "Iteration 380, loss = 0.19383363\n",
      "Iteration 381, loss = 0.19383450\n",
      "Iteration 382, loss = 0.19357706\n",
      "Iteration 383, loss = 0.19322850\n",
      "Iteration 384, loss = 0.19336997\n",
      "Iteration 385, loss = 0.19357697\n",
      "Iteration 386, loss = 0.19368006\n",
      "Iteration 387, loss = 0.19340511\n",
      "Iteration 388, loss = 0.19312486\n",
      "Iteration 389, loss = 0.19275736\n",
      "Iteration 390, loss = 0.19277977\n",
      "Iteration 391, loss = 0.19249110\n",
      "Iteration 392, loss = 0.19298386\n",
      "Iteration 393, loss = 0.19214755\n",
      "Iteration 394, loss = 0.19276420\n",
      "Iteration 395, loss = 0.19264019\n",
      "Iteration 396, loss = 0.19194486\n",
      "Iteration 397, loss = 0.19195811\n",
      "Iteration 398, loss = 0.19205543\n",
      "Iteration 399, loss = 0.19157757\n",
      "Iteration 400, loss = 0.19166100\n",
      "Iteration 401, loss = 0.19219700\n",
      "Iteration 402, loss = 0.19170144\n",
      "Iteration 403, loss = 0.19158761\n",
      "Iteration 404, loss = 0.19108208\n",
      "Iteration 405, loss = 0.19153038\n",
      "Iteration 406, loss = 0.19141694\n",
      "Iteration 407, loss = 0.19086374\n",
      "Iteration 408, loss = 0.19180141\n",
      "Iteration 409, loss = 0.19082835\n",
      "Iteration 410, loss = 0.19071387\n",
      "Iteration 411, loss = 0.19090005\n",
      "Iteration 412, loss = 0.19049192\n",
      "Iteration 413, loss = 0.19029389\n",
      "Iteration 414, loss = 0.19075223\n",
      "Iteration 415, loss = 0.19044040\n",
      "Iteration 416, loss = 0.19031038\n",
      "Iteration 417, loss = 0.19028457\n",
      "Iteration 418, loss = 0.18993210\n",
      "Iteration 419, loss = 0.19019010\n",
      "Iteration 420, loss = 0.18989646\n",
      "Iteration 421, loss = 0.18944513\n",
      "Iteration 422, loss = 0.18938019\n",
      "Iteration 423, loss = 0.18953557\n",
      "Iteration 424, loss = 0.18919892\n",
      "Iteration 425, loss = 0.19035423\n",
      "Iteration 426, loss = 0.18932315\n",
      "Iteration 427, loss = 0.18918507\n",
      "Iteration 428, loss = 0.18887655\n",
      "Iteration 429, loss = 0.18930289\n",
      "Iteration 430, loss = 0.18868536\n",
      "Iteration 431, loss = 0.18931268\n",
      "Iteration 432, loss = 0.18866559\n",
      "Iteration 433, loss = 0.18865296\n",
      "Iteration 434, loss = 0.18893358\n",
      "Iteration 435, loss = 0.18896571\n",
      "Iteration 436, loss = 0.18881186\n",
      "Iteration 437, loss = 0.18912784\n",
      "Iteration 438, loss = 0.18823250\n",
      "Iteration 439, loss = 0.18795948\n",
      "Iteration 440, loss = 0.18851782\n",
      "Iteration 441, loss = 0.18816953\n",
      "Iteration 442, loss = 0.18802184\n",
      "Iteration 443, loss = 0.18841915\n",
      "Iteration 444, loss = 0.18855491\n",
      "Iteration 445, loss = 0.18779864\n",
      "Iteration 446, loss = 0.18870378\n",
      "Iteration 447, loss = 0.18821082\n",
      "Iteration 448, loss = 0.18826452\n",
      "Iteration 449, loss = 0.18806228\n",
      "Iteration 450, loss = 0.18824527\n",
      "Iteration 451, loss = 0.18786134\n",
      "Iteration 452, loss = 0.18783216\n",
      "Iteration 453, loss = 0.18762619\n",
      "Iteration 454, loss = 0.18739450\n",
      "Iteration 455, loss = 0.18734698\n",
      "Iteration 456, loss = 0.18791472\n",
      "Iteration 457, loss = 0.18715859\n",
      "Iteration 458, loss = 0.18740734\n",
      "Iteration 459, loss = 0.18678689\n",
      "Iteration 460, loss = 0.18685650\n",
      "Iteration 461, loss = 0.18667316\n",
      "Iteration 462, loss = 0.18729776\n",
      "Iteration 463, loss = 0.18685213\n",
      "Iteration 464, loss = 0.18738771\n",
      "Iteration 465, loss = 0.18649283\n",
      "Iteration 466, loss = 0.18624848\n",
      "Iteration 467, loss = 0.18631491\n",
      "Iteration 468, loss = 0.18695074\n",
      "Iteration 469, loss = 0.18625568\n",
      "Iteration 470, loss = 0.18640952\n",
      "Iteration 471, loss = 0.18634832\n",
      "Iteration 472, loss = 0.18602814\n",
      "Iteration 473, loss = 0.18643168\n",
      "Iteration 474, loss = 0.18601414\n",
      "Iteration 475, loss = 0.18623411\n",
      "Iteration 476, loss = 0.18544494\n",
      "Iteration 477, loss = 0.18582231\n",
      "Iteration 478, loss = 0.18666289\n",
      "Iteration 479, loss = 0.18569645\n",
      "Iteration 480, loss = 0.18564194\n",
      "Iteration 481, loss = 0.18543279\n",
      "Iteration 482, loss = 0.18562338\n",
      "Iteration 483, loss = 0.18597593\n",
      "Iteration 484, loss = 0.18545048\n",
      "Iteration 485, loss = 0.18507557\n",
      "Iteration 486, loss = 0.18687327\n",
      "Iteration 487, loss = 0.18635798\n",
      "Iteration 488, loss = 0.18524118\n",
      "Iteration 489, loss = 0.18594458\n",
      "Iteration 490, loss = 0.18536889\n",
      "Iteration 491, loss = 0.18486273\n",
      "Iteration 492, loss = 0.18547129\n",
      "Iteration 493, loss = 0.18617465\n",
      "Iteration 494, loss = 0.18594954\n",
      "Iteration 495, loss = 0.18427452\n",
      "Iteration 496, loss = 0.18569623\n",
      "Iteration 497, loss = 0.18501720\n",
      "Iteration 498, loss = 0.18475023\n",
      "Iteration 499, loss = 0.18484278\n",
      "Iteration 500, loss = 0.18473379\n",
      "Iteration 501, loss = 0.18451697\n",
      "Iteration 502, loss = 0.18462074\n",
      "Iteration 503, loss = 0.18467621\n",
      "Iteration 504, loss = 0.18432426\n",
      "Iteration 505, loss = 0.18468470\n",
      "Iteration 506, loss = 0.18435703\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76811906\n",
      "Iteration 2, loss = 0.74266985\n",
      "Iteration 3, loss = 0.72112860\n",
      "Iteration 4, loss = 0.70402016\n",
      "Iteration 5, loss = 0.69004796\n",
      "Iteration 6, loss = 0.67647457\n",
      "Iteration 7, loss = 0.66207790\n",
      "Iteration 8, loss = 0.64615109\n",
      "Iteration 9, loss = 0.62732872\n",
      "Iteration 10, loss = 0.60800722\n",
      "Iteration 11, loss = 0.58857675\n",
      "Iteration 12, loss = 0.56990086\n",
      "Iteration 13, loss = 0.55247633\n",
      "Iteration 14, loss = 0.53831455\n",
      "Iteration 15, loss = 0.52535377\n",
      "Iteration 16, loss = 0.51471039\n",
      "Iteration 17, loss = 0.50517715\n",
      "Iteration 18, loss = 0.49712883\n",
      "Iteration 19, loss = 0.48993137\n",
      "Iteration 20, loss = 0.48330957\n",
      "Iteration 21, loss = 0.47699125\n",
      "Iteration 22, loss = 0.47115260\n",
      "Iteration 23, loss = 0.46554599\n",
      "Iteration 24, loss = 0.46043406\n",
      "Iteration 25, loss = 0.45574070\n",
      "Iteration 26, loss = 0.45110696\n",
      "Iteration 27, loss = 0.44688996\n",
      "Iteration 28, loss = 0.44270111\n",
      "Iteration 29, loss = 0.43873982\n",
      "Iteration 30, loss = 0.43497063\n",
      "Iteration 31, loss = 0.43139583\n",
      "Iteration 32, loss = 0.42767321\n",
      "Iteration 33, loss = 0.42412130\n",
      "Iteration 34, loss = 0.42074652\n",
      "Iteration 35, loss = 0.41736622\n",
      "Iteration 36, loss = 0.41407307\n",
      "Iteration 37, loss = 0.41085049\n",
      "Iteration 38, loss = 0.40773861\n",
      "Iteration 39, loss = 0.40466168\n",
      "Iteration 40, loss = 0.40163658\n",
      "Iteration 41, loss = 0.39852996\n",
      "Iteration 42, loss = 0.39580868\n",
      "Iteration 43, loss = 0.39277509\n",
      "Iteration 44, loss = 0.39022294\n",
      "Iteration 45, loss = 0.38728763\n",
      "Iteration 46, loss = 0.38437493\n",
      "Iteration 47, loss = 0.38175533\n",
      "Iteration 48, loss = 0.37940754\n",
      "Iteration 49, loss = 0.37682290\n",
      "Iteration 50, loss = 0.37405729\n",
      "Iteration 51, loss = 0.37159410\n",
      "Iteration 52, loss = 0.36914762\n",
      "Iteration 53, loss = 0.36683240\n",
      "Iteration 54, loss = 0.36459901\n",
      "Iteration 55, loss = 0.36239040\n",
      "Iteration 56, loss = 0.36017191\n",
      "Iteration 57, loss = 0.35772135\n",
      "Iteration 58, loss = 0.35589718\n",
      "Iteration 59, loss = 0.35365532\n",
      "Iteration 60, loss = 0.35161831\n",
      "Iteration 61, loss = 0.34944976\n",
      "Iteration 62, loss = 0.34746721\n",
      "Iteration 63, loss = 0.34591712\n",
      "Iteration 64, loss = 0.34342257\n",
      "Iteration 65, loss = 0.34172882\n",
      "Iteration 66, loss = 0.33994992\n",
      "Iteration 67, loss = 0.33819432\n",
      "Iteration 68, loss = 0.33608825\n",
      "Iteration 69, loss = 0.33439811\n",
      "Iteration 70, loss = 0.33279407\n",
      "Iteration 71, loss = 0.33123039\n",
      "Iteration 72, loss = 0.32929587\n",
      "Iteration 73, loss = 0.32765436\n",
      "Iteration 74, loss = 0.32583297\n",
      "Iteration 75, loss = 0.32425827\n",
      "Iteration 76, loss = 0.32279379\n",
      "Iteration 77, loss = 0.32123462\n",
      "Iteration 78, loss = 0.31969582\n",
      "Iteration 79, loss = 0.31834620\n",
      "Iteration 80, loss = 0.31655948\n",
      "Iteration 81, loss = 0.31552221\n",
      "Iteration 82, loss = 0.31387878\n",
      "Iteration 83, loss = 0.31233944\n",
      "Iteration 84, loss = 0.31107189\n",
      "Iteration 85, loss = 0.30959195\n",
      "Iteration 86, loss = 0.30826245\n",
      "Iteration 87, loss = 0.30680753\n",
      "Iteration 88, loss = 0.30553609\n",
      "Iteration 89, loss = 0.30419482\n",
      "Iteration 90, loss = 0.30307746\n",
      "Iteration 91, loss = 0.30179273\n",
      "Iteration 92, loss = 0.30033594\n",
      "Iteration 93, loss = 0.29905499\n",
      "Iteration 94, loss = 0.29787867\n",
      "Iteration 95, loss = 0.29659756\n",
      "Iteration 96, loss = 0.29547591\n",
      "Iteration 97, loss = 0.29444457\n",
      "Iteration 98, loss = 0.29338975\n",
      "Iteration 99, loss = 0.29181972\n",
      "Iteration 100, loss = 0.29084849\n",
      "Iteration 101, loss = 0.28979745\n",
      "Iteration 102, loss = 0.28857172\n",
      "Iteration 103, loss = 0.28762428\n",
      "Iteration 104, loss = 0.28644109\n",
      "Iteration 105, loss = 0.28533161\n",
      "Iteration 106, loss = 0.28426719\n",
      "Iteration 107, loss = 0.28311865\n",
      "Iteration 108, loss = 0.28231469\n",
      "Iteration 109, loss = 0.28142365\n",
      "Iteration 110, loss = 0.28032602\n",
      "Iteration 111, loss = 0.27918993\n",
      "Iteration 112, loss = 0.27836298\n",
      "Iteration 113, loss = 0.27773580\n",
      "Iteration 114, loss = 0.27645066\n",
      "Iteration 115, loss = 0.27599114\n",
      "Iteration 116, loss = 0.27492222\n",
      "Iteration 117, loss = 0.27403850\n",
      "Iteration 118, loss = 0.27305883\n",
      "Iteration 119, loss = 0.27233689\n",
      "Iteration 120, loss = 0.27141305\n",
      "Iteration 121, loss = 0.27033411\n",
      "Iteration 122, loss = 0.26976350\n",
      "Iteration 123, loss = 0.26868624\n",
      "Iteration 124, loss = 0.26785964\n",
      "Iteration 125, loss = 0.26728670\n",
      "Iteration 126, loss = 0.26626686\n",
      "Iteration 127, loss = 0.26574410\n",
      "Iteration 128, loss = 0.26459382\n",
      "Iteration 129, loss = 0.26397488\n",
      "Iteration 130, loss = 0.26321074\n",
      "Iteration 131, loss = 0.26245449\n",
      "Iteration 132, loss = 0.26164523\n",
      "Iteration 133, loss = 0.26158141\n",
      "Iteration 134, loss = 0.26022981\n",
      "Iteration 135, loss = 0.25980906\n",
      "Iteration 136, loss = 0.25919627\n",
      "Iteration 137, loss = 0.25863591\n",
      "Iteration 138, loss = 0.25770010\n",
      "Iteration 139, loss = 0.25716411\n",
      "Iteration 140, loss = 0.25641426\n",
      "Iteration 141, loss = 0.25575075\n",
      "Iteration 142, loss = 0.25515962\n",
      "Iteration 143, loss = 0.25487520\n",
      "Iteration 144, loss = 0.25419339\n",
      "Iteration 145, loss = 0.25327065\n",
      "Iteration 146, loss = 0.25262036\n",
      "Iteration 147, loss = 0.25186032\n",
      "Iteration 148, loss = 0.25131310\n",
      "Iteration 149, loss = 0.25058268\n",
      "Iteration 150, loss = 0.24988287\n",
      "Iteration 151, loss = 0.24967798\n",
      "Iteration 152, loss = 0.24908473\n",
      "Iteration 153, loss = 0.24962384\n",
      "Iteration 154, loss = 0.24773932\n",
      "Iteration 155, loss = 0.24705796\n",
      "Iteration 156, loss = 0.24653621\n",
      "Iteration 157, loss = 0.24621825\n",
      "Iteration 158, loss = 0.24592359\n",
      "Iteration 159, loss = 0.24520040\n",
      "Iteration 160, loss = 0.24435513\n",
      "Iteration 161, loss = 0.24380095\n",
      "Iteration 162, loss = 0.24481731\n",
      "Iteration 163, loss = 0.24333195\n",
      "Iteration 164, loss = 0.24251776\n",
      "Iteration 165, loss = 0.24195941\n",
      "Iteration 166, loss = 0.24158740\n",
      "Iteration 167, loss = 0.24130585\n",
      "Iteration 168, loss = 0.24034457\n",
      "Iteration 169, loss = 0.23998839\n",
      "Iteration 170, loss = 0.23986071\n",
      "Iteration 171, loss = 0.23902288\n",
      "Iteration 172, loss = 0.23866707\n",
      "Iteration 173, loss = 0.23890638\n",
      "Iteration 174, loss = 0.23780615\n",
      "Iteration 175, loss = 0.23744249\n",
      "Iteration 176, loss = 0.23692872\n",
      "Iteration 177, loss = 0.23653854\n",
      "Iteration 178, loss = 0.23613939\n",
      "Iteration 179, loss = 0.23572628\n",
      "Iteration 180, loss = 0.23516126\n",
      "Iteration 181, loss = 0.23462601\n",
      "Iteration 182, loss = 0.23432476\n",
      "Iteration 183, loss = 0.23380617\n",
      "Iteration 184, loss = 0.23342597\n",
      "Iteration 185, loss = 0.23285420\n",
      "Iteration 186, loss = 0.23258549\n",
      "Iteration 187, loss = 0.23178177\n",
      "Iteration 188, loss = 0.23142631\n",
      "Iteration 189, loss = 0.23095367\n",
      "Iteration 190, loss = 0.23064731\n",
      "Iteration 191, loss = 0.23021416\n",
      "Iteration 192, loss = 0.23001210\n",
      "Iteration 193, loss = 0.22991784\n",
      "Iteration 194, loss = 0.22902704\n",
      "Iteration 195, loss = 0.22881722\n",
      "Iteration 196, loss = 0.22856364\n",
      "Iteration 197, loss = 0.22778374\n",
      "Iteration 198, loss = 0.22820419\n",
      "Iteration 199, loss = 0.22746555\n",
      "Iteration 200, loss = 0.22661206\n",
      "Iteration 201, loss = 0.22664257\n",
      "Iteration 202, loss = 0.22611808\n",
      "Iteration 203, loss = 0.22583062\n",
      "Iteration 204, loss = 0.22539726\n",
      "Iteration 205, loss = 0.22492175\n",
      "Iteration 206, loss = 0.22470616\n",
      "Iteration 207, loss = 0.22402021\n",
      "Iteration 208, loss = 0.22368456\n",
      "Iteration 209, loss = 0.22340739\n",
      "Iteration 210, loss = 0.22346740\n",
      "Iteration 211, loss = 0.22306244\n",
      "Iteration 212, loss = 0.22237396\n",
      "Iteration 213, loss = 0.22211311\n",
      "Iteration 214, loss = 0.22163423\n",
      "Iteration 215, loss = 0.22198583\n",
      "Iteration 216, loss = 0.22110518\n",
      "Iteration 217, loss = 0.22075339\n",
      "Iteration 218, loss = 0.22036302\n",
      "Iteration 219, loss = 0.21995506\n",
      "Iteration 220, loss = 0.22008709\n",
      "Iteration 221, loss = 0.21932627\n",
      "Iteration 222, loss = 0.21938693\n",
      "Iteration 223, loss = 0.21871218\n",
      "Iteration 224, loss = 0.21863358\n",
      "Iteration 225, loss = 0.21802147\n",
      "Iteration 226, loss = 0.21795226\n",
      "Iteration 227, loss = 0.21764571\n",
      "Iteration 228, loss = 0.21731819\n",
      "Iteration 229, loss = 0.21703697\n",
      "Iteration 230, loss = 0.21669682\n",
      "Iteration 231, loss = 0.21635314\n",
      "Iteration 232, loss = 0.21601979\n",
      "Iteration 233, loss = 0.21580222\n",
      "Iteration 234, loss = 0.21545319\n",
      "Iteration 235, loss = 0.21518176\n",
      "Iteration 236, loss = 0.21495222\n",
      "Iteration 237, loss = 0.21504483\n",
      "Iteration 238, loss = 0.21412923\n",
      "Iteration 239, loss = 0.21428683\n",
      "Iteration 240, loss = 0.21353231\n",
      "Iteration 241, loss = 0.21300867\n",
      "Iteration 242, loss = 0.21282084\n",
      "Iteration 243, loss = 0.21282853\n",
      "Iteration 244, loss = 0.21228361\n",
      "Iteration 245, loss = 0.21181772\n",
      "Iteration 246, loss = 0.21179509\n",
      "Iteration 247, loss = 0.21117346\n",
      "Iteration 248, loss = 0.21103145\n",
      "Iteration 249, loss = 0.21118285\n",
      "Iteration 250, loss = 0.21056466\n",
      "Iteration 251, loss = 0.21027947\n",
      "Iteration 252, loss = 0.21025798\n",
      "Iteration 253, loss = 0.21011934\n",
      "Iteration 254, loss = 0.20971933\n",
      "Iteration 255, loss = 0.20919226\n",
      "Iteration 256, loss = 0.20881396\n",
      "Iteration 257, loss = 0.20887516\n",
      "Iteration 258, loss = 0.20833942\n",
      "Iteration 259, loss = 0.20841497\n",
      "Iteration 260, loss = 0.20801150\n",
      "Iteration 261, loss = 0.20744551\n",
      "Iteration 262, loss = 0.20774021\n",
      "Iteration 263, loss = 0.20705256\n",
      "Iteration 264, loss = 0.20703941\n",
      "Iteration 265, loss = 0.20737339\n",
      "Iteration 266, loss = 0.20614332\n",
      "Iteration 267, loss = 0.20638110\n",
      "Iteration 268, loss = 0.20568445\n",
      "Iteration 269, loss = 0.20545203\n",
      "Iteration 270, loss = 0.20527977\n",
      "Iteration 271, loss = 0.20497889\n",
      "Iteration 272, loss = 0.20540482\n",
      "Iteration 273, loss = 0.20401099\n",
      "Iteration 274, loss = 0.20403435\n",
      "Iteration 275, loss = 0.20385509\n",
      "Iteration 276, loss = 0.20329554\n",
      "Iteration 277, loss = 0.20348781\n",
      "Iteration 278, loss = 0.20345098\n",
      "Iteration 279, loss = 0.20303260\n",
      "Iteration 280, loss = 0.20281091\n",
      "Iteration 281, loss = 0.20317729\n",
      "Iteration 282, loss = 0.20240454\n",
      "Iteration 283, loss = 0.20192533\n",
      "Iteration 284, loss = 0.20163996\n",
      "Iteration 285, loss = 0.20161770\n",
      "Iteration 286, loss = 0.20140022\n",
      "Iteration 287, loss = 0.20092333\n",
      "Iteration 288, loss = 0.20074484\n",
      "Iteration 289, loss = 0.20072293\n",
      "Iteration 290, loss = 0.20079054\n",
      "Iteration 291, loss = 0.20047150\n",
      "Iteration 292, loss = 0.19993353\n",
      "Iteration 293, loss = 0.19987771\n",
      "Iteration 294, loss = 0.19929216\n",
      "Iteration 295, loss = 0.19945352\n",
      "Iteration 296, loss = 0.19950819\n",
      "Iteration 297, loss = 0.19901334\n",
      "Iteration 298, loss = 0.19860914\n",
      "Iteration 299, loss = 0.19867657\n",
      "Iteration 300, loss = 0.19855944\n",
      "Iteration 301, loss = 0.19802014\n",
      "Iteration 302, loss = 0.19781170\n",
      "Iteration 303, loss = 0.19750389\n",
      "Iteration 304, loss = 0.19749336\n",
      "Iteration 305, loss = 0.19723704\n",
      "Iteration 306, loss = 0.19704526\n",
      "Iteration 307, loss = 0.19729324\n",
      "Iteration 308, loss = 0.19673077\n",
      "Iteration 309, loss = 0.19666748\n",
      "Iteration 310, loss = 0.19655849\n",
      "Iteration 311, loss = 0.19631930\n",
      "Iteration 312, loss = 0.19609185\n",
      "Iteration 313, loss = 0.19569067\n",
      "Iteration 314, loss = 0.19568738\n",
      "Iteration 315, loss = 0.19589659\n",
      "Iteration 316, loss = 0.19528383\n",
      "Iteration 317, loss = 0.19554036\n",
      "Iteration 318, loss = 0.19507195\n",
      "Iteration 319, loss = 0.19488239\n",
      "Iteration 320, loss = 0.19471898\n",
      "Iteration 321, loss = 0.19499880\n",
      "Iteration 322, loss = 0.19470298\n",
      "Iteration 323, loss = 0.19489999\n",
      "Iteration 324, loss = 0.19430700\n",
      "Iteration 325, loss = 0.19423690\n",
      "Iteration 326, loss = 0.19383660\n",
      "Iteration 327, loss = 0.19366124\n",
      "Iteration 328, loss = 0.19396637\n",
      "Iteration 329, loss = 0.19325411\n",
      "Iteration 330, loss = 0.19353617\n",
      "Iteration 331, loss = 0.19309626\n",
      "Iteration 332, loss = 0.19314032\n",
      "Iteration 333, loss = 0.19259725\n",
      "Iteration 334, loss = 0.19314917\n",
      "Iteration 335, loss = 0.19218910\n",
      "Iteration 336, loss = 0.19235368\n",
      "Iteration 337, loss = 0.19219990\n",
      "Iteration 338, loss = 0.19231327\n",
      "Iteration 339, loss = 0.19184844\n",
      "Iteration 340, loss = 0.19130842\n",
      "Iteration 341, loss = 0.19129229\n",
      "Iteration 342, loss = 0.19095240\n",
      "Iteration 343, loss = 0.19105702\n",
      "Iteration 344, loss = 0.19112282\n",
      "Iteration 345, loss = 0.19040261\n",
      "Iteration 346, loss = 0.18994242\n",
      "Iteration 347, loss = 0.19036590\n",
      "Iteration 348, loss = 0.19001785\n",
      "Iteration 349, loss = 0.18930318\n",
      "Iteration 350, loss = 0.18886169\n",
      "Iteration 351, loss = 0.18975109\n",
      "Iteration 352, loss = 0.18897631\n",
      "Iteration 353, loss = 0.18902913\n",
      "Iteration 354, loss = 0.18825860\n",
      "Iteration 355, loss = 0.18806409\n",
      "Iteration 356, loss = 0.18833785\n",
      "Iteration 357, loss = 0.18829813\n",
      "Iteration 358, loss = 0.18770251\n",
      "Iteration 359, loss = 0.18753572\n",
      "Iteration 360, loss = 0.18727416\n",
      "Iteration 361, loss = 0.18727250\n",
      "Iteration 362, loss = 0.18682090\n",
      "Iteration 363, loss = 0.18661128\n",
      "Iteration 364, loss = 0.18675791\n",
      "Iteration 365, loss = 0.18597514\n",
      "Iteration 366, loss = 0.18589846\n",
      "Iteration 367, loss = 0.18630042\n",
      "Iteration 368, loss = 0.18578521\n",
      "Iteration 369, loss = 0.18545952\n",
      "Iteration 370, loss = 0.18528071\n",
      "Iteration 371, loss = 0.18510856\n",
      "Iteration 372, loss = 0.18500811\n",
      "Iteration 373, loss = 0.18474865\n",
      "Iteration 374, loss = 0.18469813\n",
      "Iteration 375, loss = 0.18442120\n",
      "Iteration 376, loss = 0.18395387\n",
      "Iteration 377, loss = 0.18393527\n",
      "Iteration 378, loss = 0.18379691\n",
      "Iteration 379, loss = 0.18383036\n",
      "Iteration 380, loss = 0.18377401\n",
      "Iteration 381, loss = 0.18360982\n",
      "Iteration 382, loss = 0.18406687\n",
      "Iteration 383, loss = 0.18298293\n",
      "Iteration 384, loss = 0.18276985\n",
      "Iteration 385, loss = 0.18300755\n",
      "Iteration 386, loss = 0.18293112\n",
      "Iteration 387, loss = 0.18295547\n",
      "Iteration 388, loss = 0.18241533\n",
      "Iteration 389, loss = 0.18178913\n",
      "Iteration 390, loss = 0.18260490\n",
      "Iteration 391, loss = 0.18213398\n",
      "Iteration 392, loss = 0.18135406\n",
      "Iteration 393, loss = 0.18171989\n",
      "Iteration 394, loss = 0.18116743\n",
      "Iteration 395, loss = 0.18118776\n",
      "Iteration 396, loss = 0.18160434\n",
      "Iteration 397, loss = 0.18094476\n",
      "Iteration 398, loss = 0.18172662\n",
      "Iteration 399, loss = 0.18076056\n",
      "Iteration 400, loss = 0.18050704\n",
      "Iteration 401, loss = 0.18128356\n",
      "Iteration 402, loss = 0.18039662\n",
      "Iteration 403, loss = 0.18096830\n",
      "Iteration 404, loss = 0.17963300\n",
      "Iteration 405, loss = 0.17979562\n",
      "Iteration 406, loss = 0.18030581\n",
      "Iteration 407, loss = 0.17932119\n",
      "Iteration 408, loss = 0.17972973\n",
      "Iteration 409, loss = 0.17990497\n",
      "Iteration 410, loss = 0.17903254\n",
      "Iteration 411, loss = 0.17911072\n",
      "Iteration 412, loss = 0.17891195\n",
      "Iteration 413, loss = 0.17860022\n",
      "Iteration 414, loss = 0.17847252\n",
      "Iteration 415, loss = 0.17822189\n",
      "Iteration 416, loss = 0.17844858\n",
      "Iteration 417, loss = 0.17891052\n",
      "Iteration 418, loss = 0.17986260\n",
      "Iteration 419, loss = 0.17858766\n",
      "Iteration 420, loss = 0.17876379\n",
      "Iteration 421, loss = 0.17771561\n",
      "Iteration 422, loss = 0.17791936\n",
      "Iteration 423, loss = 0.17775016\n",
      "Iteration 424, loss = 0.17776779\n",
      "Iteration 425, loss = 0.17731123\n",
      "Iteration 426, loss = 0.17786502\n",
      "Iteration 427, loss = 0.17705654\n",
      "Iteration 428, loss = 0.17750634\n",
      "Iteration 429, loss = 0.17805599\n",
      "Iteration 430, loss = 0.17736138\n",
      "Iteration 431, loss = 0.17767781\n",
      "Iteration 432, loss = 0.17717191\n",
      "Iteration 433, loss = 0.17704949\n",
      "Iteration 434, loss = 0.17730488\n",
      "Iteration 435, loss = 0.17721226\n",
      "Iteration 436, loss = 0.17626184\n",
      "Iteration 437, loss = 0.17672234\n",
      "Iteration 438, loss = 0.17631314\n",
      "Iteration 439, loss = 0.17603085\n",
      "Iteration 440, loss = 0.17834024\n",
      "Iteration 441, loss = 0.17649607\n",
      "Iteration 442, loss = 0.17590971\n",
      "Iteration 443, loss = 0.17575985\n",
      "Iteration 444, loss = 0.17580469\n",
      "Iteration 445, loss = 0.17532884\n",
      "Iteration 446, loss = 0.17530634\n",
      "Iteration 447, loss = 0.17554541\n",
      "Iteration 448, loss = 0.17583346\n",
      "Iteration 449, loss = 0.17615962\n",
      "Iteration 450, loss = 0.17553928\n",
      "Iteration 451, loss = 0.17533194\n",
      "Iteration 452, loss = 0.17514533\n",
      "Iteration 453, loss = 0.17470899\n",
      "Iteration 454, loss = 0.17499726\n",
      "Iteration 455, loss = 0.17540137\n",
      "Iteration 456, loss = 0.17420670\n",
      "Iteration 457, loss = 0.17459282\n",
      "Iteration 458, loss = 0.17487785\n",
      "Iteration 459, loss = 0.17455174\n",
      "Iteration 460, loss = 0.17440000\n",
      "Iteration 461, loss = 0.17426602\n",
      "Iteration 462, loss = 0.17448126\n",
      "Iteration 463, loss = 0.17406963\n",
      "Iteration 464, loss = 0.17417295\n",
      "Iteration 465, loss = 0.17380547\n",
      "Iteration 466, loss = 0.17404606\n",
      "Iteration 467, loss = 0.17444073\n",
      "Iteration 468, loss = 0.17457699\n",
      "Iteration 469, loss = 0.17311140\n",
      "Iteration 470, loss = 0.17385121\n",
      "Iteration 471, loss = 0.17387295\n",
      "Iteration 472, loss = 0.17400682\n",
      "Iteration 473, loss = 0.17294775\n",
      "Iteration 474, loss = 0.17368435\n",
      "Iteration 475, loss = 0.17368377\n",
      "Iteration 476, loss = 0.17391146\n",
      "Iteration 477, loss = 0.17309545\n",
      "Iteration 478, loss = 0.17278234\n",
      "Iteration 479, loss = 0.17321241\n",
      "Iteration 480, loss = 0.17298502\n",
      "Iteration 481, loss = 0.17299082\n",
      "Iteration 482, loss = 0.17303076\n",
      "Iteration 483, loss = 0.17253712\n",
      "Iteration 484, loss = 0.17287342\n",
      "Iteration 485, loss = 0.17297646\n",
      "Iteration 486, loss = 0.17228680\n",
      "Iteration 487, loss = 0.17269442\n",
      "Iteration 488, loss = 0.17251517\n",
      "Iteration 489, loss = 0.17214876\n",
      "Iteration 490, loss = 0.17211569\n",
      "Iteration 491, loss = 0.17178216\n",
      "Iteration 492, loss = 0.17208773\n",
      "Iteration 493, loss = 0.17217465\n",
      "Iteration 494, loss = 0.17201604\n",
      "Iteration 495, loss = 0.17153746\n",
      "Iteration 496, loss = 0.17167069\n",
      "Iteration 497, loss = 0.17162326\n",
      "Iteration 498, loss = 0.17163887\n",
      "Iteration 499, loss = 0.17135182\n",
      "Iteration 500, loss = 0.17121672\n",
      "Iteration 501, loss = 0.17101206\n",
      "Iteration 502, loss = 0.17127981\n",
      "Iteration 503, loss = 0.17133276\n",
      "Iteration 504, loss = 0.17097007\n",
      "Iteration 505, loss = 0.17119932\n",
      "Iteration 506, loss = 0.17039654\n",
      "Iteration 507, loss = 0.17033676\n",
      "Iteration 508, loss = 0.17027026\n",
      "Iteration 509, loss = 0.17023975\n",
      "Iteration 510, loss = 0.17058056\n",
      "Iteration 511, loss = 0.17022204\n",
      "Iteration 512, loss = 0.17013997\n",
      "Iteration 513, loss = 0.16976733\n",
      "Iteration 514, loss = 0.16936136\n",
      "Iteration 515, loss = 0.17002715\n",
      "Iteration 516, loss = 0.16935799\n",
      "Iteration 517, loss = 0.16843089\n",
      "Iteration 518, loss = 0.16880177\n",
      "Iteration 519, loss = 0.16780598\n",
      "Iteration 520, loss = 0.16757165\n",
      "Iteration 521, loss = 0.16750388\n",
      "Iteration 522, loss = 0.16704031\n",
      "Iteration 523, loss = 0.16737108\n",
      "Iteration 524, loss = 0.16706086\n",
      "Iteration 525, loss = 0.16759856\n",
      "Iteration 526, loss = 0.16635112\n",
      "Iteration 527, loss = 0.16675711\n",
      "Iteration 528, loss = 0.16672908\n",
      "Iteration 529, loss = 0.16697785\n",
      "Iteration 530, loss = 0.16632665\n",
      "Iteration 531, loss = 0.16628942\n",
      "Iteration 532, loss = 0.16652221\n",
      "Iteration 533, loss = 0.16608980\n",
      "Iteration 534, loss = 0.16582131\n",
      "Iteration 535, loss = 0.16601052\n",
      "Iteration 536, loss = 0.16524003\n",
      "Iteration 537, loss = 0.16587290\n",
      "Iteration 538, loss = 0.16568969\n",
      "Iteration 539, loss = 0.16550778\n",
      "Iteration 540, loss = 0.16594466\n",
      "Iteration 541, loss = 0.16541278\n",
      "Iteration 542, loss = 0.16494605\n",
      "Iteration 543, loss = 0.16522612\n",
      "Iteration 544, loss = 0.16503470\n",
      "Iteration 545, loss = 0.16510415\n",
      "Iteration 546, loss = 0.16569323\n",
      "Iteration 547, loss = 0.16469215\n",
      "Iteration 548, loss = 0.16482614\n",
      "Iteration 549, loss = 0.16492139\n",
      "Iteration 550, loss = 0.16499418\n",
      "Iteration 551, loss = 0.16512538\n",
      "Iteration 552, loss = 0.16545878\n",
      "Iteration 553, loss = 0.16494068\n",
      "Iteration 554, loss = 0.16451842\n",
      "Iteration 555, loss = 0.16450408\n",
      "Iteration 556, loss = 0.16455971\n",
      "Iteration 557, loss = 0.16427409\n",
      "Iteration 558, loss = 0.16404362\n",
      "Iteration 559, loss = 0.16407053\n",
      "Iteration 560, loss = 0.16419621\n",
      "Iteration 561, loss = 0.16412677\n",
      "Iteration 562, loss = 0.16443705\n",
      "Iteration 563, loss = 0.16441309\n",
      "Iteration 564, loss = 0.16422545\n",
      "Iteration 565, loss = 0.16374424\n",
      "Iteration 566, loss = 0.16358761\n",
      "Iteration 567, loss = 0.16414017\n",
      "Iteration 568, loss = 0.16392247\n",
      "Iteration 569, loss = 0.16327459\n",
      "Iteration 570, loss = 0.16360529\n",
      "Iteration 571, loss = 0.16495114\n",
      "Iteration 572, loss = 0.16361711\n",
      "Iteration 573, loss = 0.16394771\n",
      "Iteration 574, loss = 0.16348373\n",
      "Iteration 575, loss = 0.16352299\n",
      "Iteration 576, loss = 0.16308589\n",
      "Iteration 577, loss = 0.16348952\n",
      "Iteration 578, loss = 0.16255234\n",
      "Iteration 579, loss = 0.16393021\n",
      "Iteration 580, loss = 0.16257182\n",
      "Iteration 581, loss = 0.16311909\n",
      "Iteration 582, loss = 0.16293951\n",
      "Iteration 583, loss = 0.16288495\n",
      "Iteration 584, loss = 0.16371990\n",
      "Iteration 585, loss = 0.16340602\n",
      "Iteration 586, loss = 0.16360676\n",
      "Iteration 587, loss = 0.16301128\n",
      "Iteration 588, loss = 0.16274591\n",
      "Iteration 589, loss = 0.16253702\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77047403\n",
      "Iteration 2, loss = 0.74461227\n",
      "Iteration 3, loss = 0.72305982\n",
      "Iteration 4, loss = 0.70539556\n",
      "Iteration 5, loss = 0.69145016\n",
      "Iteration 6, loss = 0.67914040\n",
      "Iteration 7, loss = 0.66695969\n",
      "Iteration 8, loss = 0.65292506\n",
      "Iteration 9, loss = 0.63608546\n",
      "Iteration 10, loss = 0.61765946\n",
      "Iteration 11, loss = 0.59858531\n",
      "Iteration 12, loss = 0.57948673\n",
      "Iteration 13, loss = 0.56275359\n",
      "Iteration 14, loss = 0.54786874\n",
      "Iteration 15, loss = 0.53538840\n",
      "Iteration 16, loss = 0.52420217\n",
      "Iteration 17, loss = 0.51430733\n",
      "Iteration 18, loss = 0.50546516\n",
      "Iteration 19, loss = 0.49753278\n",
      "Iteration 20, loss = 0.49029938\n",
      "Iteration 21, loss = 0.48376220\n",
      "Iteration 22, loss = 0.47784504\n",
      "Iteration 23, loss = 0.47196142\n",
      "Iteration 24, loss = 0.46676479\n",
      "Iteration 25, loss = 0.46150223\n",
      "Iteration 26, loss = 0.45685529\n",
      "Iteration 27, loss = 0.45236766\n",
      "Iteration 28, loss = 0.44813062\n",
      "Iteration 29, loss = 0.44416686\n",
      "Iteration 30, loss = 0.44049863\n",
      "Iteration 31, loss = 0.43649700\n",
      "Iteration 32, loss = 0.43297788\n",
      "Iteration 33, loss = 0.42936109\n",
      "Iteration 34, loss = 0.42586977\n",
      "Iteration 35, loss = 0.42277905\n",
      "Iteration 36, loss = 0.41946478\n",
      "Iteration 37, loss = 0.41611037\n",
      "Iteration 38, loss = 0.41314712\n",
      "Iteration 39, loss = 0.40997553\n",
      "Iteration 40, loss = 0.40704400\n",
      "Iteration 41, loss = 0.40407019\n",
      "Iteration 42, loss = 0.40117213\n",
      "Iteration 43, loss = 0.39847070\n",
      "Iteration 44, loss = 0.39576321\n",
      "Iteration 45, loss = 0.39308492\n",
      "Iteration 46, loss = 0.39042891\n",
      "Iteration 47, loss = 0.38786850\n",
      "Iteration 48, loss = 0.38533331\n",
      "Iteration 49, loss = 0.38282161\n",
      "Iteration 50, loss = 0.38047860\n",
      "Iteration 51, loss = 0.37801072\n",
      "Iteration 52, loss = 0.37565717\n",
      "Iteration 53, loss = 0.37362690\n",
      "Iteration 54, loss = 0.37118219\n",
      "Iteration 55, loss = 0.36893053\n",
      "Iteration 56, loss = 0.36669756\n",
      "Iteration 57, loss = 0.36438006\n",
      "Iteration 58, loss = 0.36243551\n",
      "Iteration 59, loss = 0.36024576\n",
      "Iteration 60, loss = 0.35839952\n",
      "Iteration 61, loss = 0.35628613\n",
      "Iteration 62, loss = 0.35449144\n",
      "Iteration 63, loss = 0.35264410\n",
      "Iteration 64, loss = 0.35077652\n",
      "Iteration 65, loss = 0.34889049\n",
      "Iteration 66, loss = 0.34715880\n",
      "Iteration 67, loss = 0.34538832\n",
      "Iteration 68, loss = 0.34362232\n",
      "Iteration 69, loss = 0.34205243\n",
      "Iteration 70, loss = 0.34025659\n",
      "Iteration 71, loss = 0.33863959\n",
      "Iteration 72, loss = 0.33679424\n",
      "Iteration 73, loss = 0.33541385\n",
      "Iteration 74, loss = 0.33367517\n",
      "Iteration 75, loss = 0.33186902\n",
      "Iteration 76, loss = 0.33022793\n",
      "Iteration 77, loss = 0.32915448\n",
      "Iteration 78, loss = 0.32729794\n",
      "Iteration 79, loss = 0.32581968\n",
      "Iteration 80, loss = 0.32434796\n",
      "Iteration 81, loss = 0.32305463\n",
      "Iteration 82, loss = 0.32148169\n",
      "Iteration 83, loss = 0.32013471\n",
      "Iteration 84, loss = 0.31871802\n",
      "Iteration 85, loss = 0.31714034\n",
      "Iteration 86, loss = 0.31584577\n",
      "Iteration 87, loss = 0.31464248\n",
      "Iteration 88, loss = 0.31322330\n",
      "Iteration 89, loss = 0.31208420\n",
      "Iteration 90, loss = 0.31089536\n",
      "Iteration 91, loss = 0.30993109\n",
      "Iteration 92, loss = 0.30830392\n",
      "Iteration 93, loss = 0.30722420\n",
      "Iteration 94, loss = 0.30607150\n",
      "Iteration 95, loss = 0.30476211\n",
      "Iteration 96, loss = 0.30358576\n",
      "Iteration 97, loss = 0.30237229\n",
      "Iteration 98, loss = 0.30131329\n",
      "Iteration 99, loss = 0.30049678\n",
      "Iteration 100, loss = 0.29910423\n",
      "Iteration 101, loss = 0.29801504\n",
      "Iteration 102, loss = 0.29731389\n",
      "Iteration 103, loss = 0.29592718\n",
      "Iteration 104, loss = 0.29527595\n",
      "Iteration 105, loss = 0.29367256\n",
      "Iteration 106, loss = 0.29330233\n",
      "Iteration 107, loss = 0.29167611\n",
      "Iteration 108, loss = 0.29069267\n",
      "Iteration 109, loss = 0.28943999\n",
      "Iteration 110, loss = 0.28837830\n",
      "Iteration 111, loss = 0.28773461\n",
      "Iteration 112, loss = 0.28662183\n",
      "Iteration 113, loss = 0.28560682\n",
      "Iteration 114, loss = 0.28438198\n",
      "Iteration 115, loss = 0.28353128\n",
      "Iteration 116, loss = 0.28266707\n",
      "Iteration 117, loss = 0.28166542\n",
      "Iteration 118, loss = 0.28034195\n",
      "Iteration 119, loss = 0.27977602\n",
      "Iteration 120, loss = 0.27860994\n",
      "Iteration 121, loss = 0.27845713\n",
      "Iteration 122, loss = 0.27730068\n",
      "Iteration 123, loss = 0.27608364\n",
      "Iteration 124, loss = 0.27523705\n",
      "Iteration 125, loss = 0.27446237\n",
      "Iteration 126, loss = 0.27353836\n",
      "Iteration 127, loss = 0.27275781\n",
      "Iteration 128, loss = 0.27159652\n",
      "Iteration 129, loss = 0.27124689\n",
      "Iteration 130, loss = 0.27029992\n",
      "Iteration 131, loss = 0.26909880\n",
      "Iteration 132, loss = 0.26920644\n",
      "Iteration 133, loss = 0.26774312\n",
      "Iteration 134, loss = 0.26703466\n",
      "Iteration 135, loss = 0.26667817\n",
      "Iteration 136, loss = 0.26649585\n",
      "Iteration 137, loss = 0.26493258\n",
      "Iteration 138, loss = 0.26502982\n",
      "Iteration 139, loss = 0.26358473\n",
      "Iteration 140, loss = 0.26312489\n",
      "Iteration 141, loss = 0.26222802\n",
      "Iteration 142, loss = 0.26155967\n",
      "Iteration 143, loss = 0.26095822\n",
      "Iteration 144, loss = 0.26012927\n",
      "Iteration 145, loss = 0.25966566\n",
      "Iteration 146, loss = 0.25929242\n",
      "Iteration 147, loss = 0.25851866\n",
      "Iteration 148, loss = 0.25774741\n",
      "Iteration 149, loss = 0.25740280\n",
      "Iteration 150, loss = 0.25680585\n",
      "Iteration 151, loss = 0.25629265\n",
      "Iteration 152, loss = 0.25556225\n",
      "Iteration 153, loss = 0.25467839\n",
      "Iteration 154, loss = 0.25437996\n",
      "Iteration 155, loss = 0.25383063\n",
      "Iteration 156, loss = 0.25317681\n",
      "Iteration 157, loss = 0.25269263\n",
      "Iteration 158, loss = 0.25249790\n",
      "Iteration 159, loss = 0.25165509\n",
      "Iteration 160, loss = 0.25170579\n",
      "Iteration 161, loss = 0.25095395\n",
      "Iteration 162, loss = 0.25024650\n",
      "Iteration 163, loss = 0.24968292\n",
      "Iteration 164, loss = 0.24932843\n",
      "Iteration 165, loss = 0.24930387\n",
      "Iteration 166, loss = 0.24844868\n",
      "Iteration 167, loss = 0.24801558\n",
      "Iteration 168, loss = 0.24794413\n",
      "Iteration 169, loss = 0.24744979\n",
      "Iteration 170, loss = 0.24652793\n",
      "Iteration 171, loss = 0.24624202\n",
      "Iteration 172, loss = 0.24613991\n",
      "Iteration 173, loss = 0.24520774\n",
      "Iteration 174, loss = 0.24475405\n",
      "Iteration 175, loss = 0.24446197\n",
      "Iteration 176, loss = 0.24410931\n",
      "Iteration 177, loss = 0.24345639\n",
      "Iteration 178, loss = 0.24307327\n",
      "Iteration 179, loss = 0.24267279\n",
      "Iteration 180, loss = 0.24306801\n",
      "Iteration 181, loss = 0.24192232\n",
      "Iteration 182, loss = 0.24153136\n",
      "Iteration 183, loss = 0.24102556\n",
      "Iteration 184, loss = 0.24095696\n",
      "Iteration 185, loss = 0.24016589\n",
      "Iteration 186, loss = 0.23973428\n",
      "Iteration 187, loss = 0.23929953\n",
      "Iteration 188, loss = 0.23908269\n",
      "Iteration 189, loss = 0.23870845\n",
      "Iteration 190, loss = 0.23825452\n",
      "Iteration 191, loss = 0.23769955\n",
      "Iteration 192, loss = 0.23751263\n",
      "Iteration 193, loss = 0.23729618\n",
      "Iteration 194, loss = 0.23712916\n",
      "Iteration 195, loss = 0.23620857\n",
      "Iteration 196, loss = 0.23619103\n",
      "Iteration 197, loss = 0.23596460\n",
      "Iteration 198, loss = 0.23585055\n",
      "Iteration 199, loss = 0.23506614\n",
      "Iteration 200, loss = 0.23459181\n",
      "Iteration 201, loss = 0.23421043\n",
      "Iteration 202, loss = 0.23436389\n",
      "Iteration 203, loss = 0.23400394\n",
      "Iteration 204, loss = 0.23334177\n",
      "Iteration 205, loss = 0.23297697\n",
      "Iteration 206, loss = 0.23255954\n",
      "Iteration 207, loss = 0.23203362\n",
      "Iteration 208, loss = 0.23181777\n",
      "Iteration 209, loss = 0.23187520\n",
      "Iteration 210, loss = 0.23110443\n",
      "Iteration 211, loss = 0.23080269\n",
      "Iteration 212, loss = 0.23037177\n",
      "Iteration 213, loss = 0.22968246\n",
      "Iteration 214, loss = 0.22963578\n",
      "Iteration 215, loss = 0.22908776\n",
      "Iteration 216, loss = 0.22864737\n",
      "Iteration 217, loss = 0.22912088\n",
      "Iteration 218, loss = 0.22868797\n",
      "Iteration 219, loss = 0.22757551\n",
      "Iteration 220, loss = 0.22756175\n",
      "Iteration 221, loss = 0.22671711\n",
      "Iteration 222, loss = 0.22662649\n",
      "Iteration 223, loss = 0.22639102\n",
      "Iteration 224, loss = 0.22561964\n",
      "Iteration 225, loss = 0.22561763\n",
      "Iteration 226, loss = 0.22525951\n",
      "Iteration 227, loss = 0.22497948\n",
      "Iteration 228, loss = 0.22435087\n",
      "Iteration 229, loss = 0.22502235\n",
      "Iteration 230, loss = 0.22394894\n",
      "Iteration 231, loss = 0.22417225\n",
      "Iteration 232, loss = 0.22366567\n",
      "Iteration 233, loss = 0.22292770\n",
      "Iteration 234, loss = 0.22342423\n",
      "Iteration 235, loss = 0.22256831\n",
      "Iteration 236, loss = 0.22176132\n",
      "Iteration 237, loss = 0.22167490\n",
      "Iteration 238, loss = 0.22143166\n",
      "Iteration 239, loss = 0.22083012\n",
      "Iteration 240, loss = 0.22080592\n",
      "Iteration 241, loss = 0.22005629\n",
      "Iteration 242, loss = 0.22010385\n",
      "Iteration 243, loss = 0.21999686\n",
      "Iteration 244, loss = 0.21930165\n",
      "Iteration 245, loss = 0.21915713\n",
      "Iteration 246, loss = 0.21916419\n",
      "Iteration 247, loss = 0.21865118\n",
      "Iteration 248, loss = 0.21854576\n",
      "Iteration 249, loss = 0.21836830\n",
      "Iteration 250, loss = 0.21772337\n",
      "Iteration 251, loss = 0.21760437\n",
      "Iteration 252, loss = 0.21801879\n",
      "Iteration 253, loss = 0.21767119\n",
      "Iteration 254, loss = 0.21698446\n",
      "Iteration 255, loss = 0.21692755\n",
      "Iteration 256, loss = 0.21655760\n",
      "Iteration 257, loss = 0.21663566\n",
      "Iteration 258, loss = 0.21644877\n",
      "Iteration 259, loss = 0.21576737\n",
      "Iteration 260, loss = 0.21546549\n",
      "Iteration 261, loss = 0.21572281\n",
      "Iteration 262, loss = 0.21573980\n",
      "Iteration 263, loss = 0.21589456\n",
      "Iteration 264, loss = 0.21477012\n",
      "Iteration 265, loss = 0.21480846\n",
      "Iteration 266, loss = 0.21467711\n",
      "Iteration 267, loss = 0.21456185\n",
      "Iteration 268, loss = 0.21420109\n",
      "Iteration 269, loss = 0.21400327\n",
      "Iteration 270, loss = 0.21394630\n",
      "Iteration 271, loss = 0.21392754\n",
      "Iteration 272, loss = 0.21323723\n",
      "Iteration 273, loss = 0.21392136\n",
      "Iteration 274, loss = 0.21276679\n",
      "Iteration 275, loss = 0.21302544\n",
      "Iteration 276, loss = 0.21276095\n",
      "Iteration 277, loss = 0.21234370\n",
      "Iteration 278, loss = 0.21233933\n",
      "Iteration 279, loss = 0.21213949\n",
      "Iteration 280, loss = 0.21177193\n",
      "Iteration 281, loss = 0.21148774\n",
      "Iteration 282, loss = 0.21143903\n",
      "Iteration 283, loss = 0.21146313\n",
      "Iteration 284, loss = 0.21110088\n",
      "Iteration 285, loss = 0.21194648\n",
      "Iteration 286, loss = 0.21158767\n",
      "Iteration 287, loss = 0.21091128\n",
      "Iteration 288, loss = 0.21068766\n",
      "Iteration 289, loss = 0.21018050\n",
      "Iteration 290, loss = 0.21034791\n",
      "Iteration 291, loss = 0.21021712\n",
      "Iteration 292, loss = 0.21035210\n",
      "Iteration 293, loss = 0.21032043\n",
      "Iteration 294, loss = 0.21079168\n",
      "Iteration 295, loss = 0.20918121\n",
      "Iteration 296, loss = 0.20941448\n",
      "Iteration 297, loss = 0.20887931\n",
      "Iteration 298, loss = 0.20931564\n",
      "Iteration 299, loss = 0.20967604\n",
      "Iteration 300, loss = 0.20879621\n",
      "Iteration 301, loss = 0.20863398\n",
      "Iteration 302, loss = 0.20824307\n",
      "Iteration 303, loss = 0.20859402\n",
      "Iteration 304, loss = 0.20879885\n",
      "Iteration 305, loss = 0.20811612\n",
      "Iteration 306, loss = 0.20782709\n",
      "Iteration 307, loss = 0.20778975\n",
      "Iteration 308, loss = 0.20761618\n",
      "Iteration 309, loss = 0.20734363\n",
      "Iteration 310, loss = 0.20734510\n",
      "Iteration 311, loss = 0.20736327\n",
      "Iteration 312, loss = 0.20756328\n",
      "Iteration 313, loss = 0.20701263\n",
      "Iteration 314, loss = 0.20668159\n",
      "Iteration 315, loss = 0.20697095\n",
      "Iteration 316, loss = 0.20703278\n",
      "Iteration 317, loss = 0.20646744\n",
      "Iteration 318, loss = 0.20687936\n",
      "Iteration 319, loss = 0.20637235\n",
      "Iteration 320, loss = 0.20647253\n",
      "Iteration 321, loss = 0.20592953\n",
      "Iteration 322, loss = 0.20629674\n",
      "Iteration 323, loss = 0.20582762\n",
      "Iteration 324, loss = 0.20570344\n",
      "Iteration 325, loss = 0.20570126\n",
      "Iteration 326, loss = 0.20576710\n",
      "Iteration 327, loss = 0.20607706\n",
      "Iteration 328, loss = 0.20514554\n",
      "Iteration 329, loss = 0.20567985\n",
      "Iteration 330, loss = 0.20490681\n",
      "Iteration 331, loss = 0.20484787\n",
      "Iteration 332, loss = 0.20465141\n",
      "Iteration 333, loss = 0.20463215\n",
      "Iteration 334, loss = 0.20486928\n",
      "Iteration 335, loss = 0.20423837\n",
      "Iteration 336, loss = 0.20436447\n",
      "Iteration 337, loss = 0.20416957\n",
      "Iteration 338, loss = 0.20487132\n",
      "Iteration 339, loss = 0.20410443\n",
      "Iteration 340, loss = 0.20370698\n",
      "Iteration 341, loss = 0.20353193\n",
      "Iteration 342, loss = 0.20369616\n",
      "Iteration 343, loss = 0.20344417\n",
      "Iteration 344, loss = 0.20322942\n",
      "Iteration 345, loss = 0.20313593\n",
      "Iteration 346, loss = 0.20330318\n",
      "Iteration 347, loss = 0.20316426\n",
      "Iteration 348, loss = 0.20308453\n",
      "Iteration 349, loss = 0.20316681\n",
      "Iteration 350, loss = 0.20276046\n",
      "Iteration 351, loss = 0.20244770\n",
      "Iteration 352, loss = 0.20241291\n",
      "Iteration 353, loss = 0.20293045\n",
      "Iteration 354, loss = 0.20269673\n",
      "Iteration 355, loss = 0.20251127\n",
      "Iteration 356, loss = 0.20210471\n",
      "Iteration 357, loss = 0.20231447\n",
      "Iteration 358, loss = 0.20222929\n",
      "Iteration 359, loss = 0.20215788\n",
      "Iteration 360, loss = 0.20265301\n",
      "Iteration 361, loss = 0.20234761\n",
      "Iteration 362, loss = 0.20183304\n",
      "Iteration 363, loss = 0.20216534\n",
      "Iteration 364, loss = 0.20151280\n",
      "Iteration 365, loss = 0.20120496\n",
      "Iteration 366, loss = 0.20187920\n",
      "Iteration 367, loss = 0.20112612\n",
      "Iteration 368, loss = 0.20105827\n",
      "Iteration 369, loss = 0.20092866\n",
      "Iteration 370, loss = 0.20090318\n",
      "Iteration 371, loss = 0.20069222\n",
      "Iteration 372, loss = 0.20042417\n",
      "Iteration 373, loss = 0.20055393\n",
      "Iteration 374, loss = 0.20034898\n",
      "Iteration 375, loss = 0.20050170\n",
      "Iteration 376, loss = 0.20065671\n",
      "Iteration 377, loss = 0.20033840\n",
      "Iteration 378, loss = 0.19990489\n",
      "Iteration 379, loss = 0.20014322\n",
      "Iteration 380, loss = 0.20039927\n",
      "Iteration 381, loss = 0.19997942\n",
      "Iteration 382, loss = 0.19948400\n",
      "Iteration 383, loss = 0.19947533\n",
      "Iteration 384, loss = 0.19958413\n",
      "Iteration 385, loss = 0.19958766\n",
      "Iteration 386, loss = 0.19916782\n",
      "Iteration 387, loss = 0.19939173\n",
      "Iteration 388, loss = 0.19975599\n",
      "Iteration 389, loss = 0.19899619\n",
      "Iteration 390, loss = 0.19917875\n",
      "Iteration 391, loss = 0.19885964\n",
      "Iteration 392, loss = 0.19905382\n",
      "Iteration 393, loss = 0.19876765\n",
      "Iteration 394, loss = 0.19880592\n",
      "Iteration 395, loss = 0.19943587\n",
      "Iteration 396, loss = 0.19844910\n",
      "Iteration 397, loss = 0.19851398\n",
      "Iteration 398, loss = 0.19872849\n",
      "Iteration 399, loss = 0.19820594\n",
      "Iteration 400, loss = 0.19814024\n",
      "Iteration 401, loss = 0.19962887\n",
      "Iteration 402, loss = 0.19784194\n",
      "Iteration 403, loss = 0.19803576\n",
      "Iteration 404, loss = 0.19769078\n",
      "Iteration 405, loss = 0.19761948\n",
      "Iteration 406, loss = 0.19765381\n",
      "Iteration 407, loss = 0.19770436\n",
      "Iteration 408, loss = 0.19788003\n",
      "Iteration 409, loss = 0.19720749\n",
      "Iteration 410, loss = 0.19695918\n",
      "Iteration 411, loss = 0.19809789\n",
      "Iteration 412, loss = 0.19672431\n",
      "Iteration 413, loss = 0.19687067\n",
      "Iteration 414, loss = 0.19695750\n",
      "Iteration 415, loss = 0.19658826\n",
      "Iteration 416, loss = 0.19690965\n",
      "Iteration 417, loss = 0.19636934\n",
      "Iteration 418, loss = 0.19638118\n",
      "Iteration 419, loss = 0.19654587\n",
      "Iteration 420, loss = 0.19650119\n",
      "Iteration 421, loss = 0.19643498\n",
      "Iteration 422, loss = 0.19621073\n",
      "Iteration 423, loss = 0.19667936\n",
      "Iteration 424, loss = 0.19622736\n",
      "Iteration 425, loss = 0.19619036\n",
      "Iteration 426, loss = 0.19624118\n",
      "Iteration 427, loss = 0.19569206\n",
      "Iteration 428, loss = 0.19674706\n",
      "Iteration 429, loss = 0.19579163\n",
      "Iteration 430, loss = 0.19562278\n",
      "Iteration 431, loss = 0.19564025\n",
      "Iteration 432, loss = 0.19591407\n",
      "Iteration 433, loss = 0.19548113\n",
      "Iteration 434, loss = 0.19576196\n",
      "Iteration 435, loss = 0.19605248\n",
      "Iteration 436, loss = 0.19602121\n",
      "Iteration 437, loss = 0.19530151\n",
      "Iteration 438, loss = 0.19592917\n",
      "Iteration 439, loss = 0.19520356\n",
      "Iteration 440, loss = 0.19574211\n",
      "Iteration 441, loss = 0.19551224\n",
      "Iteration 442, loss = 0.19573651\n",
      "Iteration 443, loss = 0.19507750\n",
      "Iteration 444, loss = 0.19502937\n",
      "Iteration 445, loss = 0.19557887\n",
      "Iteration 446, loss = 0.19472216\n",
      "Iteration 447, loss = 0.19515245\n",
      "Iteration 448, loss = 0.19546458\n",
      "Iteration 449, loss = 0.19638694\n",
      "Iteration 450, loss = 0.19491066\n",
      "Iteration 451, loss = 0.19465999\n",
      "Iteration 452, loss = 0.19492490\n",
      "Iteration 453, loss = 0.19442323\n",
      "Iteration 454, loss = 0.19444154\n",
      "Iteration 455, loss = 0.19405958\n",
      "Iteration 456, loss = 0.19478563\n",
      "Iteration 457, loss = 0.19474787\n",
      "Iteration 458, loss = 0.19407453\n",
      "Iteration 459, loss = 0.19444982\n",
      "Iteration 460, loss = 0.19431359\n",
      "Iteration 461, loss = 0.19406139\n",
      "Iteration 462, loss = 0.19377436\n",
      "Iteration 463, loss = 0.19389074\n",
      "Iteration 464, loss = 0.19433750\n",
      "Iteration 465, loss = 0.19376350\n",
      "Iteration 466, loss = 0.19390712\n",
      "Iteration 467, loss = 0.19338120\n",
      "Iteration 468, loss = 0.19375110\n",
      "Iteration 469, loss = 0.19395509\n",
      "Iteration 470, loss = 0.19357360\n",
      "Iteration 471, loss = 0.19356904\n",
      "Iteration 472, loss = 0.19296074\n",
      "Iteration 473, loss = 0.19311112\n",
      "Iteration 474, loss = 0.19356971\n",
      "Iteration 475, loss = 0.19277275\n",
      "Iteration 476, loss = 0.19271666\n",
      "Iteration 477, loss = 0.19261868\n",
      "Iteration 478, loss = 0.19233499\n",
      "Iteration 479, loss = 0.19234720\n",
      "Iteration 480, loss = 0.19222533\n",
      "Iteration 481, loss = 0.19255585\n",
      "Iteration 482, loss = 0.19207623\n",
      "Iteration 483, loss = 0.19201536\n",
      "Iteration 484, loss = 0.19204929\n",
      "Iteration 485, loss = 0.19193588\n",
      "Iteration 486, loss = 0.19224003\n",
      "Iteration 487, loss = 0.19162510\n",
      "Iteration 488, loss = 0.19208700\n",
      "Iteration 489, loss = 0.19222528\n",
      "Iteration 490, loss = 0.19187273\n",
      "Iteration 491, loss = 0.19132644\n",
      "Iteration 492, loss = 0.19143535\n",
      "Iteration 493, loss = 0.19136559\n",
      "Iteration 494, loss = 0.19184552\n",
      "Iteration 495, loss = 0.19183439\n",
      "Iteration 496, loss = 0.19089578\n",
      "Iteration 497, loss = 0.19205289\n",
      "Iteration 498, loss = 0.19141016\n",
      "Iteration 499, loss = 0.19155200\n",
      "Iteration 500, loss = 0.19127460\n",
      "Iteration 501, loss = 0.19110218\n",
      "Iteration 502, loss = 0.19117332\n",
      "Iteration 503, loss = 0.19107878\n",
      "Iteration 504, loss = 0.19126109\n",
      "Iteration 505, loss = 0.19084220\n",
      "Iteration 506, loss = 0.19108694\n",
      "Iteration 507, loss = 0.19186655\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76983112\n",
      "Iteration 2, loss = 0.74393253\n",
      "Iteration 3, loss = 0.72250979\n",
      "Iteration 4, loss = 0.70509663\n",
      "Iteration 5, loss = 0.69060460\n",
      "Iteration 6, loss = 0.67792999\n",
      "Iteration 7, loss = 0.66562449\n",
      "Iteration 8, loss = 0.65141086\n",
      "Iteration 9, loss = 0.63410574\n",
      "Iteration 10, loss = 0.61536426\n",
      "Iteration 11, loss = 0.59544808\n",
      "Iteration 12, loss = 0.57639752\n",
      "Iteration 13, loss = 0.56011999\n",
      "Iteration 14, loss = 0.54554611\n",
      "Iteration 15, loss = 0.53302083\n",
      "Iteration 16, loss = 0.52261872\n",
      "Iteration 17, loss = 0.51261638\n",
      "Iteration 18, loss = 0.50410877\n",
      "Iteration 19, loss = 0.49644692\n",
      "Iteration 20, loss = 0.48942933\n",
      "Iteration 21, loss = 0.48274653\n",
      "Iteration 22, loss = 0.47683922\n",
      "Iteration 23, loss = 0.47146670\n",
      "Iteration 24, loss = 0.46621279\n",
      "Iteration 25, loss = 0.46125667\n",
      "Iteration 26, loss = 0.45647415\n",
      "Iteration 27, loss = 0.45199405\n",
      "Iteration 28, loss = 0.44753127\n",
      "Iteration 29, loss = 0.44350333\n",
      "Iteration 30, loss = 0.43926139\n",
      "Iteration 31, loss = 0.43541526\n",
      "Iteration 32, loss = 0.43178193\n",
      "Iteration 33, loss = 0.42820030\n",
      "Iteration 34, loss = 0.42459617\n",
      "Iteration 35, loss = 0.42123847\n",
      "Iteration 36, loss = 0.41773998\n",
      "Iteration 37, loss = 0.41457026\n",
      "Iteration 38, loss = 0.41119997\n",
      "Iteration 39, loss = 0.40842741\n",
      "Iteration 40, loss = 0.40532141\n",
      "Iteration 41, loss = 0.40240292\n",
      "Iteration 42, loss = 0.39942490\n",
      "Iteration 43, loss = 0.39665150\n",
      "Iteration 44, loss = 0.39416589\n",
      "Iteration 45, loss = 0.39141499\n",
      "Iteration 46, loss = 0.38876729\n",
      "Iteration 47, loss = 0.38629960\n",
      "Iteration 48, loss = 0.38382089\n",
      "Iteration 49, loss = 0.38137776\n",
      "Iteration 50, loss = 0.37904451\n",
      "Iteration 51, loss = 0.37652393\n",
      "Iteration 52, loss = 0.37418519\n",
      "Iteration 53, loss = 0.37200784\n",
      "Iteration 54, loss = 0.36970343\n",
      "Iteration 55, loss = 0.36758131\n",
      "Iteration 56, loss = 0.36576712\n",
      "Iteration 57, loss = 0.36357286\n",
      "Iteration 58, loss = 0.36135898\n",
      "Iteration 59, loss = 0.35920274\n",
      "Iteration 60, loss = 0.35711306\n",
      "Iteration 61, loss = 0.35512613\n",
      "Iteration 62, loss = 0.35294526\n",
      "Iteration 63, loss = 0.35186189\n",
      "Iteration 64, loss = 0.34944028\n",
      "Iteration 65, loss = 0.34784564\n",
      "Iteration 66, loss = 0.34590954\n",
      "Iteration 67, loss = 0.34354281\n",
      "Iteration 68, loss = 0.34199643\n",
      "Iteration 69, loss = 0.34007945\n",
      "Iteration 70, loss = 0.33849298\n",
      "Iteration 71, loss = 0.33690373\n",
      "Iteration 72, loss = 0.33503906\n",
      "Iteration 73, loss = 0.33343649\n",
      "Iteration 74, loss = 0.33182522\n",
      "Iteration 75, loss = 0.33033485\n",
      "Iteration 76, loss = 0.32883455\n",
      "Iteration 77, loss = 0.32709762\n",
      "Iteration 78, loss = 0.32550485\n",
      "Iteration 79, loss = 0.32408290\n",
      "Iteration 80, loss = 0.32231399\n",
      "Iteration 81, loss = 0.32091915\n",
      "Iteration 82, loss = 0.31930889\n",
      "Iteration 83, loss = 0.31805875\n",
      "Iteration 84, loss = 0.31649086\n",
      "Iteration 85, loss = 0.31512245\n",
      "Iteration 86, loss = 0.31375062\n",
      "Iteration 87, loss = 0.31232952\n",
      "Iteration 88, loss = 0.31091384\n",
      "Iteration 89, loss = 0.30944802\n",
      "Iteration 90, loss = 0.30828389\n",
      "Iteration 91, loss = 0.30691030\n",
      "Iteration 92, loss = 0.30560029\n",
      "Iteration 93, loss = 0.30413452\n",
      "Iteration 94, loss = 0.30288130\n",
      "Iteration 95, loss = 0.30173891\n",
      "Iteration 96, loss = 0.30026178\n",
      "Iteration 97, loss = 0.29948550\n",
      "Iteration 98, loss = 0.29793037\n",
      "Iteration 99, loss = 0.29722402\n",
      "Iteration 100, loss = 0.29563869\n",
      "Iteration 101, loss = 0.29444577\n",
      "Iteration 102, loss = 0.29335161\n",
      "Iteration 103, loss = 0.29205987\n",
      "Iteration 104, loss = 0.29117716\n",
      "Iteration 105, loss = 0.29010701\n",
      "Iteration 106, loss = 0.28851878\n",
      "Iteration 107, loss = 0.28764815\n",
      "Iteration 108, loss = 0.28629160\n",
      "Iteration 109, loss = 0.28597422\n",
      "Iteration 110, loss = 0.28412793\n",
      "Iteration 111, loss = 0.28346611\n",
      "Iteration 112, loss = 0.28257670\n",
      "Iteration 113, loss = 0.28126141\n",
      "Iteration 114, loss = 0.28024201\n",
      "Iteration 115, loss = 0.27922584\n",
      "Iteration 116, loss = 0.27833512\n",
      "Iteration 117, loss = 0.27733152\n",
      "Iteration 118, loss = 0.27651374\n",
      "Iteration 119, loss = 0.27615471\n",
      "Iteration 120, loss = 0.27518166\n",
      "Iteration 121, loss = 0.27340722\n",
      "Iteration 122, loss = 0.27311570\n",
      "Iteration 123, loss = 0.27195593\n",
      "Iteration 124, loss = 0.27138114\n",
      "Iteration 125, loss = 0.27056031\n",
      "Iteration 126, loss = 0.26962285\n",
      "Iteration 127, loss = 0.26873562\n",
      "Iteration 128, loss = 0.26771509\n",
      "Iteration 129, loss = 0.26692570\n",
      "Iteration 130, loss = 0.26611066\n",
      "Iteration 131, loss = 0.26521833\n",
      "Iteration 132, loss = 0.26448403\n",
      "Iteration 133, loss = 0.26367545\n",
      "Iteration 134, loss = 0.26294209\n",
      "Iteration 135, loss = 0.26230466\n",
      "Iteration 136, loss = 0.26158773\n",
      "Iteration 137, loss = 0.26111039\n",
      "Iteration 138, loss = 0.26063254\n",
      "Iteration 139, loss = 0.25940319\n",
      "Iteration 140, loss = 0.25881742\n",
      "Iteration 141, loss = 0.25854324\n",
      "Iteration 142, loss = 0.25765715\n",
      "Iteration 143, loss = 0.25688805\n",
      "Iteration 144, loss = 0.25653170\n",
      "Iteration 145, loss = 0.25569389\n",
      "Iteration 146, loss = 0.25484673\n",
      "Iteration 147, loss = 0.25492402\n",
      "Iteration 148, loss = 0.25345812\n",
      "Iteration 149, loss = 0.25358122\n",
      "Iteration 150, loss = 0.25221166\n",
      "Iteration 151, loss = 0.25178324\n",
      "Iteration 152, loss = 0.25118734\n",
      "Iteration 153, loss = 0.25040130\n",
      "Iteration 154, loss = 0.25048488\n",
      "Iteration 155, loss = 0.24955724\n",
      "Iteration 156, loss = 0.24872794\n",
      "Iteration 157, loss = 0.24853787\n",
      "Iteration 158, loss = 0.24866989\n",
      "Iteration 159, loss = 0.24732975\n",
      "Iteration 160, loss = 0.24667601\n",
      "Iteration 161, loss = 0.24637862\n",
      "Iteration 162, loss = 0.24575285\n",
      "Iteration 163, loss = 0.24573018\n",
      "Iteration 164, loss = 0.24466341\n",
      "Iteration 165, loss = 0.24446856\n",
      "Iteration 166, loss = 0.24375662\n",
      "Iteration 167, loss = 0.24361976\n",
      "Iteration 168, loss = 0.24278138\n",
      "Iteration 169, loss = 0.24237746\n",
      "Iteration 170, loss = 0.24198593\n",
      "Iteration 171, loss = 0.24186913\n",
      "Iteration 172, loss = 0.24093764\n",
      "Iteration 173, loss = 0.24053637\n",
      "Iteration 174, loss = 0.24021992\n",
      "Iteration 175, loss = 0.23973823\n",
      "Iteration 176, loss = 0.23937628\n",
      "Iteration 177, loss = 0.23848273\n",
      "Iteration 178, loss = 0.23821382\n",
      "Iteration 179, loss = 0.23732985\n",
      "Iteration 180, loss = 0.23691146\n",
      "Iteration 181, loss = 0.23667001\n",
      "Iteration 182, loss = 0.23609487\n",
      "Iteration 183, loss = 0.23538045\n",
      "Iteration 184, loss = 0.23537924\n",
      "Iteration 185, loss = 0.23467217\n",
      "Iteration 186, loss = 0.23485503\n",
      "Iteration 187, loss = 0.23451679\n",
      "Iteration 188, loss = 0.23357355\n",
      "Iteration 189, loss = 0.23281556\n",
      "Iteration 190, loss = 0.23262100\n",
      "Iteration 191, loss = 0.23218927\n",
      "Iteration 192, loss = 0.23179758\n",
      "Iteration 193, loss = 0.23146834\n",
      "Iteration 194, loss = 0.23106919\n",
      "Iteration 195, loss = 0.23040705\n",
      "Iteration 196, loss = 0.22994898\n",
      "Iteration 197, loss = 0.22965627\n",
      "Iteration 198, loss = 0.22933082\n",
      "Iteration 199, loss = 0.22845846\n",
      "Iteration 200, loss = 0.22802173\n",
      "Iteration 201, loss = 0.22769702\n",
      "Iteration 202, loss = 0.22721422\n",
      "Iteration 203, loss = 0.22667287\n",
      "Iteration 204, loss = 0.22636985\n",
      "Iteration 205, loss = 0.22626950\n",
      "Iteration 206, loss = 0.22537887\n",
      "Iteration 207, loss = 0.22549722\n",
      "Iteration 208, loss = 0.22448169\n",
      "Iteration 209, loss = 0.22452232\n",
      "Iteration 210, loss = 0.22389501\n",
      "Iteration 211, loss = 0.22324667\n",
      "Iteration 212, loss = 0.22338615\n",
      "Iteration 213, loss = 0.22261482\n",
      "Iteration 214, loss = 0.22208692\n",
      "Iteration 215, loss = 0.22261565\n",
      "Iteration 216, loss = 0.22162879\n",
      "Iteration 217, loss = 0.22106368\n",
      "Iteration 218, loss = 0.22166468\n",
      "Iteration 219, loss = 0.22019392\n",
      "Iteration 220, loss = 0.22021155\n",
      "Iteration 221, loss = 0.21955654\n",
      "Iteration 222, loss = 0.21949240\n",
      "Iteration 223, loss = 0.21884021\n",
      "Iteration 224, loss = 0.21900213\n",
      "Iteration 225, loss = 0.21867852\n",
      "Iteration 226, loss = 0.21800573\n",
      "Iteration 227, loss = 0.21789309\n",
      "Iteration 228, loss = 0.21763154\n",
      "Iteration 229, loss = 0.21710688\n",
      "Iteration 230, loss = 0.21696767\n",
      "Iteration 231, loss = 0.21670823\n",
      "Iteration 232, loss = 0.21630212\n",
      "Iteration 233, loss = 0.21615681\n",
      "Iteration 234, loss = 0.21571103\n",
      "Iteration 235, loss = 0.21556991\n",
      "Iteration 236, loss = 0.21537376\n",
      "Iteration 237, loss = 0.21514292\n",
      "Iteration 238, loss = 0.21456516\n",
      "Iteration 239, loss = 0.21425853\n",
      "Iteration 240, loss = 0.21479432\n",
      "Iteration 241, loss = 0.21435634\n",
      "Iteration 242, loss = 0.21365536\n",
      "Iteration 243, loss = 0.21380094\n",
      "Iteration 244, loss = 0.21350503\n",
      "Iteration 245, loss = 0.21306186\n",
      "Iteration 246, loss = 0.21274473\n",
      "Iteration 247, loss = 0.21242115\n",
      "Iteration 248, loss = 0.21238238\n",
      "Iteration 249, loss = 0.21194823\n",
      "Iteration 250, loss = 0.21252588\n",
      "Iteration 251, loss = 0.21182050\n",
      "Iteration 252, loss = 0.21145771\n",
      "Iteration 253, loss = 0.21103230\n",
      "Iteration 254, loss = 0.21078608\n",
      "Iteration 255, loss = 0.21110952\n",
      "Iteration 256, loss = 0.21039385\n",
      "Iteration 257, loss = 0.21025001\n",
      "Iteration 258, loss = 0.21025478\n",
      "Iteration 259, loss = 0.21009755\n",
      "Iteration 260, loss = 0.20947347\n",
      "Iteration 261, loss = 0.20916784\n",
      "Iteration 262, loss = 0.20919928\n",
      "Iteration 263, loss = 0.20898517\n",
      "Iteration 264, loss = 0.20842974\n",
      "Iteration 265, loss = 0.20864653\n",
      "Iteration 266, loss = 0.20824327\n",
      "Iteration 267, loss = 0.20795221\n",
      "Iteration 268, loss = 0.20796318\n",
      "Iteration 269, loss = 0.20874285\n",
      "Iteration 270, loss = 0.20772047\n",
      "Iteration 271, loss = 0.20761478\n",
      "Iteration 272, loss = 0.20733880\n",
      "Iteration 273, loss = 0.20709649\n",
      "Iteration 274, loss = 0.20704789\n",
      "Iteration 275, loss = 0.20644658\n",
      "Iteration 276, loss = 0.20635730\n",
      "Iteration 277, loss = 0.20606351\n",
      "Iteration 278, loss = 0.20723637\n",
      "Iteration 279, loss = 0.20607252\n",
      "Iteration 280, loss = 0.20563941\n",
      "Iteration 281, loss = 0.20584253\n",
      "Iteration 282, loss = 0.20537491\n",
      "Iteration 283, loss = 0.20545760\n",
      "Iteration 284, loss = 0.20497818\n",
      "Iteration 285, loss = 0.20468996\n",
      "Iteration 286, loss = 0.20516799\n",
      "Iteration 287, loss = 0.20476937\n",
      "Iteration 288, loss = 0.20386659\n",
      "Iteration 289, loss = 0.20419841\n",
      "Iteration 290, loss = 0.20391265\n",
      "Iteration 291, loss = 0.20394298\n",
      "Iteration 292, loss = 0.20344797\n",
      "Iteration 293, loss = 0.20333022\n",
      "Iteration 294, loss = 0.20291616\n",
      "Iteration 295, loss = 0.20382469\n",
      "Iteration 296, loss = 0.20404827\n",
      "Iteration 297, loss = 0.20377398\n",
      "Iteration 298, loss = 0.20309943\n",
      "Iteration 299, loss = 0.20277171\n",
      "Iteration 300, loss = 0.20266753\n",
      "Iteration 301, loss = 0.20188485\n",
      "Iteration 302, loss = 0.20216083\n",
      "Iteration 303, loss = 0.20164347\n",
      "Iteration 304, loss = 0.20186847\n",
      "Iteration 305, loss = 0.20108779\n",
      "Iteration 306, loss = 0.20158838\n",
      "Iteration 307, loss = 0.20294612\n",
      "Iteration 308, loss = 0.20115403\n",
      "Iteration 309, loss = 0.20067360\n",
      "Iteration 310, loss = 0.20100023\n",
      "Iteration 311, loss = 0.20132204\n",
      "Iteration 312, loss = 0.20043205\n",
      "Iteration 313, loss = 0.20044473\n",
      "Iteration 314, loss = 0.19990715\n",
      "Iteration 315, loss = 0.19960072\n",
      "Iteration 316, loss = 0.19912840\n",
      "Iteration 317, loss = 0.19907936\n",
      "Iteration 318, loss = 0.19930757\n",
      "Iteration 319, loss = 0.19862998\n",
      "Iteration 320, loss = 0.19853690\n",
      "Iteration 321, loss = 0.19834827\n",
      "Iteration 322, loss = 0.19831967\n",
      "Iteration 323, loss = 0.19832921\n",
      "Iteration 324, loss = 0.19807734\n",
      "Iteration 325, loss = 0.19782886\n",
      "Iteration 326, loss = 0.19775375\n",
      "Iteration 327, loss = 0.19755347\n",
      "Iteration 328, loss = 0.19727955\n",
      "Iteration 329, loss = 0.19716028\n",
      "Iteration 330, loss = 0.19766832\n",
      "Iteration 331, loss = 0.19707718\n",
      "Iteration 332, loss = 0.19706886\n",
      "Iteration 333, loss = 0.19684241\n",
      "Iteration 334, loss = 0.19648354\n",
      "Iteration 335, loss = 0.19621438\n",
      "Iteration 336, loss = 0.19637186\n",
      "Iteration 337, loss = 0.19597013\n",
      "Iteration 338, loss = 0.19609564\n",
      "Iteration 339, loss = 0.19596365\n",
      "Iteration 340, loss = 0.19575982\n",
      "Iteration 341, loss = 0.19619236\n",
      "Iteration 342, loss = 0.19577753\n",
      "Iteration 343, loss = 0.19617727\n",
      "Iteration 344, loss = 0.19509660\n",
      "Iteration 345, loss = 0.19494569\n",
      "Iteration 346, loss = 0.19479362\n",
      "Iteration 347, loss = 0.19527557\n",
      "Iteration 348, loss = 0.19440015\n",
      "Iteration 349, loss = 0.19480698\n",
      "Iteration 350, loss = 0.19440664\n",
      "Iteration 351, loss = 0.19421091\n",
      "Iteration 352, loss = 0.19458913\n",
      "Iteration 353, loss = 0.19377115\n",
      "Iteration 354, loss = 0.19374346\n",
      "Iteration 355, loss = 0.19331327\n",
      "Iteration 356, loss = 0.19336964\n",
      "Iteration 357, loss = 0.19336734\n",
      "Iteration 358, loss = 0.19268590\n",
      "Iteration 359, loss = 0.19260084\n",
      "Iteration 360, loss = 0.19218155\n",
      "Iteration 361, loss = 0.19235245\n",
      "Iteration 362, loss = 0.19205279\n",
      "Iteration 363, loss = 0.19196889\n",
      "Iteration 364, loss = 0.19213411\n",
      "Iteration 365, loss = 0.19193795\n",
      "Iteration 366, loss = 0.19159188\n",
      "Iteration 367, loss = 0.19168668\n",
      "Iteration 368, loss = 0.19157209\n",
      "Iteration 369, loss = 0.19131056\n",
      "Iteration 370, loss = 0.19146775\n",
      "Iteration 371, loss = 0.19103289\n",
      "Iteration 372, loss = 0.19132802\n",
      "Iteration 373, loss = 0.19084051\n",
      "Iteration 374, loss = 0.19089240\n",
      "Iteration 375, loss = 0.19104617\n",
      "Iteration 376, loss = 0.19020637\n",
      "Iteration 377, loss = 0.19048863\n",
      "Iteration 378, loss = 0.19054757\n",
      "Iteration 379, loss = 0.19046267\n",
      "Iteration 380, loss = 0.19044585\n",
      "Iteration 381, loss = 0.19009079\n",
      "Iteration 382, loss = 0.19009701\n",
      "Iteration 383, loss = 0.19022163\n",
      "Iteration 384, loss = 0.18943686\n",
      "Iteration 385, loss = 0.18985536\n",
      "Iteration 386, loss = 0.18968676\n",
      "Iteration 387, loss = 0.18969090\n",
      "Iteration 388, loss = 0.18980808\n",
      "Iteration 389, loss = 0.18929379\n",
      "Iteration 390, loss = 0.18904708\n",
      "Iteration 391, loss = 0.18954868\n",
      "Iteration 392, loss = 0.18886307\n",
      "Iteration 393, loss = 0.18870999\n",
      "Iteration 394, loss = 0.18893211\n",
      "Iteration 395, loss = 0.18876500\n",
      "Iteration 396, loss = 0.18866602\n",
      "Iteration 397, loss = 0.18832787\n",
      "Iteration 398, loss = 0.18836628\n",
      "Iteration 399, loss = 0.18840096\n",
      "Iteration 400, loss = 0.18817782\n",
      "Iteration 401, loss = 0.18835659\n",
      "Iteration 402, loss = 0.18813874\n",
      "Iteration 403, loss = 0.18848849\n",
      "Iteration 404, loss = 0.18841683\n",
      "Iteration 405, loss = 0.18788391\n",
      "Iteration 406, loss = 0.18770868\n",
      "Iteration 407, loss = 0.18721767\n",
      "Iteration 408, loss = 0.18753515\n",
      "Iteration 409, loss = 0.18721579\n",
      "Iteration 410, loss = 0.18809939\n",
      "Iteration 411, loss = 0.18712523\n",
      "Iteration 412, loss = 0.18722368\n",
      "Iteration 413, loss = 0.18722714\n",
      "Iteration 414, loss = 0.18673114\n",
      "Iteration 415, loss = 0.18736035\n",
      "Iteration 416, loss = 0.18681471\n",
      "Iteration 417, loss = 0.18659658\n",
      "Iteration 418, loss = 0.18687622\n",
      "Iteration 419, loss = 0.18665574\n",
      "Iteration 420, loss = 0.18669399\n",
      "Iteration 421, loss = 0.18640620\n",
      "Iteration 422, loss = 0.18624774\n",
      "Iteration 423, loss = 0.18606313\n",
      "Iteration 424, loss = 0.18658541\n",
      "Iteration 425, loss = 0.18652907\n",
      "Iteration 426, loss = 0.18608394\n",
      "Iteration 427, loss = 0.18588320\n",
      "Iteration 428, loss = 0.18598404\n",
      "Iteration 429, loss = 0.18597834\n",
      "Iteration 430, loss = 0.18603042\n",
      "Iteration 431, loss = 0.18573115\n",
      "Iteration 432, loss = 0.18602461\n",
      "Iteration 433, loss = 0.18549875\n",
      "Iteration 434, loss = 0.18575410\n",
      "Iteration 435, loss = 0.18525193\n",
      "Iteration 436, loss = 0.18570049\n",
      "Iteration 437, loss = 0.18516727\n",
      "Iteration 438, loss = 0.18537615\n",
      "Iteration 439, loss = 0.18515521\n",
      "Iteration 440, loss = 0.18505144\n",
      "Iteration 441, loss = 0.18544156\n",
      "Iteration 442, loss = 0.18495102\n",
      "Iteration 443, loss = 0.18452102\n",
      "Iteration 444, loss = 0.18515428\n",
      "Iteration 445, loss = 0.18459542\n",
      "Iteration 446, loss = 0.18425019\n",
      "Iteration 447, loss = 0.18480606\n",
      "Iteration 448, loss = 0.18434040\n",
      "Iteration 449, loss = 0.18443085\n",
      "Iteration 450, loss = 0.18466537\n",
      "Iteration 451, loss = 0.18391553\n",
      "Iteration 452, loss = 0.18396429\n",
      "Iteration 453, loss = 0.18421173\n",
      "Iteration 454, loss = 0.18377863\n",
      "Iteration 455, loss = 0.18354577\n",
      "Iteration 456, loss = 0.18357532\n",
      "Iteration 457, loss = 0.18398735\n",
      "Iteration 458, loss = 0.18387151\n",
      "Iteration 459, loss = 0.18372401\n",
      "Iteration 460, loss = 0.18344204\n",
      "Iteration 461, loss = 0.18330732\n",
      "Iteration 462, loss = 0.18312290\n",
      "Iteration 463, loss = 0.18341397\n",
      "Iteration 464, loss = 0.18323272\n",
      "Iteration 465, loss = 0.18298895\n",
      "Iteration 466, loss = 0.18323816\n",
      "Iteration 467, loss = 0.18404163\n",
      "Iteration 468, loss = 0.18291447\n",
      "Iteration 469, loss = 0.18283632\n",
      "Iteration 470, loss = 0.18264212\n",
      "Iteration 471, loss = 0.18290238\n",
      "Iteration 472, loss = 0.18257647\n",
      "Iteration 473, loss = 0.18298991\n",
      "Iteration 474, loss = 0.18237386\n",
      "Iteration 475, loss = 0.18234047\n",
      "Iteration 476, loss = 0.18236260\n",
      "Iteration 477, loss = 0.18224518\n",
      "Iteration 478, loss = 0.18236393\n",
      "Iteration 479, loss = 0.18276061\n",
      "Iteration 480, loss = 0.18128723\n",
      "Iteration 481, loss = 0.18181999\n",
      "Iteration 482, loss = 0.18219437\n",
      "Iteration 483, loss = 0.18151602\n",
      "Iteration 484, loss = 0.18106816\n",
      "Iteration 485, loss = 0.18156425\n",
      "Iteration 486, loss = 0.18117146\n",
      "Iteration 487, loss = 0.18126868\n",
      "Iteration 488, loss = 0.18111087\n",
      "Iteration 489, loss = 0.18059526\n",
      "Iteration 490, loss = 0.18091354\n",
      "Iteration 491, loss = 0.18054330\n",
      "Iteration 492, loss = 0.17998686\n",
      "Iteration 493, loss = 0.18034174\n",
      "Iteration 494, loss = 0.17991218\n",
      "Iteration 495, loss = 0.18002149\n",
      "Iteration 496, loss = 0.18006510\n",
      "Iteration 497, loss = 0.17945172\n",
      "Iteration 498, loss = 0.17980613\n",
      "Iteration 499, loss = 0.17980450\n",
      "Iteration 500, loss = 0.17957634\n",
      "Iteration 501, loss = 0.17937703\n",
      "Iteration 502, loss = 0.17917830\n",
      "Iteration 503, loss = 0.17897319\n",
      "Iteration 504, loss = 0.17993105\n",
      "Iteration 505, loss = 0.17896239\n",
      "Iteration 506, loss = 0.17856664\n",
      "Iteration 507, loss = 0.17862436\n",
      "Iteration 508, loss = 0.17808324\n",
      "Iteration 509, loss = 0.17813364\n",
      "Iteration 510, loss = 0.17808925\n",
      "Iteration 511, loss = 0.17800451\n",
      "Iteration 512, loss = 0.17767353\n",
      "Iteration 513, loss = 0.17780526\n",
      "Iteration 514, loss = 0.17734702\n",
      "Iteration 515, loss = 0.17758626\n",
      "Iteration 516, loss = 0.17719324\n",
      "Iteration 517, loss = 0.17728342\n",
      "Iteration 518, loss = 0.17723404\n",
      "Iteration 519, loss = 0.17703892\n",
      "Iteration 520, loss = 0.17699580\n",
      "Iteration 521, loss = 0.17718640\n",
      "Iteration 522, loss = 0.17693119\n",
      "Iteration 523, loss = 0.17678117\n",
      "Iteration 524, loss = 0.17703318\n",
      "Iteration 525, loss = 0.17710356\n",
      "Iteration 526, loss = 0.17661091\n",
      "Iteration 527, loss = 0.17668885\n",
      "Iteration 528, loss = 0.17665040\n",
      "Iteration 529, loss = 0.17656336\n",
      "Iteration 530, loss = 0.17623100\n",
      "Iteration 531, loss = 0.17636984\n",
      "Iteration 532, loss = 0.17690830\n",
      "Iteration 533, loss = 0.17638464\n",
      "Iteration 534, loss = 0.17587714\n",
      "Iteration 535, loss = 0.17640229\n",
      "Iteration 536, loss = 0.17628578\n",
      "Iteration 537, loss = 0.17583548\n",
      "Iteration 538, loss = 0.17611183\n",
      "Iteration 539, loss = 0.17607306\n",
      "Iteration 540, loss = 0.17613587\n",
      "Iteration 541, loss = 0.17587066\n",
      "Iteration 542, loss = 0.17666275\n",
      "Iteration 543, loss = 0.17699159\n",
      "Iteration 544, loss = 0.17613437\n",
      "Iteration 545, loss = 0.17554250\n",
      "Iteration 546, loss = 0.17560586\n",
      "Iteration 547, loss = 0.17607773\n",
      "Iteration 548, loss = 0.17532142\n",
      "Iteration 549, loss = 0.17554040\n",
      "Iteration 550, loss = 0.17508426\n",
      "Iteration 551, loss = 0.17571907\n",
      "Iteration 552, loss = 0.17514570\n",
      "Iteration 553, loss = 0.17506394\n",
      "Iteration 554, loss = 0.17511713\n",
      "Iteration 555, loss = 0.17468726\n",
      "Iteration 556, loss = 0.17519893\n",
      "Iteration 557, loss = 0.17474477\n",
      "Iteration 558, loss = 0.17445714\n",
      "Iteration 559, loss = 0.17472033\n",
      "Iteration 560, loss = 0.17463332\n",
      "Iteration 561, loss = 0.17441283\n",
      "Iteration 562, loss = 0.17438612\n",
      "Iteration 563, loss = 0.17402741\n",
      "Iteration 564, loss = 0.17403972\n",
      "Iteration 565, loss = 0.17370818\n",
      "Iteration 566, loss = 0.17387599\n",
      "Iteration 567, loss = 0.17407405\n",
      "Iteration 568, loss = 0.17450821\n",
      "Iteration 569, loss = 0.17359133\n",
      "Iteration 570, loss = 0.17492027\n",
      "Iteration 571, loss = 0.17434350\n",
      "Iteration 572, loss = 0.17395151\n",
      "Iteration 573, loss = 0.17379625\n",
      "Iteration 574, loss = 0.17354831\n",
      "Iteration 575, loss = 0.17370326\n",
      "Iteration 576, loss = 0.17352103\n",
      "Iteration 577, loss = 0.17340625\n",
      "Iteration 578, loss = 0.17346002\n",
      "Iteration 579, loss = 0.17325823\n",
      "Iteration 580, loss = 0.17309064\n",
      "Iteration 581, loss = 0.17414297\n",
      "Iteration 582, loss = 0.17314975\n",
      "Iteration 583, loss = 0.17311948\n",
      "Iteration 584, loss = 0.17294674\n",
      "Iteration 585, loss = 0.17312113\n",
      "Iteration 586, loss = 0.17305454\n",
      "Iteration 587, loss = 0.17414138\n",
      "Iteration 588, loss = 0.17396452\n",
      "Iteration 589, loss = 0.17341933\n",
      "Iteration 590, loss = 0.17293945\n",
      "Iteration 591, loss = 0.17297529\n",
      "Iteration 592, loss = 0.17340768\n",
      "Iteration 593, loss = 0.17300591\n",
      "Iteration 594, loss = 0.17286119\n",
      "Iteration 595, loss = 0.17306208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76925042\n",
      "Iteration 2, loss = 0.74304528\n",
      "Iteration 3, loss = 0.72176161\n",
      "Iteration 4, loss = 0.70458812\n",
      "Iteration 5, loss = 0.69033744\n",
      "Iteration 6, loss = 0.67691594\n",
      "Iteration 7, loss = 0.66268587\n",
      "Iteration 8, loss = 0.64664511\n",
      "Iteration 9, loss = 0.62820279\n",
      "Iteration 10, loss = 0.60874533\n",
      "Iteration 11, loss = 0.58960023\n",
      "Iteration 12, loss = 0.57125226\n",
      "Iteration 13, loss = 0.55474106\n",
      "Iteration 14, loss = 0.54062021\n",
      "Iteration 15, loss = 0.52833337\n",
      "Iteration 16, loss = 0.51731949\n",
      "Iteration 17, loss = 0.50789028\n",
      "Iteration 18, loss = 0.49974991\n",
      "Iteration 19, loss = 0.49234476\n",
      "Iteration 20, loss = 0.48557640\n",
      "Iteration 21, loss = 0.47902261\n",
      "Iteration 22, loss = 0.47303159\n",
      "Iteration 23, loss = 0.46766575\n",
      "Iteration 24, loss = 0.46275508\n",
      "Iteration 25, loss = 0.45780228\n",
      "Iteration 26, loss = 0.45322156\n",
      "Iteration 27, loss = 0.44875400\n",
      "Iteration 28, loss = 0.44459639\n",
      "Iteration 29, loss = 0.44067278\n",
      "Iteration 30, loss = 0.43683613\n",
      "Iteration 31, loss = 0.43293564\n",
      "Iteration 32, loss = 0.42930600\n",
      "Iteration 33, loss = 0.42591460\n",
      "Iteration 34, loss = 0.42270184\n",
      "Iteration 35, loss = 0.41921754\n",
      "Iteration 36, loss = 0.41583301\n",
      "Iteration 37, loss = 0.41285256\n",
      "Iteration 38, loss = 0.40947686\n",
      "Iteration 39, loss = 0.40660781\n",
      "Iteration 40, loss = 0.40363742\n",
      "Iteration 41, loss = 0.40062484\n",
      "Iteration 42, loss = 0.39785331\n",
      "Iteration 43, loss = 0.39528985\n",
      "Iteration 44, loss = 0.39235833\n",
      "Iteration 45, loss = 0.38964125\n",
      "Iteration 46, loss = 0.38680795\n",
      "Iteration 47, loss = 0.38424292\n",
      "Iteration 48, loss = 0.38186858\n",
      "Iteration 49, loss = 0.37919217\n",
      "Iteration 50, loss = 0.37683652\n",
      "Iteration 51, loss = 0.37440799\n",
      "Iteration 52, loss = 0.37213404\n",
      "Iteration 53, loss = 0.36983376\n",
      "Iteration 54, loss = 0.36754127\n",
      "Iteration 55, loss = 0.36535016\n",
      "Iteration 56, loss = 0.36322967\n",
      "Iteration 57, loss = 0.36110278\n",
      "Iteration 58, loss = 0.35889567\n",
      "Iteration 59, loss = 0.35691753\n",
      "Iteration 60, loss = 0.35494429\n",
      "Iteration 61, loss = 0.35289085\n",
      "Iteration 62, loss = 0.35106330\n",
      "Iteration 63, loss = 0.34914645\n",
      "Iteration 64, loss = 0.34729783\n",
      "Iteration 65, loss = 0.34530777\n",
      "Iteration 66, loss = 0.34334648\n",
      "Iteration 67, loss = 0.34167855\n",
      "Iteration 68, loss = 0.33981759\n",
      "Iteration 69, loss = 0.33790986\n",
      "Iteration 70, loss = 0.33633220\n",
      "Iteration 71, loss = 0.33484575\n",
      "Iteration 72, loss = 0.33287547\n",
      "Iteration 73, loss = 0.33112479\n",
      "Iteration 74, loss = 0.32973397\n",
      "Iteration 75, loss = 0.32773778\n",
      "Iteration 76, loss = 0.32632622\n",
      "Iteration 77, loss = 0.32481752\n",
      "Iteration 78, loss = 0.32317543\n",
      "Iteration 79, loss = 0.32182696\n",
      "Iteration 80, loss = 0.32040250\n",
      "Iteration 81, loss = 0.31883304\n",
      "Iteration 82, loss = 0.31732300\n",
      "Iteration 83, loss = 0.31598471\n",
      "Iteration 84, loss = 0.31455736\n",
      "Iteration 85, loss = 0.31319954\n",
      "Iteration 86, loss = 0.31169906\n",
      "Iteration 87, loss = 0.31037177\n",
      "Iteration 88, loss = 0.30927036\n",
      "Iteration 89, loss = 0.30777157\n",
      "Iteration 90, loss = 0.30654909\n",
      "Iteration 91, loss = 0.30542656\n",
      "Iteration 92, loss = 0.30410734\n",
      "Iteration 93, loss = 0.30294733\n",
      "Iteration 94, loss = 0.30161763\n",
      "Iteration 95, loss = 0.30054557\n",
      "Iteration 96, loss = 0.29939785\n",
      "Iteration 97, loss = 0.29825871\n",
      "Iteration 98, loss = 0.29697017\n",
      "Iteration 99, loss = 0.29595032\n",
      "Iteration 100, loss = 0.29483013\n",
      "Iteration 101, loss = 0.29363324\n",
      "Iteration 102, loss = 0.29240863\n",
      "Iteration 103, loss = 0.29176495\n",
      "Iteration 104, loss = 0.29042995\n",
      "Iteration 105, loss = 0.28987182\n",
      "Iteration 106, loss = 0.28826481\n",
      "Iteration 107, loss = 0.28753701\n",
      "Iteration 108, loss = 0.28618106\n",
      "Iteration 109, loss = 0.28536517\n",
      "Iteration 110, loss = 0.28459660\n",
      "Iteration 111, loss = 0.28322095\n",
      "Iteration 112, loss = 0.28246054\n",
      "Iteration 113, loss = 0.28131085\n",
      "Iteration 114, loss = 0.28034531\n",
      "Iteration 115, loss = 0.27941478\n",
      "Iteration 116, loss = 0.27832952\n",
      "Iteration 117, loss = 0.27765684\n",
      "Iteration 118, loss = 0.27701275\n",
      "Iteration 119, loss = 0.27566840\n",
      "Iteration 120, loss = 0.27473577\n",
      "Iteration 121, loss = 0.27384748\n",
      "Iteration 122, loss = 0.27329856\n",
      "Iteration 123, loss = 0.27243945\n",
      "Iteration 124, loss = 0.27148463\n",
      "Iteration 125, loss = 0.27092150\n",
      "Iteration 126, loss = 0.27009293\n",
      "Iteration 127, loss = 0.26938062\n",
      "Iteration 128, loss = 0.26842473\n",
      "Iteration 129, loss = 0.26720759\n",
      "Iteration 130, loss = 0.26674260\n",
      "Iteration 131, loss = 0.26567150\n",
      "Iteration 132, loss = 0.26491459\n",
      "Iteration 133, loss = 0.26421951\n",
      "Iteration 134, loss = 0.26300648\n",
      "Iteration 135, loss = 0.26228813\n",
      "Iteration 136, loss = 0.26149705\n",
      "Iteration 137, loss = 0.26096619\n",
      "Iteration 138, loss = 0.25993874\n",
      "Iteration 139, loss = 0.25951072\n",
      "Iteration 140, loss = 0.25849841\n",
      "Iteration 141, loss = 0.25786066\n",
      "Iteration 142, loss = 0.25748726\n",
      "Iteration 143, loss = 0.25658875\n",
      "Iteration 144, loss = 0.25578248\n",
      "Iteration 145, loss = 0.25561964\n",
      "Iteration 146, loss = 0.25451617\n",
      "Iteration 147, loss = 0.25383666\n",
      "Iteration 148, loss = 0.25327086\n",
      "Iteration 149, loss = 0.25264726\n",
      "Iteration 150, loss = 0.25197237\n",
      "Iteration 151, loss = 0.25131010\n",
      "Iteration 152, loss = 0.25066707\n",
      "Iteration 153, loss = 0.25089352\n",
      "Iteration 154, loss = 0.25022985\n",
      "Iteration 155, loss = 0.24903277\n",
      "Iteration 156, loss = 0.24881123\n",
      "Iteration 157, loss = 0.24796412\n",
      "Iteration 158, loss = 0.24740885\n",
      "Iteration 159, loss = 0.24684806\n",
      "Iteration 160, loss = 0.24615253\n",
      "Iteration 161, loss = 0.24564380\n",
      "Iteration 162, loss = 0.24518953\n",
      "Iteration 163, loss = 0.24467106\n",
      "Iteration 164, loss = 0.24406710\n",
      "Iteration 165, loss = 0.24405489\n",
      "Iteration 166, loss = 0.24289705\n",
      "Iteration 167, loss = 0.24329463\n",
      "Iteration 168, loss = 0.24214998\n",
      "Iteration 169, loss = 0.24156045\n",
      "Iteration 170, loss = 0.24158291\n",
      "Iteration 171, loss = 0.24046234\n",
      "Iteration 172, loss = 0.24011507\n",
      "Iteration 173, loss = 0.23991379\n",
      "Iteration 174, loss = 0.23936151\n",
      "Iteration 175, loss = 0.23867537\n",
      "Iteration 176, loss = 0.23812716\n",
      "Iteration 177, loss = 0.23810700\n",
      "Iteration 178, loss = 0.23770077\n",
      "Iteration 179, loss = 0.23725747\n",
      "Iteration 180, loss = 0.23635704\n",
      "Iteration 181, loss = 0.23590173\n",
      "Iteration 182, loss = 0.23563653\n",
      "Iteration 183, loss = 0.23504741\n",
      "Iteration 184, loss = 0.23466210\n",
      "Iteration 185, loss = 0.23441588\n",
      "Iteration 186, loss = 0.23379914\n",
      "Iteration 187, loss = 0.23394581\n",
      "Iteration 188, loss = 0.23319376\n",
      "Iteration 189, loss = 0.23272583\n",
      "Iteration 190, loss = 0.23202274\n",
      "Iteration 191, loss = 0.23182942\n",
      "Iteration 192, loss = 0.23151029\n",
      "Iteration 193, loss = 0.23177136\n",
      "Iteration 194, loss = 0.23068604\n",
      "Iteration 195, loss = 0.23039987\n",
      "Iteration 196, loss = 0.23040330\n",
      "Iteration 197, loss = 0.22956839\n",
      "Iteration 198, loss = 0.22952526\n",
      "Iteration 199, loss = 0.22940899\n",
      "Iteration 200, loss = 0.22862687\n",
      "Iteration 201, loss = 0.22870592\n",
      "Iteration 202, loss = 0.22797749\n",
      "Iteration 203, loss = 0.22740975\n",
      "Iteration 204, loss = 0.22715123\n",
      "Iteration 205, loss = 0.22818896\n",
      "Iteration 206, loss = 0.22732719\n",
      "Iteration 207, loss = 0.22633393\n",
      "Iteration 208, loss = 0.22619420\n",
      "Iteration 209, loss = 0.22629184\n",
      "Iteration 210, loss = 0.22563979\n",
      "Iteration 211, loss = 0.22510570\n",
      "Iteration 212, loss = 0.22481820\n",
      "Iteration 213, loss = 0.22436889\n",
      "Iteration 214, loss = 0.22421957\n",
      "Iteration 215, loss = 0.22387612\n",
      "Iteration 216, loss = 0.22353508\n",
      "Iteration 217, loss = 0.22351427\n",
      "Iteration 218, loss = 0.22283233\n",
      "Iteration 219, loss = 0.22305494\n",
      "Iteration 220, loss = 0.22299833\n",
      "Iteration 221, loss = 0.22243525\n",
      "Iteration 222, loss = 0.22145763\n",
      "Iteration 223, loss = 0.22137415\n",
      "Iteration 224, loss = 0.22106826\n",
      "Iteration 225, loss = 0.22133577\n",
      "Iteration 226, loss = 0.22084435\n",
      "Iteration 227, loss = 0.22027859\n",
      "Iteration 228, loss = 0.22009258\n",
      "Iteration 229, loss = 0.21953282\n",
      "Iteration 230, loss = 0.21982519\n",
      "Iteration 231, loss = 0.21904917\n",
      "Iteration 232, loss = 0.21918116\n",
      "Iteration 233, loss = 0.21878169\n",
      "Iteration 234, loss = 0.21841759\n",
      "Iteration 235, loss = 0.21827798\n",
      "Iteration 236, loss = 0.21766865\n",
      "Iteration 237, loss = 0.21774619\n",
      "Iteration 238, loss = 0.21729752\n",
      "Iteration 239, loss = 0.21729566\n",
      "Iteration 240, loss = 0.21685536\n",
      "Iteration 241, loss = 0.21699678\n",
      "Iteration 242, loss = 0.21687122\n",
      "Iteration 243, loss = 0.21708683\n",
      "Iteration 244, loss = 0.21597288\n",
      "Iteration 245, loss = 0.21568065\n",
      "Iteration 246, loss = 0.21539142\n",
      "Iteration 247, loss = 0.21537913\n",
      "Iteration 248, loss = 0.21510456\n",
      "Iteration 249, loss = 0.21479033\n",
      "Iteration 250, loss = 0.21459078\n",
      "Iteration 251, loss = 0.21422250\n",
      "Iteration 252, loss = 0.21383893\n",
      "Iteration 253, loss = 0.21363021\n",
      "Iteration 254, loss = 0.21347545\n",
      "Iteration 255, loss = 0.21346126\n",
      "Iteration 256, loss = 0.21290430\n",
      "Iteration 257, loss = 0.21293410\n",
      "Iteration 258, loss = 0.21244432\n",
      "Iteration 259, loss = 0.21281028\n",
      "Iteration 260, loss = 0.21261964\n",
      "Iteration 261, loss = 0.21176258\n",
      "Iteration 262, loss = 0.21179956\n",
      "Iteration 263, loss = 0.21138222\n",
      "Iteration 264, loss = 0.21142887\n",
      "Iteration 265, loss = 0.21109193\n",
      "Iteration 266, loss = 0.21110813\n",
      "Iteration 267, loss = 0.21107593\n",
      "Iteration 268, loss = 0.21052031\n",
      "Iteration 269, loss = 0.21063353\n",
      "Iteration 270, loss = 0.21016655\n",
      "Iteration 271, loss = 0.21005025\n",
      "Iteration 272, loss = 0.20962519\n",
      "Iteration 273, loss = 0.20974135\n",
      "Iteration 274, loss = 0.20884131\n",
      "Iteration 275, loss = 0.20914769\n",
      "Iteration 276, loss = 0.20870076\n",
      "Iteration 277, loss = 0.20881563\n",
      "Iteration 278, loss = 0.20804989\n",
      "Iteration 279, loss = 0.20779080\n",
      "Iteration 280, loss = 0.20768711\n",
      "Iteration 281, loss = 0.20754756\n",
      "Iteration 282, loss = 0.20774687\n",
      "Iteration 283, loss = 0.20771508\n",
      "Iteration 284, loss = 0.20693761\n",
      "Iteration 285, loss = 0.20689676\n",
      "Iteration 286, loss = 0.20669912\n",
      "Iteration 287, loss = 0.20652179\n",
      "Iteration 288, loss = 0.20639753\n",
      "Iteration 289, loss = 0.20573114\n",
      "Iteration 290, loss = 0.20574983\n",
      "Iteration 291, loss = 0.20550658\n",
      "Iteration 292, loss = 0.20509616\n",
      "Iteration 293, loss = 0.20450933\n",
      "Iteration 294, loss = 0.20465475\n",
      "Iteration 295, loss = 0.20430629\n",
      "Iteration 296, loss = 0.20417446\n",
      "Iteration 297, loss = 0.20401054\n",
      "Iteration 298, loss = 0.20388335\n",
      "Iteration 299, loss = 0.20352387\n",
      "Iteration 300, loss = 0.20320064\n",
      "Iteration 301, loss = 0.20294302\n",
      "Iteration 302, loss = 0.20277439\n",
      "Iteration 303, loss = 0.20258285\n",
      "Iteration 304, loss = 0.20248176\n",
      "Iteration 305, loss = 0.20213940\n",
      "Iteration 306, loss = 0.20241690\n",
      "Iteration 307, loss = 0.20248913\n",
      "Iteration 308, loss = 0.20192794\n",
      "Iteration 309, loss = 0.20160464\n",
      "Iteration 310, loss = 0.20140552\n",
      "Iteration 311, loss = 0.20148797\n",
      "Iteration 312, loss = 0.20099032\n",
      "Iteration 313, loss = 0.20069877\n",
      "Iteration 314, loss = 0.20117957\n",
      "Iteration 315, loss = 0.20043882\n",
      "Iteration 316, loss = 0.20022774\n",
      "Iteration 317, loss = 0.20032282\n",
      "Iteration 318, loss = 0.20028385\n",
      "Iteration 319, loss = 0.19973435\n",
      "Iteration 320, loss = 0.19997499\n",
      "Iteration 321, loss = 0.19973558\n",
      "Iteration 322, loss = 0.19937138\n",
      "Iteration 323, loss = 0.19910659\n",
      "Iteration 324, loss = 0.19984077\n",
      "Iteration 325, loss = 0.19879949\n",
      "Iteration 326, loss = 0.19892255\n",
      "Iteration 327, loss = 0.19865284\n",
      "Iteration 328, loss = 0.19936166\n",
      "Iteration 329, loss = 0.19836291\n",
      "Iteration 330, loss = 0.19845319\n",
      "Iteration 331, loss = 0.19831921\n",
      "Iteration 332, loss = 0.19800697\n",
      "Iteration 333, loss = 0.19822503\n",
      "Iteration 334, loss = 0.19755643\n",
      "Iteration 335, loss = 0.19748572\n",
      "Iteration 336, loss = 0.19739155\n",
      "Iteration 337, loss = 0.19755500\n",
      "Iteration 338, loss = 0.19843438\n",
      "Iteration 339, loss = 0.19764069\n",
      "Iteration 340, loss = 0.19689240\n",
      "Iteration 341, loss = 0.19697951\n",
      "Iteration 342, loss = 0.19696022\n",
      "Iteration 343, loss = 0.19650151\n",
      "Iteration 344, loss = 0.19626228\n",
      "Iteration 345, loss = 0.19623136\n",
      "Iteration 346, loss = 0.19600806\n",
      "Iteration 347, loss = 0.19633246\n",
      "Iteration 348, loss = 0.19667020\n",
      "Iteration 349, loss = 0.19654496\n",
      "Iteration 350, loss = 0.19540311\n",
      "Iteration 351, loss = 0.19619237\n",
      "Iteration 352, loss = 0.19515244\n",
      "Iteration 353, loss = 0.19545601\n",
      "Iteration 354, loss = 0.19564885\n",
      "Iteration 355, loss = 0.19514916\n",
      "Iteration 356, loss = 0.19486016\n",
      "Iteration 357, loss = 0.19544476\n",
      "Iteration 358, loss = 0.19464643\n",
      "Iteration 359, loss = 0.19470347\n",
      "Iteration 360, loss = 0.19416613\n",
      "Iteration 361, loss = 0.19437275\n",
      "Iteration 362, loss = 0.19437617\n",
      "Iteration 363, loss = 0.19421304\n",
      "Iteration 364, loss = 0.19369153\n",
      "Iteration 365, loss = 0.19423868\n",
      "Iteration 366, loss = 0.19334053\n",
      "Iteration 367, loss = 0.19366440\n",
      "Iteration 368, loss = 0.19340260\n",
      "Iteration 369, loss = 0.19382946\n",
      "Iteration 370, loss = 0.19448982\n",
      "Iteration 371, loss = 0.19311071\n",
      "Iteration 372, loss = 0.19311350\n",
      "Iteration 373, loss = 0.19295603\n",
      "Iteration 374, loss = 0.19294954\n",
      "Iteration 375, loss = 0.19290838\n",
      "Iteration 376, loss = 0.19273962\n",
      "Iteration 377, loss = 0.19413715\n",
      "Iteration 378, loss = 0.19317287\n",
      "Iteration 379, loss = 0.19246650\n",
      "Iteration 380, loss = 0.19195062\n",
      "Iteration 381, loss = 0.19202606\n",
      "Iteration 382, loss = 0.19210476\n",
      "Iteration 383, loss = 0.19240548\n",
      "Iteration 384, loss = 0.19282227\n",
      "Iteration 385, loss = 0.19240056\n",
      "Iteration 386, loss = 0.19223727\n",
      "Iteration 387, loss = 0.19154211\n",
      "Iteration 388, loss = 0.19183405\n",
      "Iteration 389, loss = 0.19104225\n",
      "Iteration 390, loss = 0.19201335\n",
      "Iteration 391, loss = 0.19176389\n",
      "Iteration 392, loss = 0.19142822\n",
      "Iteration 393, loss = 0.19062422\n",
      "Iteration 394, loss = 0.19077727\n",
      "Iteration 395, loss = 0.19038306\n",
      "Iteration 396, loss = 0.19031986\n",
      "Iteration 397, loss = 0.19031069\n",
      "Iteration 398, loss = 0.18997283\n",
      "Iteration 399, loss = 0.19109164\n",
      "Iteration 400, loss = 0.18971082\n",
      "Iteration 401, loss = 0.19035898\n",
      "Iteration 402, loss = 0.18990218\n",
      "Iteration 403, loss = 0.18955286\n",
      "Iteration 404, loss = 0.18960755\n",
      "Iteration 405, loss = 0.18973085\n",
      "Iteration 406, loss = 0.18905733\n",
      "Iteration 407, loss = 0.18940613\n",
      "Iteration 408, loss = 0.18861862\n",
      "Iteration 409, loss = 0.18898916\n",
      "Iteration 410, loss = 0.18843573\n",
      "Iteration 411, loss = 0.18887692\n",
      "Iteration 412, loss = 0.18835914\n",
      "Iteration 413, loss = 0.18832488\n",
      "Iteration 414, loss = 0.18824615\n",
      "Iteration 415, loss = 0.18807541\n",
      "Iteration 416, loss = 0.18885964\n",
      "Iteration 417, loss = 0.18809987\n",
      "Iteration 418, loss = 0.18824948\n",
      "Iteration 419, loss = 0.18832749\n",
      "Iteration 420, loss = 0.18788321\n",
      "Iteration 421, loss = 0.18751074\n",
      "Iteration 422, loss = 0.18748718\n",
      "Iteration 423, loss = 0.18757152\n",
      "Iteration 424, loss = 0.18773135\n",
      "Iteration 425, loss = 0.18698425\n",
      "Iteration 426, loss = 0.18695020\n",
      "Iteration 427, loss = 0.18718258\n",
      "Iteration 428, loss = 0.18678414\n",
      "Iteration 429, loss = 0.18696424\n",
      "Iteration 430, loss = 0.18703642\n",
      "Iteration 431, loss = 0.18731160\n",
      "Iteration 432, loss = 0.18634943\n",
      "Iteration 433, loss = 0.18646137\n",
      "Iteration 434, loss = 0.18682227\n",
      "Iteration 435, loss = 0.18633846\n",
      "Iteration 436, loss = 0.18621910\n",
      "Iteration 437, loss = 0.18559219\n",
      "Iteration 438, loss = 0.18635545\n",
      "Iteration 439, loss = 0.18587600\n",
      "Iteration 440, loss = 0.18544441\n",
      "Iteration 441, loss = 0.18542562\n",
      "Iteration 442, loss = 0.18608263\n",
      "Iteration 443, loss = 0.18582193\n",
      "Iteration 444, loss = 0.18529743\n",
      "Iteration 445, loss = 0.18563820\n",
      "Iteration 446, loss = 0.18536296\n",
      "Iteration 447, loss = 0.18521171\n",
      "Iteration 448, loss = 0.18528944\n",
      "Iteration 449, loss = 0.18510156\n",
      "Iteration 450, loss = 0.18467438\n",
      "Iteration 451, loss = 0.18454027\n",
      "Iteration 452, loss = 0.18499081\n",
      "Iteration 453, loss = 0.18471912\n",
      "Iteration 454, loss = 0.18478985\n",
      "Iteration 455, loss = 0.18494945\n",
      "Iteration 456, loss = 0.18474636\n",
      "Iteration 457, loss = 0.18400300\n",
      "Iteration 458, loss = 0.18401317\n",
      "Iteration 459, loss = 0.18418952\n",
      "Iteration 460, loss = 0.18410108\n",
      "Iteration 461, loss = 0.18373596\n",
      "Iteration 462, loss = 0.18387369\n",
      "Iteration 463, loss = 0.18360331\n",
      "Iteration 464, loss = 0.18383532\n",
      "Iteration 465, loss = 0.18365218\n",
      "Iteration 466, loss = 0.18297582\n",
      "Iteration 467, loss = 0.18326440\n",
      "Iteration 468, loss = 0.18342783\n",
      "Iteration 469, loss = 0.18307510\n",
      "Iteration 470, loss = 0.18302701\n",
      "Iteration 471, loss = 0.18328892\n",
      "Iteration 472, loss = 0.18262467\n",
      "Iteration 473, loss = 0.18294903\n",
      "Iteration 474, loss = 0.18241731\n",
      "Iteration 475, loss = 0.18268840\n",
      "Iteration 476, loss = 0.18300460\n",
      "Iteration 477, loss = 0.18221767\n",
      "Iteration 478, loss = 0.18256548\n",
      "Iteration 479, loss = 0.18199965\n",
      "Iteration 480, loss = 0.18262273\n",
      "Iteration 481, loss = 0.18242326\n",
      "Iteration 482, loss = 0.18191077\n",
      "Iteration 483, loss = 0.18220542\n",
      "Iteration 484, loss = 0.18180877\n",
      "Iteration 485, loss = 0.18133970\n",
      "Iteration 486, loss = 0.18250172\n",
      "Iteration 487, loss = 0.18163871\n",
      "Iteration 488, loss = 0.18116856\n",
      "Iteration 489, loss = 0.18139669\n",
      "Iteration 490, loss = 0.18183133\n",
      "Iteration 491, loss = 0.18096026\n",
      "Iteration 492, loss = 0.18121122\n",
      "Iteration 493, loss = 0.18144600\n",
      "Iteration 494, loss = 0.18093872\n",
      "Iteration 495, loss = 0.18103685\n",
      "Iteration 496, loss = 0.18080278\n",
      "Iteration 497, loss = 0.18049381\n",
      "Iteration 498, loss = 0.18061584\n",
      "Iteration 499, loss = 0.18147389\n",
      "Iteration 500, loss = 0.18089476\n",
      "Iteration 501, loss = 0.18058188\n",
      "Iteration 502, loss = 0.18034050\n",
      "Iteration 503, loss = 0.18073542\n",
      "Iteration 504, loss = 0.18105867\n",
      "Iteration 505, loss = 0.18079716\n",
      "Iteration 506, loss = 0.17977022\n",
      "Iteration 507, loss = 0.17983763\n",
      "Iteration 508, loss = 0.18024792\n",
      "Iteration 509, loss = 0.17930526\n",
      "Iteration 510, loss = 0.18052595\n",
      "Iteration 511, loss = 0.17966018\n",
      "Iteration 512, loss = 0.17900370\n",
      "Iteration 513, loss = 0.17954801\n",
      "Iteration 514, loss = 0.17934223\n",
      "Iteration 515, loss = 0.17917386\n",
      "Iteration 516, loss = 0.17860594\n",
      "Iteration 517, loss = 0.17905359\n",
      "Iteration 518, loss = 0.17854073\n",
      "Iteration 519, loss = 0.17837805\n",
      "Iteration 520, loss = 0.17863422\n",
      "Iteration 521, loss = 0.17842045\n",
      "Iteration 522, loss = 0.17789680\n",
      "Iteration 523, loss = 0.17794053\n",
      "Iteration 524, loss = 0.17815495\n",
      "Iteration 525, loss = 0.17840897\n",
      "Iteration 526, loss = 0.17796474\n",
      "Iteration 527, loss = 0.17820336\n",
      "Iteration 528, loss = 0.17723709\n",
      "Iteration 529, loss = 0.17728535\n",
      "Iteration 530, loss = 0.17745452\n",
      "Iteration 531, loss = 0.17819361\n",
      "Iteration 532, loss = 0.17831798\n",
      "Iteration 533, loss = 0.17740726\n",
      "Iteration 534, loss = 0.17743689\n",
      "Iteration 535, loss = 0.17663771\n",
      "Iteration 536, loss = 0.17734178\n",
      "Iteration 537, loss = 0.17634498\n",
      "Iteration 538, loss = 0.17671320\n",
      "Iteration 539, loss = 0.17716677\n",
      "Iteration 540, loss = 0.17682611\n",
      "Iteration 541, loss = 0.17718961\n",
      "Iteration 542, loss = 0.17646659\n",
      "Iteration 543, loss = 0.17632134\n",
      "Iteration 544, loss = 0.17750371\n",
      "Iteration 545, loss = 0.17666195\n",
      "Iteration 546, loss = 0.17616622\n",
      "Iteration 547, loss = 0.17648369\n",
      "Iteration 548, loss = 0.17616624\n",
      "Iteration 549, loss = 0.17560794\n",
      "Iteration 550, loss = 0.17588669\n",
      "Iteration 551, loss = 0.17572906\n",
      "Iteration 552, loss = 0.17604111\n",
      "Iteration 553, loss = 0.17666107\n",
      "Iteration 554, loss = 0.17630986\n",
      "Iteration 555, loss = 0.17508045\n",
      "Iteration 556, loss = 0.17529626\n",
      "Iteration 557, loss = 0.17508539\n",
      "Iteration 558, loss = 0.17519113\n",
      "Iteration 559, loss = 0.17492932\n",
      "Iteration 560, loss = 0.17539931\n",
      "Iteration 561, loss = 0.17501532\n",
      "Iteration 562, loss = 0.17534896\n",
      "Iteration 563, loss = 0.17600262\n",
      "Iteration 564, loss = 0.17513006\n",
      "Iteration 565, loss = 0.17506996\n",
      "Iteration 566, loss = 0.17454015\n",
      "Iteration 567, loss = 0.17462538\n",
      "Iteration 568, loss = 0.17508451\n",
      "Iteration 569, loss = 0.17558710\n",
      "Iteration 570, loss = 0.17462700\n",
      "Iteration 571, loss = 0.17437597\n",
      "Iteration 572, loss = 0.17405772\n",
      "Iteration 573, loss = 0.17422822\n",
      "Iteration 574, loss = 0.17416964\n",
      "Iteration 575, loss = 0.17481752\n",
      "Iteration 576, loss = 0.17384736\n",
      "Iteration 577, loss = 0.17443153\n",
      "Iteration 578, loss = 0.17402241\n",
      "Iteration 579, loss = 0.17440659\n",
      "Iteration 580, loss = 0.17387110\n",
      "Iteration 581, loss = 0.17369256\n",
      "Iteration 582, loss = 0.17427265\n",
      "Iteration 583, loss = 0.17404850\n",
      "Iteration 584, loss = 0.17387846\n",
      "Iteration 585, loss = 0.17354132\n",
      "Iteration 586, loss = 0.17369293\n",
      "Iteration 587, loss = 0.17394788\n",
      "Iteration 588, loss = 0.17331627\n",
      "Iteration 589, loss = 0.17395925\n",
      "Iteration 590, loss = 0.17447141\n",
      "Iteration 591, loss = 0.17352260\n",
      "Iteration 592, loss = 0.17390509\n",
      "Iteration 593, loss = 0.17440472\n",
      "Iteration 594, loss = 0.17312482\n",
      "Iteration 595, loss = 0.17321593\n",
      "Iteration 596, loss = 0.17300848\n",
      "Iteration 597, loss = 0.17304019\n",
      "Iteration 598, loss = 0.17315392\n",
      "Iteration 599, loss = 0.17321172\n",
      "Iteration 600, loss = 0.17365707\n",
      "Iteration 601, loss = 0.17376901\n",
      "Iteration 602, loss = 0.17300642\n",
      "Iteration 603, loss = 0.17334663\n",
      "Iteration 604, loss = 0.17273880\n",
      "Iteration 605, loss = 0.17287012\n",
      "Iteration 606, loss = 0.17286154\n",
      "Iteration 607, loss = 0.17263382\n",
      "Iteration 608, loss = 0.17301957\n",
      "Iteration 609, loss = 0.17273685\n",
      "Iteration 610, loss = 0.17310009\n",
      "Iteration 611, loss = 0.17276939\n",
      "Iteration 612, loss = 0.17266929\n",
      "Iteration 613, loss = 0.17229137\n",
      "Iteration 614, loss = 0.17272343\n",
      "Iteration 615, loss = 0.17282018\n",
      "Iteration 616, loss = 0.17256601\n",
      "Iteration 617, loss = 0.17223532\n",
      "Iteration 618, loss = 0.17193229\n",
      "Iteration 619, loss = 0.17213183\n",
      "Iteration 620, loss = 0.17344547\n",
      "Iteration 621, loss = 0.17169850\n",
      "Iteration 622, loss = 0.17219867\n",
      "Iteration 623, loss = 0.17241333\n",
      "Iteration 624, loss = 0.17180798\n",
      "Iteration 625, loss = 0.17201733\n",
      "Iteration 626, loss = 0.17185767\n",
      "Iteration 627, loss = 0.17154003\n",
      "Iteration 628, loss = 0.17188197\n",
      "Iteration 629, loss = 0.17112668\n",
      "Iteration 630, loss = 0.17147685\n",
      "Iteration 631, loss = 0.17146886\n",
      "Iteration 632, loss = 0.17137063\n",
      "Iteration 633, loss = 0.17169283\n",
      "Iteration 634, loss = 0.17139192\n",
      "Iteration 635, loss = 0.17150050\n",
      "Iteration 636, loss = 0.17155273\n",
      "Iteration 637, loss = 0.17137194\n",
      "Iteration 638, loss = 0.17131672\n",
      "Iteration 639, loss = 0.17124224\n",
      "Iteration 640, loss = 0.17124178\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76924192\n",
      "Iteration 2, loss = 0.74373687\n",
      "Iteration 3, loss = 0.72187940\n",
      "Iteration 4, loss = 0.70452416\n",
      "Iteration 5, loss = 0.69037271\n",
      "Iteration 6, loss = 0.67678359\n",
      "Iteration 7, loss = 0.66322839\n",
      "Iteration 8, loss = 0.64815129\n",
      "Iteration 9, loss = 0.63027273\n",
      "Iteration 10, loss = 0.61142730\n",
      "Iteration 11, loss = 0.59241745\n",
      "Iteration 12, loss = 0.57410231\n",
      "Iteration 13, loss = 0.55806328\n",
      "Iteration 14, loss = 0.54396246\n",
      "Iteration 15, loss = 0.53203311\n",
      "Iteration 16, loss = 0.52164391\n",
      "Iteration 17, loss = 0.51177819\n",
      "Iteration 18, loss = 0.50347886\n",
      "Iteration 19, loss = 0.49581195\n",
      "Iteration 20, loss = 0.48918103\n",
      "Iteration 21, loss = 0.48267002\n",
      "Iteration 22, loss = 0.47706000\n",
      "Iteration 23, loss = 0.47158217\n",
      "Iteration 24, loss = 0.46682028\n",
      "Iteration 25, loss = 0.46175927\n",
      "Iteration 26, loss = 0.45707751\n",
      "Iteration 27, loss = 0.45259070\n",
      "Iteration 28, loss = 0.44800782\n",
      "Iteration 29, loss = 0.44392928\n",
      "Iteration 30, loss = 0.43993874\n",
      "Iteration 31, loss = 0.43586720\n",
      "Iteration 32, loss = 0.43219701\n",
      "Iteration 33, loss = 0.42852539\n",
      "Iteration 34, loss = 0.42530275\n",
      "Iteration 35, loss = 0.42161030\n",
      "Iteration 36, loss = 0.41810549\n",
      "Iteration 37, loss = 0.41481643\n",
      "Iteration 38, loss = 0.41169195\n",
      "Iteration 39, loss = 0.40846378\n",
      "Iteration 40, loss = 0.40545704\n",
      "Iteration 41, loss = 0.40235962\n",
      "Iteration 42, loss = 0.39944479\n",
      "Iteration 43, loss = 0.39676181\n",
      "Iteration 44, loss = 0.39383380\n",
      "Iteration 45, loss = 0.39152568\n",
      "Iteration 46, loss = 0.38838380\n",
      "Iteration 47, loss = 0.38575083\n",
      "Iteration 48, loss = 0.38323688\n",
      "Iteration 49, loss = 0.38064392\n",
      "Iteration 50, loss = 0.37789193\n",
      "Iteration 51, loss = 0.37574129\n",
      "Iteration 52, loss = 0.37327222\n",
      "Iteration 53, loss = 0.37075139\n",
      "Iteration 54, loss = 0.36859387\n",
      "Iteration 55, loss = 0.36626977\n",
      "Iteration 56, loss = 0.36343061\n",
      "Iteration 57, loss = 0.36115727\n",
      "Iteration 58, loss = 0.35901627\n",
      "Iteration 59, loss = 0.35661318\n",
      "Iteration 60, loss = 0.35459735\n",
      "Iteration 61, loss = 0.35250335\n",
      "Iteration 62, loss = 0.35054340\n",
      "Iteration 63, loss = 0.34823160\n",
      "Iteration 64, loss = 0.34625193\n",
      "Iteration 65, loss = 0.34424812\n",
      "Iteration 66, loss = 0.34242478\n",
      "Iteration 67, loss = 0.34057663\n",
      "Iteration 68, loss = 0.33872898\n",
      "Iteration 69, loss = 0.33682936\n",
      "Iteration 70, loss = 0.33508949\n",
      "Iteration 71, loss = 0.33332229\n",
      "Iteration 72, loss = 0.33186677\n",
      "Iteration 73, loss = 0.32975154\n",
      "Iteration 74, loss = 0.32807181\n",
      "Iteration 75, loss = 0.32637092\n",
      "Iteration 76, loss = 0.32517743\n",
      "Iteration 77, loss = 0.32323113\n",
      "Iteration 78, loss = 0.32154487\n",
      "Iteration 79, loss = 0.32016403\n",
      "Iteration 80, loss = 0.31862500\n",
      "Iteration 81, loss = 0.31710658\n",
      "Iteration 82, loss = 0.31542630\n",
      "Iteration 83, loss = 0.31394819\n",
      "Iteration 84, loss = 0.31263914\n",
      "Iteration 85, loss = 0.31087631\n",
      "Iteration 86, loss = 0.30951779\n",
      "Iteration 87, loss = 0.30799175\n",
      "Iteration 88, loss = 0.30686132\n",
      "Iteration 89, loss = 0.30523816\n",
      "Iteration 90, loss = 0.30379467\n",
      "Iteration 91, loss = 0.30248932\n",
      "Iteration 92, loss = 0.30116764\n",
      "Iteration 93, loss = 0.29998191\n",
      "Iteration 94, loss = 0.29908430\n",
      "Iteration 95, loss = 0.29721548\n",
      "Iteration 96, loss = 0.29653825\n",
      "Iteration 97, loss = 0.29501836\n",
      "Iteration 98, loss = 0.29391608\n",
      "Iteration 99, loss = 0.29244790\n",
      "Iteration 100, loss = 0.29126441\n",
      "Iteration 101, loss = 0.29037777\n",
      "Iteration 102, loss = 0.28893481\n",
      "Iteration 103, loss = 0.28787765\n",
      "Iteration 104, loss = 0.28698235\n",
      "Iteration 105, loss = 0.28573740\n",
      "Iteration 106, loss = 0.28479870\n",
      "Iteration 107, loss = 0.28362834\n",
      "Iteration 108, loss = 0.28258867\n",
      "Iteration 109, loss = 0.28191362\n",
      "Iteration 110, loss = 0.28060409\n",
      "Iteration 111, loss = 0.27958295\n",
      "Iteration 112, loss = 0.27857194\n",
      "Iteration 113, loss = 0.27751668\n",
      "Iteration 114, loss = 0.27610568\n",
      "Iteration 115, loss = 0.27573432\n",
      "Iteration 116, loss = 0.27449838\n",
      "Iteration 117, loss = 0.27354198\n",
      "Iteration 118, loss = 0.27236738\n",
      "Iteration 119, loss = 0.27128642\n",
      "Iteration 120, loss = 0.27041265\n",
      "Iteration 121, loss = 0.27022808\n",
      "Iteration 122, loss = 0.26905901\n",
      "Iteration 123, loss = 0.26782030\n",
      "Iteration 124, loss = 0.26711174\n",
      "Iteration 125, loss = 0.26624031\n",
      "Iteration 126, loss = 0.26531775\n",
      "Iteration 127, loss = 0.26471523\n",
      "Iteration 128, loss = 0.26375769\n",
      "Iteration 129, loss = 0.26314025\n",
      "Iteration 130, loss = 0.26280864\n",
      "Iteration 131, loss = 0.26181503\n",
      "Iteration 132, loss = 0.26094207\n",
      "Iteration 133, loss = 0.26026664\n",
      "Iteration 134, loss = 0.25924013\n",
      "Iteration 135, loss = 0.25872385\n",
      "Iteration 136, loss = 0.25827127\n",
      "Iteration 137, loss = 0.25729474\n",
      "Iteration 138, loss = 0.25655560\n",
      "Iteration 139, loss = 0.25589798\n",
      "Iteration 140, loss = 0.25514487\n",
      "Iteration 141, loss = 0.25440722\n",
      "Iteration 142, loss = 0.25395718\n",
      "Iteration 143, loss = 0.25311735\n",
      "Iteration 144, loss = 0.25256805\n",
      "Iteration 145, loss = 0.25172829\n",
      "Iteration 146, loss = 0.25100575\n",
      "Iteration 147, loss = 0.25061085\n",
      "Iteration 148, loss = 0.24990142\n",
      "Iteration 149, loss = 0.24926144\n",
      "Iteration 150, loss = 0.24872244\n",
      "Iteration 151, loss = 0.24876960\n",
      "Iteration 152, loss = 0.24865795\n",
      "Iteration 153, loss = 0.24713269\n",
      "Iteration 154, loss = 0.24686439\n",
      "Iteration 155, loss = 0.24602414\n",
      "Iteration 156, loss = 0.24527608\n",
      "Iteration 157, loss = 0.24490747\n",
      "Iteration 158, loss = 0.24454101\n",
      "Iteration 159, loss = 0.24406264\n",
      "Iteration 160, loss = 0.24355914\n",
      "Iteration 161, loss = 0.24248048\n",
      "Iteration 162, loss = 0.24183137\n",
      "Iteration 163, loss = 0.24127037\n",
      "Iteration 164, loss = 0.24092160\n",
      "Iteration 165, loss = 0.24033626\n",
      "Iteration 166, loss = 0.23980145\n",
      "Iteration 167, loss = 0.23983068\n",
      "Iteration 168, loss = 0.23908050\n",
      "Iteration 169, loss = 0.23778911\n",
      "Iteration 170, loss = 0.23780006\n",
      "Iteration 171, loss = 0.23705962\n",
      "Iteration 172, loss = 0.23667489\n",
      "Iteration 173, loss = 0.23635928\n",
      "Iteration 174, loss = 0.23557657\n",
      "Iteration 175, loss = 0.23489204\n",
      "Iteration 176, loss = 0.23406464\n",
      "Iteration 177, loss = 0.23362895\n",
      "Iteration 178, loss = 0.23382591\n",
      "Iteration 179, loss = 0.23295632\n",
      "Iteration 180, loss = 0.23206483\n",
      "Iteration 181, loss = 0.23198423\n",
      "Iteration 182, loss = 0.23106447\n",
      "Iteration 183, loss = 0.23085303\n",
      "Iteration 184, loss = 0.23027613\n",
      "Iteration 185, loss = 0.22955643\n",
      "Iteration 186, loss = 0.22942559\n",
      "Iteration 187, loss = 0.22886400\n",
      "Iteration 188, loss = 0.22826320\n",
      "Iteration 189, loss = 0.22792464\n",
      "Iteration 190, loss = 0.22732320\n",
      "Iteration 191, loss = 0.22702668\n",
      "Iteration 192, loss = 0.22656283\n",
      "Iteration 193, loss = 0.22617780\n",
      "Iteration 194, loss = 0.22700482\n",
      "Iteration 195, loss = 0.22546314\n",
      "Iteration 196, loss = 0.22502111\n",
      "Iteration 197, loss = 0.22474511\n",
      "Iteration 198, loss = 0.22440847\n",
      "Iteration 199, loss = 0.22383711\n",
      "Iteration 200, loss = 0.22351092\n",
      "Iteration 201, loss = 0.22318261\n",
      "Iteration 202, loss = 0.22290033\n",
      "Iteration 203, loss = 0.22209463\n",
      "Iteration 204, loss = 0.22161297\n",
      "Iteration 205, loss = 0.22188199\n",
      "Iteration 206, loss = 0.22123486\n",
      "Iteration 207, loss = 0.22072805\n",
      "Iteration 208, loss = 0.21999833\n",
      "Iteration 209, loss = 0.21964903\n",
      "Iteration 210, loss = 0.21990274\n",
      "Iteration 211, loss = 0.21894025\n",
      "Iteration 212, loss = 0.21837265\n",
      "Iteration 213, loss = 0.21786953\n",
      "Iteration 214, loss = 0.21741592\n",
      "Iteration 215, loss = 0.21753060\n",
      "Iteration 216, loss = 0.21666531\n",
      "Iteration 217, loss = 0.21590919\n",
      "Iteration 218, loss = 0.21606220\n",
      "Iteration 219, loss = 0.21549289\n",
      "Iteration 220, loss = 0.21511696\n",
      "Iteration 221, loss = 0.21452061\n",
      "Iteration 222, loss = 0.21476614\n",
      "Iteration 223, loss = 0.21432082\n",
      "Iteration 224, loss = 0.21392953\n",
      "Iteration 225, loss = 0.21357011\n",
      "Iteration 226, loss = 0.21353330\n",
      "Iteration 227, loss = 0.21283862\n",
      "Iteration 228, loss = 0.21270744\n",
      "Iteration 229, loss = 0.21238007\n",
      "Iteration 230, loss = 0.21187474\n",
      "Iteration 231, loss = 0.21192892\n",
      "Iteration 232, loss = 0.21128907\n",
      "Iteration 233, loss = 0.21091885\n",
      "Iteration 234, loss = 0.21088226\n",
      "Iteration 235, loss = 0.21048505\n",
      "Iteration 236, loss = 0.21028006\n",
      "Iteration 237, loss = 0.21005244\n",
      "Iteration 238, loss = 0.20974301\n",
      "Iteration 239, loss = 0.20914735\n",
      "Iteration 240, loss = 0.20968300\n",
      "Iteration 241, loss = 0.20913980\n",
      "Iteration 242, loss = 0.20853862\n",
      "Iteration 243, loss = 0.20908378\n",
      "Iteration 244, loss = 0.20844194\n",
      "Iteration 245, loss = 0.20820469\n",
      "Iteration 246, loss = 0.20805716\n",
      "Iteration 247, loss = 0.20774736\n",
      "Iteration 248, loss = 0.20743896\n",
      "Iteration 249, loss = 0.20728458\n",
      "Iteration 250, loss = 0.20700546\n",
      "Iteration 251, loss = 0.20675228\n",
      "Iteration 252, loss = 0.20633219\n",
      "Iteration 253, loss = 0.20677238\n",
      "Iteration 254, loss = 0.20633089\n",
      "Iteration 255, loss = 0.20573322\n",
      "Iteration 256, loss = 0.20600680\n",
      "Iteration 257, loss = 0.20575243\n",
      "Iteration 258, loss = 0.20557022\n",
      "Iteration 259, loss = 0.20522979\n",
      "Iteration 260, loss = 0.20536376\n",
      "Iteration 261, loss = 0.20505730\n",
      "Iteration 262, loss = 0.20541187\n",
      "Iteration 263, loss = 0.20457978\n",
      "Iteration 264, loss = 0.20437401\n",
      "Iteration 265, loss = 0.20430640\n",
      "Iteration 266, loss = 0.20402096\n",
      "Iteration 267, loss = 0.20374793\n",
      "Iteration 268, loss = 0.20421852\n",
      "Iteration 269, loss = 0.20368584\n",
      "Iteration 270, loss = 0.20351936\n",
      "Iteration 271, loss = 0.20340791\n",
      "Iteration 272, loss = 0.20263387\n",
      "Iteration 273, loss = 0.20279945\n",
      "Iteration 274, loss = 0.20239510\n",
      "Iteration 275, loss = 0.20255965\n",
      "Iteration 276, loss = 0.20240071\n",
      "Iteration 277, loss = 0.20214876\n",
      "Iteration 278, loss = 0.20221076\n",
      "Iteration 279, loss = 0.20219278\n",
      "Iteration 280, loss = 0.20156474\n",
      "Iteration 281, loss = 0.20160146\n",
      "Iteration 282, loss = 0.20162748\n",
      "Iteration 283, loss = 0.20160308\n",
      "Iteration 284, loss = 0.20128521\n",
      "Iteration 285, loss = 0.20101502\n",
      "Iteration 286, loss = 0.20117960\n",
      "Iteration 287, loss = 0.20115703\n",
      "Iteration 288, loss = 0.20192489\n",
      "Iteration 289, loss = 0.20057594\n",
      "Iteration 290, loss = 0.20051677\n",
      "Iteration 291, loss = 0.20064483\n",
      "Iteration 292, loss = 0.20014852\n",
      "Iteration 293, loss = 0.20007114\n",
      "Iteration 294, loss = 0.19979806\n",
      "Iteration 295, loss = 0.20139614\n",
      "Iteration 296, loss = 0.19997926\n",
      "Iteration 297, loss = 0.19944965\n",
      "Iteration 298, loss = 0.19975206\n",
      "Iteration 299, loss = 0.19933110\n",
      "Iteration 300, loss = 0.19941296\n",
      "Iteration 301, loss = 0.20013656\n",
      "Iteration 302, loss = 0.20012279\n",
      "Iteration 303, loss = 0.19864766\n",
      "Iteration 304, loss = 0.19916307\n",
      "Iteration 305, loss = 0.19898690\n",
      "Iteration 306, loss = 0.19860438\n",
      "Iteration 307, loss = 0.19893480\n",
      "Iteration 308, loss = 0.19859265\n",
      "Iteration 309, loss = 0.19868336\n",
      "Iteration 310, loss = 0.19827947\n",
      "Iteration 311, loss = 0.19811304\n",
      "Iteration 312, loss = 0.19837093\n",
      "Iteration 313, loss = 0.19823301\n",
      "Iteration 314, loss = 0.19768276\n",
      "Iteration 315, loss = 0.19765707\n",
      "Iteration 316, loss = 0.19777838\n",
      "Iteration 317, loss = 0.19784824\n",
      "Iteration 318, loss = 0.19753559\n",
      "Iteration 319, loss = 0.19750650\n",
      "Iteration 320, loss = 0.19743194\n",
      "Iteration 321, loss = 0.19738307\n",
      "Iteration 322, loss = 0.19724709\n",
      "Iteration 323, loss = 0.19706732\n",
      "Iteration 324, loss = 0.19694338\n",
      "Iteration 325, loss = 0.19726616\n",
      "Iteration 326, loss = 0.19716697\n",
      "Iteration 327, loss = 0.19709637\n",
      "Iteration 328, loss = 0.19743348\n",
      "Iteration 329, loss = 0.19679992\n",
      "Iteration 330, loss = 0.19658564\n",
      "Iteration 331, loss = 0.19654147\n",
      "Iteration 332, loss = 0.19692219\n",
      "Iteration 333, loss = 0.19675215\n",
      "Iteration 334, loss = 0.19578937\n",
      "Iteration 335, loss = 0.19637503\n",
      "Iteration 336, loss = 0.19599106\n",
      "Iteration 337, loss = 0.19604265\n",
      "Iteration 338, loss = 0.19602344\n",
      "Iteration 339, loss = 0.19567970\n",
      "Iteration 340, loss = 0.19551883\n",
      "Iteration 341, loss = 0.19560529\n",
      "Iteration 342, loss = 0.19534679\n",
      "Iteration 343, loss = 0.19561505\n",
      "Iteration 344, loss = 0.19538721\n",
      "Iteration 345, loss = 0.19501767\n",
      "Iteration 346, loss = 0.19529277\n",
      "Iteration 347, loss = 0.19506070\n",
      "Iteration 348, loss = 0.19475812\n",
      "Iteration 349, loss = 0.19516260\n",
      "Iteration 350, loss = 0.19456006\n",
      "Iteration 351, loss = 0.19487880\n",
      "Iteration 352, loss = 0.19445636\n",
      "Iteration 353, loss = 0.19464919\n",
      "Iteration 354, loss = 0.19421178\n",
      "Iteration 355, loss = 0.19447430\n",
      "Iteration 356, loss = 0.19412032\n",
      "Iteration 357, loss = 0.19426681\n",
      "Iteration 358, loss = 0.19397859\n",
      "Iteration 359, loss = 0.19422277\n",
      "Iteration 360, loss = 0.19377022\n",
      "Iteration 361, loss = 0.19447663\n",
      "Iteration 362, loss = 0.19373921\n",
      "Iteration 363, loss = 0.19344635\n",
      "Iteration 364, loss = 0.19354170\n",
      "Iteration 365, loss = 0.19353541\n",
      "Iteration 366, loss = 0.19336051\n",
      "Iteration 367, loss = 0.19306598\n",
      "Iteration 368, loss = 0.19362827\n",
      "Iteration 369, loss = 0.19329034\n",
      "Iteration 370, loss = 0.19318051\n",
      "Iteration 371, loss = 0.19298406\n",
      "Iteration 372, loss = 0.19312718\n",
      "Iteration 373, loss = 0.19309154\n",
      "Iteration 374, loss = 0.19275017\n",
      "Iteration 375, loss = 0.19353853\n",
      "Iteration 376, loss = 0.19394435\n",
      "Iteration 377, loss = 0.19213261\n",
      "Iteration 378, loss = 0.19218845\n",
      "Iteration 379, loss = 0.19186382\n",
      "Iteration 380, loss = 0.19188922\n",
      "Iteration 381, loss = 0.19234262\n",
      "Iteration 382, loss = 0.19215131\n",
      "Iteration 383, loss = 0.19149375\n",
      "Iteration 384, loss = 0.19144594\n",
      "Iteration 385, loss = 0.19126757\n",
      "Iteration 386, loss = 0.19121974\n",
      "Iteration 387, loss = 0.19138939\n",
      "Iteration 388, loss = 0.19106615\n",
      "Iteration 389, loss = 0.19087500\n",
      "Iteration 390, loss = 0.19135329\n",
      "Iteration 391, loss = 0.19059905\n",
      "Iteration 392, loss = 0.19096134\n",
      "Iteration 393, loss = 0.19018187\n",
      "Iteration 394, loss = 0.19061767\n",
      "Iteration 395, loss = 0.19028595\n",
      "Iteration 396, loss = 0.19029964\n",
      "Iteration 397, loss = 0.19003053\n",
      "Iteration 398, loss = 0.19032923\n",
      "Iteration 399, loss = 0.18995641\n",
      "Iteration 400, loss = 0.19007642\n",
      "Iteration 401, loss = 0.19012635\n",
      "Iteration 402, loss = 0.18992856\n",
      "Iteration 403, loss = 0.18950376\n",
      "Iteration 404, loss = 0.18951897\n",
      "Iteration 405, loss = 0.18940969\n",
      "Iteration 406, loss = 0.18939998\n",
      "Iteration 407, loss = 0.18939334\n",
      "Iteration 408, loss = 0.18924337\n",
      "Iteration 409, loss = 0.18927165\n",
      "Iteration 410, loss = 0.18918622\n",
      "Iteration 411, loss = 0.18887727\n",
      "Iteration 412, loss = 0.18890496\n",
      "Iteration 413, loss = 0.18870148\n",
      "Iteration 414, loss = 0.18897677\n",
      "Iteration 415, loss = 0.18848362\n",
      "Iteration 416, loss = 0.18916949\n",
      "Iteration 417, loss = 0.18846932\n",
      "Iteration 418, loss = 0.18849357\n",
      "Iteration 419, loss = 0.18865667\n",
      "Iteration 420, loss = 0.18841398\n",
      "Iteration 421, loss = 0.18793374\n",
      "Iteration 422, loss = 0.18799530\n",
      "Iteration 423, loss = 0.18769424\n",
      "Iteration 424, loss = 0.18819767\n",
      "Iteration 425, loss = 0.18815697\n",
      "Iteration 426, loss = 0.18744473\n",
      "Iteration 427, loss = 0.18740107\n",
      "Iteration 428, loss = 0.18757554\n",
      "Iteration 429, loss = 0.18724533\n",
      "Iteration 430, loss = 0.18717040\n",
      "Iteration 431, loss = 0.18739257\n",
      "Iteration 432, loss = 0.18762005\n",
      "Iteration 433, loss = 0.18729632\n",
      "Iteration 434, loss = 0.18711270\n",
      "Iteration 435, loss = 0.18733934\n",
      "Iteration 436, loss = 0.18726736\n",
      "Iteration 437, loss = 0.18750725\n",
      "Iteration 438, loss = 0.18727528\n",
      "Iteration 439, loss = 0.18671388\n",
      "Iteration 440, loss = 0.18734193\n",
      "Iteration 441, loss = 0.18723838\n",
      "Iteration 442, loss = 0.18672758\n",
      "Iteration 443, loss = 0.18652028\n",
      "Iteration 444, loss = 0.18652280\n",
      "Iteration 445, loss = 0.18706896\n",
      "Iteration 446, loss = 0.18652392\n",
      "Iteration 447, loss = 0.18657156\n",
      "Iteration 448, loss = 0.18644088\n",
      "Iteration 449, loss = 0.18649947\n",
      "Iteration 450, loss = 0.18649519\n",
      "Iteration 451, loss = 0.18624873\n",
      "Iteration 452, loss = 0.18619630\n",
      "Iteration 453, loss = 0.18619103\n",
      "Iteration 454, loss = 0.18617604\n",
      "Iteration 455, loss = 0.18609918\n",
      "Iteration 456, loss = 0.18600789\n",
      "Iteration 457, loss = 0.18605561\n",
      "Iteration 458, loss = 0.18637790\n",
      "Iteration 459, loss = 0.18616487\n",
      "Iteration 460, loss = 0.18604530\n",
      "Iteration 461, loss = 0.18622457\n",
      "Iteration 462, loss = 0.18586453\n",
      "Iteration 463, loss = 0.18658003\n",
      "Iteration 464, loss = 0.18637809\n",
      "Iteration 465, loss = 0.18606010\n",
      "Iteration 466, loss = 0.18584883\n",
      "Iteration 467, loss = 0.18502582\n",
      "Iteration 468, loss = 0.18536058\n",
      "Iteration 469, loss = 0.18529774\n",
      "Iteration 470, loss = 0.18566547\n",
      "Iteration 471, loss = 0.18511917\n",
      "Iteration 472, loss = 0.18525101\n",
      "Iteration 473, loss = 0.18514359\n",
      "Iteration 474, loss = 0.18533182\n",
      "Iteration 475, loss = 0.18473570\n",
      "Iteration 476, loss = 0.18489407\n",
      "Iteration 477, loss = 0.18488041\n",
      "Iteration 478, loss = 0.18472937\n",
      "Iteration 479, loss = 0.18507064\n",
      "Iteration 480, loss = 0.18474037\n",
      "Iteration 481, loss = 0.18473299\n",
      "Iteration 482, loss = 0.18468122\n",
      "Iteration 483, loss = 0.18517716\n",
      "Iteration 484, loss = 0.18503624\n",
      "Iteration 485, loss = 0.18533579\n",
      "Iteration 486, loss = 0.18513369\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77141771\n",
      "Iteration 2, loss = 0.74520021\n",
      "Iteration 3, loss = 0.72374383\n",
      "Iteration 4, loss = 0.70577797\n",
      "Iteration 5, loss = 0.69148628\n",
      "Iteration 6, loss = 0.67897163\n",
      "Iteration 7, loss = 0.66653656\n",
      "Iteration 8, loss = 0.65297822\n",
      "Iteration 9, loss = 0.63671033\n",
      "Iteration 10, loss = 0.61857106\n",
      "Iteration 11, loss = 0.59969565\n",
      "Iteration 12, loss = 0.58063567\n",
      "Iteration 13, loss = 0.56434920\n",
      "Iteration 14, loss = 0.54904028\n",
      "Iteration 15, loss = 0.53645517\n",
      "Iteration 16, loss = 0.52537081\n",
      "Iteration 17, loss = 0.51575633\n",
      "Iteration 18, loss = 0.50710205\n",
      "Iteration 19, loss = 0.49921472\n",
      "Iteration 20, loss = 0.49213752\n",
      "Iteration 21, loss = 0.48559379\n",
      "Iteration 22, loss = 0.47939713\n",
      "Iteration 23, loss = 0.47356232\n",
      "Iteration 24, loss = 0.46817871\n",
      "Iteration 25, loss = 0.46321457\n",
      "Iteration 26, loss = 0.45849818\n",
      "Iteration 27, loss = 0.45423449\n",
      "Iteration 28, loss = 0.44986961\n",
      "Iteration 29, loss = 0.44589605\n",
      "Iteration 30, loss = 0.44184197\n",
      "Iteration 31, loss = 0.43800486\n",
      "Iteration 32, loss = 0.43429156\n",
      "Iteration 33, loss = 0.43072757\n",
      "Iteration 34, loss = 0.42719814\n",
      "Iteration 35, loss = 0.42372014\n",
      "Iteration 36, loss = 0.42053728\n",
      "Iteration 37, loss = 0.41702620\n",
      "Iteration 38, loss = 0.41377169\n",
      "Iteration 39, loss = 0.41074409\n",
      "Iteration 40, loss = 0.40781276\n",
      "Iteration 41, loss = 0.40454358\n",
      "Iteration 42, loss = 0.40181733\n",
      "Iteration 43, loss = 0.39878061\n",
      "Iteration 44, loss = 0.39612041\n",
      "Iteration 45, loss = 0.39345254\n",
      "Iteration 46, loss = 0.39067429\n",
      "Iteration 47, loss = 0.38811257\n",
      "Iteration 48, loss = 0.38550315\n",
      "Iteration 49, loss = 0.38293103\n",
      "Iteration 50, loss = 0.38043703\n",
      "Iteration 51, loss = 0.37802616\n",
      "Iteration 52, loss = 0.37565209\n",
      "Iteration 53, loss = 0.37347199\n",
      "Iteration 54, loss = 0.37119748\n",
      "Iteration 55, loss = 0.36905931\n",
      "Iteration 56, loss = 0.36680632\n",
      "Iteration 57, loss = 0.36488870\n",
      "Iteration 58, loss = 0.36283533\n",
      "Iteration 59, loss = 0.36081412\n",
      "Iteration 60, loss = 0.35890897\n",
      "Iteration 61, loss = 0.35677083\n",
      "Iteration 62, loss = 0.35471166\n",
      "Iteration 63, loss = 0.35265742\n",
      "Iteration 64, loss = 0.35069620\n",
      "Iteration 65, loss = 0.34891833\n",
      "Iteration 66, loss = 0.34679510\n",
      "Iteration 67, loss = 0.34513316\n",
      "Iteration 68, loss = 0.34319952\n",
      "Iteration 69, loss = 0.34151346\n",
      "Iteration 70, loss = 0.34007799\n",
      "Iteration 71, loss = 0.33823011\n",
      "Iteration 72, loss = 0.33650724\n",
      "Iteration 73, loss = 0.33489538\n",
      "Iteration 74, loss = 0.33316577\n",
      "Iteration 75, loss = 0.33161116\n",
      "Iteration 76, loss = 0.33048716\n",
      "Iteration 77, loss = 0.32861895\n",
      "Iteration 78, loss = 0.32733750\n",
      "Iteration 79, loss = 0.32556031\n",
      "Iteration 80, loss = 0.32422115\n",
      "Iteration 81, loss = 0.32264950\n",
      "Iteration 82, loss = 0.32131212\n",
      "Iteration 83, loss = 0.31976101\n",
      "Iteration 84, loss = 0.31861456\n",
      "Iteration 85, loss = 0.31716294\n",
      "Iteration 86, loss = 0.31557379\n",
      "Iteration 87, loss = 0.31451741\n",
      "Iteration 88, loss = 0.31343045\n",
      "Iteration 89, loss = 0.31192093\n",
      "Iteration 90, loss = 0.31042250\n",
      "Iteration 91, loss = 0.30930275\n",
      "Iteration 92, loss = 0.30799987\n",
      "Iteration 93, loss = 0.30690085\n",
      "Iteration 94, loss = 0.30576973\n",
      "Iteration 95, loss = 0.30445202\n",
      "Iteration 96, loss = 0.30347219\n",
      "Iteration 97, loss = 0.30222814\n",
      "Iteration 98, loss = 0.30115206\n",
      "Iteration 99, loss = 0.29995519\n",
      "Iteration 100, loss = 0.29864800\n",
      "Iteration 101, loss = 0.29802745\n",
      "Iteration 102, loss = 0.29664473\n",
      "Iteration 103, loss = 0.29567699\n",
      "Iteration 104, loss = 0.29468194\n",
      "Iteration 105, loss = 0.29420531\n",
      "Iteration 106, loss = 0.29221542\n",
      "Iteration 107, loss = 0.29189173\n",
      "Iteration 108, loss = 0.29075861\n",
      "Iteration 109, loss = 0.29001777\n",
      "Iteration 110, loss = 0.28826853\n",
      "Iteration 111, loss = 0.28741335\n",
      "Iteration 112, loss = 0.28613373\n",
      "Iteration 113, loss = 0.28519660\n",
      "Iteration 114, loss = 0.28416827\n",
      "Iteration 115, loss = 0.28309127\n",
      "Iteration 116, loss = 0.28213651\n",
      "Iteration 117, loss = 0.28172186\n",
      "Iteration 118, loss = 0.28046148\n",
      "Iteration 119, loss = 0.27981469\n",
      "Iteration 120, loss = 0.27877320\n",
      "Iteration 121, loss = 0.27775852\n",
      "Iteration 122, loss = 0.27706646\n",
      "Iteration 123, loss = 0.27592178\n",
      "Iteration 124, loss = 0.27511422\n",
      "Iteration 125, loss = 0.27418039\n",
      "Iteration 126, loss = 0.27320104\n",
      "Iteration 127, loss = 0.27254415\n",
      "Iteration 128, loss = 0.27227065\n",
      "Iteration 129, loss = 0.27098763\n",
      "Iteration 130, loss = 0.27067369\n",
      "Iteration 131, loss = 0.26922505\n",
      "Iteration 132, loss = 0.26848106\n",
      "Iteration 133, loss = 0.26757531\n",
      "Iteration 134, loss = 0.26696899\n",
      "Iteration 135, loss = 0.26619086\n",
      "Iteration 136, loss = 0.26552540\n",
      "Iteration 137, loss = 0.26512906\n",
      "Iteration 138, loss = 0.26380869\n",
      "Iteration 139, loss = 0.26307372\n",
      "Iteration 140, loss = 0.26223674\n",
      "Iteration 141, loss = 0.26159702\n",
      "Iteration 142, loss = 0.26095120\n",
      "Iteration 143, loss = 0.25998634\n",
      "Iteration 144, loss = 0.25940493\n",
      "Iteration 145, loss = 0.25884238\n",
      "Iteration 146, loss = 0.25799415\n",
      "Iteration 147, loss = 0.25736145\n",
      "Iteration 148, loss = 0.25676663\n",
      "Iteration 149, loss = 0.25601472\n",
      "Iteration 150, loss = 0.25552433\n",
      "Iteration 151, loss = 0.25482164\n",
      "Iteration 152, loss = 0.25422394\n",
      "Iteration 153, loss = 0.25367234\n",
      "Iteration 154, loss = 0.25348897\n",
      "Iteration 155, loss = 0.25247347\n",
      "Iteration 156, loss = 0.25200503\n",
      "Iteration 157, loss = 0.25117239\n",
      "Iteration 158, loss = 0.25067551\n",
      "Iteration 159, loss = 0.25025931\n",
      "Iteration 160, loss = 0.25014002\n",
      "Iteration 161, loss = 0.24919559\n",
      "Iteration 162, loss = 0.24852831\n",
      "Iteration 163, loss = 0.24793926\n",
      "Iteration 164, loss = 0.24722253\n",
      "Iteration 165, loss = 0.24718832\n",
      "Iteration 166, loss = 0.24619220\n",
      "Iteration 167, loss = 0.24583222\n",
      "Iteration 168, loss = 0.24554247\n",
      "Iteration 169, loss = 0.24474217\n",
      "Iteration 170, loss = 0.24469751\n",
      "Iteration 171, loss = 0.24395401\n",
      "Iteration 172, loss = 0.24356640\n",
      "Iteration 173, loss = 0.24277449\n",
      "Iteration 174, loss = 0.24242612\n",
      "Iteration 175, loss = 0.24188390\n",
      "Iteration 176, loss = 0.24138698\n",
      "Iteration 177, loss = 0.24125524\n",
      "Iteration 178, loss = 0.24054156\n",
      "Iteration 179, loss = 0.24010950\n",
      "Iteration 180, loss = 0.23913731\n",
      "Iteration 181, loss = 0.23859429\n",
      "Iteration 182, loss = 0.23829351\n",
      "Iteration 183, loss = 0.23766867\n",
      "Iteration 184, loss = 0.23771437\n",
      "Iteration 185, loss = 0.23676323\n",
      "Iteration 186, loss = 0.23644842\n",
      "Iteration 187, loss = 0.23626077\n",
      "Iteration 188, loss = 0.23585251\n",
      "Iteration 189, loss = 0.23485458\n",
      "Iteration 190, loss = 0.23469279\n",
      "Iteration 191, loss = 0.23447186\n",
      "Iteration 192, loss = 0.23362507\n",
      "Iteration 193, loss = 0.23336033\n",
      "Iteration 194, loss = 0.23275155\n",
      "Iteration 195, loss = 0.23189588\n",
      "Iteration 196, loss = 0.23191778\n",
      "Iteration 197, loss = 0.23151349\n",
      "Iteration 198, loss = 0.23094416\n",
      "Iteration 199, loss = 0.23033657\n",
      "Iteration 200, loss = 0.22993358\n",
      "Iteration 201, loss = 0.22976293\n",
      "Iteration 202, loss = 0.22921556\n",
      "Iteration 203, loss = 0.22914139\n",
      "Iteration 204, loss = 0.22813874\n",
      "Iteration 205, loss = 0.22826589\n",
      "Iteration 206, loss = 0.22768299\n",
      "Iteration 207, loss = 0.22739006\n",
      "Iteration 208, loss = 0.22715746\n",
      "Iteration 209, loss = 0.22612510\n",
      "Iteration 210, loss = 0.22588873\n",
      "Iteration 211, loss = 0.22578667\n",
      "Iteration 212, loss = 0.22532200\n",
      "Iteration 213, loss = 0.22502207\n",
      "Iteration 214, loss = 0.22451981\n",
      "Iteration 215, loss = 0.22440455\n",
      "Iteration 216, loss = 0.22361861\n",
      "Iteration 217, loss = 0.22348162\n",
      "Iteration 218, loss = 0.22320996\n",
      "Iteration 219, loss = 0.22298129\n",
      "Iteration 220, loss = 0.22283646\n",
      "Iteration 221, loss = 0.22262204\n",
      "Iteration 222, loss = 0.22195360\n",
      "Iteration 223, loss = 0.22164004\n",
      "Iteration 224, loss = 0.22114285\n",
      "Iteration 225, loss = 0.22123514\n",
      "Iteration 226, loss = 0.22037341\n",
      "Iteration 227, loss = 0.22026916\n",
      "Iteration 228, loss = 0.22010017\n",
      "Iteration 229, loss = 0.21951281\n",
      "Iteration 230, loss = 0.21937467\n",
      "Iteration 231, loss = 0.21941553\n",
      "Iteration 232, loss = 0.21873439\n",
      "Iteration 233, loss = 0.21841922\n",
      "Iteration 234, loss = 0.21799961\n",
      "Iteration 235, loss = 0.21803344\n",
      "Iteration 236, loss = 0.21770943\n",
      "Iteration 237, loss = 0.21717479\n",
      "Iteration 238, loss = 0.21712247\n",
      "Iteration 239, loss = 0.21709948\n",
      "Iteration 240, loss = 0.21714080\n",
      "Iteration 241, loss = 0.21596268\n",
      "Iteration 242, loss = 0.21561821\n",
      "Iteration 243, loss = 0.21545875\n",
      "Iteration 244, loss = 0.21485669\n",
      "Iteration 245, loss = 0.21551003\n",
      "Iteration 246, loss = 0.21469036\n",
      "Iteration 247, loss = 0.21417517\n",
      "Iteration 248, loss = 0.21416330\n",
      "Iteration 249, loss = 0.21382247\n",
      "Iteration 250, loss = 0.21364407\n",
      "Iteration 251, loss = 0.21300782\n",
      "Iteration 252, loss = 0.21271629\n",
      "Iteration 253, loss = 0.21227215\n",
      "Iteration 254, loss = 0.21193147\n",
      "Iteration 255, loss = 0.21204727\n",
      "Iteration 256, loss = 0.21125742\n",
      "Iteration 257, loss = 0.21109979\n",
      "Iteration 258, loss = 0.21110076\n",
      "Iteration 259, loss = 0.21066794\n",
      "Iteration 260, loss = 0.21042517\n",
      "Iteration 261, loss = 0.21017782\n",
      "Iteration 262, loss = 0.20957734\n",
      "Iteration 263, loss = 0.20937446\n",
      "Iteration 264, loss = 0.20914511\n",
      "Iteration 265, loss = 0.20896052\n",
      "Iteration 266, loss = 0.20857533\n",
      "Iteration 267, loss = 0.20824003\n",
      "Iteration 268, loss = 0.20827010\n",
      "Iteration 269, loss = 0.20824602\n",
      "Iteration 270, loss = 0.20788856\n",
      "Iteration 271, loss = 0.20747800\n",
      "Iteration 272, loss = 0.20746934\n",
      "Iteration 273, loss = 0.20705746\n",
      "Iteration 274, loss = 0.20656577\n",
      "Iteration 275, loss = 0.20645448\n",
      "Iteration 276, loss = 0.20603069\n",
      "Iteration 277, loss = 0.20651398\n",
      "Iteration 278, loss = 0.20546921\n",
      "Iteration 279, loss = 0.20489462\n",
      "Iteration 280, loss = 0.20459722\n",
      "Iteration 281, loss = 0.20424995\n",
      "Iteration 282, loss = 0.20492388\n",
      "Iteration 283, loss = 0.20407886\n",
      "Iteration 284, loss = 0.20407671\n",
      "Iteration 285, loss = 0.20385187\n",
      "Iteration 286, loss = 0.20336079\n",
      "Iteration 287, loss = 0.20267941\n",
      "Iteration 288, loss = 0.20311552\n",
      "Iteration 289, loss = 0.20237415\n",
      "Iteration 290, loss = 0.20238949\n",
      "Iteration 291, loss = 0.20186184\n",
      "Iteration 292, loss = 0.20223278\n",
      "Iteration 293, loss = 0.20184997\n",
      "Iteration 294, loss = 0.20141797\n",
      "Iteration 295, loss = 0.20113948\n",
      "Iteration 296, loss = 0.20107716\n",
      "Iteration 297, loss = 0.20213676\n",
      "Iteration 298, loss = 0.20070385\n",
      "Iteration 299, loss = 0.20175204\n",
      "Iteration 300, loss = 0.20032047\n",
      "Iteration 301, loss = 0.19990733\n",
      "Iteration 302, loss = 0.19990896\n",
      "Iteration 303, loss = 0.19919950\n",
      "Iteration 304, loss = 0.19941536\n",
      "Iteration 305, loss = 0.19924346\n",
      "Iteration 306, loss = 0.19924707\n",
      "Iteration 307, loss = 0.19926721\n",
      "Iteration 308, loss = 0.19842056\n",
      "Iteration 309, loss = 0.19814389\n",
      "Iteration 310, loss = 0.19782877\n",
      "Iteration 311, loss = 0.19764439\n",
      "Iteration 312, loss = 0.19763693\n",
      "Iteration 313, loss = 0.19792161\n",
      "Iteration 314, loss = 0.19706392\n",
      "Iteration 315, loss = 0.19718453\n",
      "Iteration 316, loss = 0.19697743\n",
      "Iteration 317, loss = 0.19663725\n",
      "Iteration 318, loss = 0.19658699\n",
      "Iteration 319, loss = 0.19652417\n",
      "Iteration 320, loss = 0.19604464\n",
      "Iteration 321, loss = 0.19658640\n",
      "Iteration 322, loss = 0.19603828\n",
      "Iteration 323, loss = 0.19572002\n",
      "Iteration 324, loss = 0.19530314\n",
      "Iteration 325, loss = 0.19521767\n",
      "Iteration 326, loss = 0.19554299\n",
      "Iteration 327, loss = 0.19474723\n",
      "Iteration 328, loss = 0.19502098\n",
      "Iteration 329, loss = 0.19523022\n",
      "Iteration 330, loss = 0.19474032\n",
      "Iteration 331, loss = 0.19443287\n",
      "Iteration 332, loss = 0.19390519\n",
      "Iteration 333, loss = 0.19400021\n",
      "Iteration 334, loss = 0.19362401\n",
      "Iteration 335, loss = 0.19377636\n",
      "Iteration 336, loss = 0.19333914\n",
      "Iteration 337, loss = 0.19351053\n",
      "Iteration 338, loss = 0.19389299\n",
      "Iteration 339, loss = 0.19281538\n",
      "Iteration 340, loss = 0.19307109\n",
      "Iteration 341, loss = 0.19259668\n",
      "Iteration 342, loss = 0.19229297\n",
      "Iteration 343, loss = 0.19209818\n",
      "Iteration 344, loss = 0.19244165\n",
      "Iteration 345, loss = 0.19160435\n",
      "Iteration 346, loss = 0.19161759\n",
      "Iteration 347, loss = 0.19214527\n",
      "Iteration 348, loss = 0.19131725\n",
      "Iteration 349, loss = 0.19175887\n",
      "Iteration 350, loss = 0.19095986\n",
      "Iteration 351, loss = 0.19074856\n",
      "Iteration 352, loss = 0.19111979\n",
      "Iteration 353, loss = 0.19049382\n",
      "Iteration 354, loss = 0.19048602\n",
      "Iteration 355, loss = 0.19033233\n",
      "Iteration 356, loss = 0.19060330\n",
      "Iteration 357, loss = 0.19009391\n",
      "Iteration 358, loss = 0.19012930\n",
      "Iteration 359, loss = 0.19029398\n",
      "Iteration 360, loss = 0.18969951\n",
      "Iteration 361, loss = 0.18974340\n",
      "Iteration 362, loss = 0.18952958\n",
      "Iteration 363, loss = 0.18925461\n",
      "Iteration 364, loss = 0.19035608\n",
      "Iteration 365, loss = 0.18999187\n",
      "Iteration 366, loss = 0.18880864\n",
      "Iteration 367, loss = 0.18869794\n",
      "Iteration 368, loss = 0.18854531\n",
      "Iteration 369, loss = 0.18869280\n",
      "Iteration 370, loss = 0.18844855\n",
      "Iteration 371, loss = 0.18845899\n",
      "Iteration 372, loss = 0.18827631\n",
      "Iteration 373, loss = 0.18817011\n",
      "Iteration 374, loss = 0.18877728\n",
      "Iteration 375, loss = 0.18767839\n",
      "Iteration 376, loss = 0.18834544\n",
      "Iteration 377, loss = 0.18730900\n",
      "Iteration 378, loss = 0.18719054\n",
      "Iteration 379, loss = 0.18721513\n",
      "Iteration 380, loss = 0.18682101\n",
      "Iteration 381, loss = 0.18748092\n",
      "Iteration 382, loss = 0.18717612\n",
      "Iteration 383, loss = 0.18707107\n",
      "Iteration 384, loss = 0.18639256\n",
      "Iteration 385, loss = 0.18622989\n",
      "Iteration 386, loss = 0.18659098\n",
      "Iteration 387, loss = 0.18618549\n",
      "Iteration 388, loss = 0.18600775\n",
      "Iteration 389, loss = 0.18590334\n",
      "Iteration 390, loss = 0.18590724\n",
      "Iteration 391, loss = 0.18571421\n",
      "Iteration 392, loss = 0.18609091\n",
      "Iteration 393, loss = 0.18552350\n",
      "Iteration 394, loss = 0.18606649\n",
      "Iteration 395, loss = 0.18601512\n",
      "Iteration 396, loss = 0.18533683\n",
      "Iteration 397, loss = 0.18496261\n",
      "Iteration 398, loss = 0.18507805\n",
      "Iteration 399, loss = 0.18497362\n",
      "Iteration 400, loss = 0.18468834\n",
      "Iteration 401, loss = 0.18537790\n",
      "Iteration 402, loss = 0.18464449\n",
      "Iteration 403, loss = 0.18462709\n",
      "Iteration 404, loss = 0.18452852\n",
      "Iteration 405, loss = 0.18449035\n",
      "Iteration 406, loss = 0.18400681\n",
      "Iteration 407, loss = 0.18417306\n",
      "Iteration 408, loss = 0.18402824\n",
      "Iteration 409, loss = 0.18406061\n",
      "Iteration 410, loss = 0.18443283\n",
      "Iteration 411, loss = 0.18481914\n",
      "Iteration 412, loss = 0.18483791\n",
      "Iteration 413, loss = 0.18328284\n",
      "Iteration 414, loss = 0.18350746\n",
      "Iteration 415, loss = 0.18330592\n",
      "Iteration 416, loss = 0.18309781\n",
      "Iteration 417, loss = 0.18294019\n",
      "Iteration 418, loss = 0.18287873\n",
      "Iteration 419, loss = 0.18308266\n",
      "Iteration 420, loss = 0.18294104\n",
      "Iteration 421, loss = 0.18327904\n",
      "Iteration 422, loss = 0.18273461\n",
      "Iteration 423, loss = 0.18269807\n",
      "Iteration 424, loss = 0.18346859\n",
      "Iteration 425, loss = 0.18225628\n",
      "Iteration 426, loss = 0.18214168\n",
      "Iteration 427, loss = 0.18204759\n",
      "Iteration 428, loss = 0.18206911\n",
      "Iteration 429, loss = 0.18201718\n",
      "Iteration 430, loss = 0.18228039\n",
      "Iteration 431, loss = 0.18251360\n",
      "Iteration 432, loss = 0.18159643\n",
      "Iteration 433, loss = 0.18231047\n",
      "Iteration 434, loss = 0.18287593\n",
      "Iteration 435, loss = 0.18123691\n",
      "Iteration 436, loss = 0.18228432\n",
      "Iteration 437, loss = 0.18147040\n",
      "Iteration 438, loss = 0.18099903\n",
      "Iteration 439, loss = 0.18158890\n",
      "Iteration 440, loss = 0.18123378\n",
      "Iteration 441, loss = 0.18145501\n",
      "Iteration 442, loss = 0.18111657\n",
      "Iteration 443, loss = 0.18128625\n",
      "Iteration 444, loss = 0.18100569\n",
      "Iteration 445, loss = 0.18115300\n",
      "Iteration 446, loss = 0.18128722\n",
      "Iteration 447, loss = 0.18062070\n",
      "Iteration 448, loss = 0.18043328\n",
      "Iteration 449, loss = 0.18059057\n",
      "Iteration 450, loss = 0.18057934\n",
      "Iteration 451, loss = 0.18062110\n",
      "Iteration 452, loss = 0.18050332\n",
      "Iteration 453, loss = 0.18051510\n",
      "Iteration 454, loss = 0.18059640\n",
      "Iteration 455, loss = 0.18020451\n",
      "Iteration 456, loss = 0.17981329\n",
      "Iteration 457, loss = 0.17991937\n",
      "Iteration 458, loss = 0.17988575\n",
      "Iteration 459, loss = 0.18019771\n",
      "Iteration 460, loss = 0.17978168\n",
      "Iteration 461, loss = 0.18005907\n",
      "Iteration 462, loss = 0.17997095\n",
      "Iteration 463, loss = 0.17966058\n",
      "Iteration 464, loss = 0.17959281\n",
      "Iteration 465, loss = 0.17978789\n",
      "Iteration 466, loss = 0.17955884\n",
      "Iteration 467, loss = 0.18040144\n",
      "Iteration 468, loss = 0.17969721\n",
      "Iteration 469, loss = 0.17888358\n",
      "Iteration 470, loss = 0.17956348\n",
      "Iteration 471, loss = 0.18037186\n",
      "Iteration 472, loss = 0.17910847\n",
      "Iteration 473, loss = 0.17934939\n",
      "Iteration 474, loss = 0.17953525\n",
      "Iteration 475, loss = 0.17916705\n",
      "Iteration 476, loss = 0.17894972\n",
      "Iteration 477, loss = 0.17876115\n",
      "Iteration 478, loss = 0.17880661\n",
      "Iteration 479, loss = 0.17943479\n",
      "Iteration 480, loss = 0.17865303\n",
      "Iteration 481, loss = 0.17856782\n",
      "Iteration 482, loss = 0.17882594\n",
      "Iteration 483, loss = 0.17838818\n",
      "Iteration 484, loss = 0.17863621\n",
      "Iteration 485, loss = 0.17795564\n",
      "Iteration 486, loss = 0.17877609\n",
      "Iteration 487, loss = 0.17795396\n",
      "Iteration 488, loss = 0.17837571\n",
      "Iteration 489, loss = 0.17827188\n",
      "Iteration 490, loss = 0.17800482\n",
      "Iteration 491, loss = 0.17841679\n",
      "Iteration 492, loss = 0.17795016\n",
      "Iteration 493, loss = 0.17866630\n",
      "Iteration 494, loss = 0.17809312\n",
      "Iteration 495, loss = 0.17799110\n",
      "Iteration 496, loss = 0.17830676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76936924\n",
      "Iteration 2, loss = 0.74333189\n",
      "Iteration 3, loss = 0.72140848\n",
      "Iteration 4, loss = 0.70466540\n",
      "Iteration 5, loss = 0.69037143\n",
      "Iteration 6, loss = 0.67736054\n",
      "Iteration 7, loss = 0.66390686\n",
      "Iteration 8, loss = 0.64910670\n",
      "Iteration 9, loss = 0.63089402\n",
      "Iteration 10, loss = 0.61187741\n",
      "Iteration 11, loss = 0.59276454\n",
      "Iteration 12, loss = 0.57503723\n",
      "Iteration 13, loss = 0.55884064\n",
      "Iteration 14, loss = 0.54467333\n",
      "Iteration 15, loss = 0.53294157\n",
      "Iteration 16, loss = 0.52239723\n",
      "Iteration 17, loss = 0.51281822\n",
      "Iteration 18, loss = 0.50452501\n",
      "Iteration 19, loss = 0.49712957\n",
      "Iteration 20, loss = 0.49053646\n",
      "Iteration 21, loss = 0.48419204\n",
      "Iteration 22, loss = 0.47819776\n",
      "Iteration 23, loss = 0.47271545\n",
      "Iteration 24, loss = 0.46749992\n",
      "Iteration 25, loss = 0.46249257\n",
      "Iteration 26, loss = 0.45801955\n",
      "Iteration 27, loss = 0.45361909\n",
      "Iteration 28, loss = 0.44942081\n",
      "Iteration 29, loss = 0.44551507\n",
      "Iteration 30, loss = 0.44164551\n",
      "Iteration 31, loss = 0.43798368\n",
      "Iteration 32, loss = 0.43419595\n",
      "Iteration 33, loss = 0.43089287\n",
      "Iteration 34, loss = 0.42729154\n",
      "Iteration 35, loss = 0.42384442\n",
      "Iteration 36, loss = 0.42063140\n",
      "Iteration 37, loss = 0.41768450\n",
      "Iteration 38, loss = 0.41423374\n",
      "Iteration 39, loss = 0.41120508\n",
      "Iteration 40, loss = 0.40829130\n",
      "Iteration 41, loss = 0.40523498\n",
      "Iteration 42, loss = 0.40240054\n",
      "Iteration 43, loss = 0.39964241\n",
      "Iteration 44, loss = 0.39743482\n",
      "Iteration 45, loss = 0.39407546\n",
      "Iteration 46, loss = 0.39146161\n",
      "Iteration 47, loss = 0.38891268\n",
      "Iteration 48, loss = 0.38664079\n",
      "Iteration 49, loss = 0.38435197\n",
      "Iteration 50, loss = 0.38211994\n",
      "Iteration 51, loss = 0.37962680\n",
      "Iteration 52, loss = 0.37738122\n",
      "Iteration 53, loss = 0.37508138\n",
      "Iteration 54, loss = 0.37301393\n",
      "Iteration 55, loss = 0.37083130\n",
      "Iteration 56, loss = 0.36868144\n",
      "Iteration 57, loss = 0.36681694\n",
      "Iteration 58, loss = 0.36433038\n",
      "Iteration 59, loss = 0.36239017\n",
      "Iteration 60, loss = 0.36028348\n",
      "Iteration 61, loss = 0.35848130\n",
      "Iteration 62, loss = 0.35666432\n",
      "Iteration 63, loss = 0.35479710\n",
      "Iteration 64, loss = 0.35264746\n",
      "Iteration 65, loss = 0.35080318\n",
      "Iteration 66, loss = 0.34910573\n",
      "Iteration 67, loss = 0.34726329\n",
      "Iteration 68, loss = 0.34568988\n",
      "Iteration 69, loss = 0.34382578\n",
      "Iteration 70, loss = 0.34201815\n",
      "Iteration 71, loss = 0.34022892\n",
      "Iteration 72, loss = 0.33836163\n",
      "Iteration 73, loss = 0.33669085\n",
      "Iteration 74, loss = 0.33484538\n",
      "Iteration 75, loss = 0.33335618\n",
      "Iteration 76, loss = 0.33177550\n",
      "Iteration 77, loss = 0.33007408\n",
      "Iteration 78, loss = 0.32893548\n",
      "Iteration 79, loss = 0.32714110\n",
      "Iteration 80, loss = 0.32556782\n",
      "Iteration 81, loss = 0.32431044\n",
      "Iteration 82, loss = 0.32288671\n",
      "Iteration 83, loss = 0.32147350\n",
      "Iteration 84, loss = 0.32002205\n",
      "Iteration 85, loss = 0.31851623\n",
      "Iteration 86, loss = 0.31747719\n",
      "Iteration 87, loss = 0.31585226\n",
      "Iteration 88, loss = 0.31437284\n",
      "Iteration 89, loss = 0.31308134\n",
      "Iteration 90, loss = 0.31202181\n",
      "Iteration 91, loss = 0.31123533\n",
      "Iteration 92, loss = 0.30968811\n",
      "Iteration 93, loss = 0.30807907\n",
      "Iteration 94, loss = 0.30699324\n",
      "Iteration 95, loss = 0.30611171\n",
      "Iteration 96, loss = 0.30504714\n",
      "Iteration 97, loss = 0.30346931\n",
      "Iteration 98, loss = 0.30224020\n",
      "Iteration 99, loss = 0.30144867\n",
      "Iteration 100, loss = 0.29981954\n",
      "Iteration 101, loss = 0.29899264\n",
      "Iteration 102, loss = 0.29795789\n",
      "Iteration 103, loss = 0.29720167\n",
      "Iteration 104, loss = 0.29589491\n",
      "Iteration 105, loss = 0.29463608\n",
      "Iteration 106, loss = 0.29334638\n",
      "Iteration 107, loss = 0.29235541\n",
      "Iteration 108, loss = 0.29156589\n",
      "Iteration 109, loss = 0.29046856\n",
      "Iteration 110, loss = 0.28919038\n",
      "Iteration 111, loss = 0.28815040\n",
      "Iteration 112, loss = 0.28717174\n",
      "Iteration 113, loss = 0.28624973\n",
      "Iteration 114, loss = 0.28543576\n",
      "Iteration 115, loss = 0.28442352\n",
      "Iteration 116, loss = 0.28354462\n",
      "Iteration 117, loss = 0.28251719\n",
      "Iteration 118, loss = 0.28142122\n",
      "Iteration 119, loss = 0.28089617\n",
      "Iteration 120, loss = 0.27955646\n",
      "Iteration 121, loss = 0.27887072\n",
      "Iteration 122, loss = 0.27783670\n",
      "Iteration 123, loss = 0.27703822\n",
      "Iteration 124, loss = 0.27652969\n",
      "Iteration 125, loss = 0.27527378\n",
      "Iteration 126, loss = 0.27431459\n",
      "Iteration 127, loss = 0.27351568\n",
      "Iteration 128, loss = 0.27254791\n",
      "Iteration 129, loss = 0.27201219\n",
      "Iteration 130, loss = 0.27099543\n",
      "Iteration 131, loss = 0.27046545\n",
      "Iteration 132, loss = 0.26919777\n",
      "Iteration 133, loss = 0.26858741\n",
      "Iteration 134, loss = 0.26767076\n",
      "Iteration 135, loss = 0.26688946\n",
      "Iteration 136, loss = 0.26645098\n",
      "Iteration 137, loss = 0.26541058\n",
      "Iteration 138, loss = 0.26463959\n",
      "Iteration 139, loss = 0.26350153\n",
      "Iteration 140, loss = 0.26248820\n",
      "Iteration 141, loss = 0.26154319\n",
      "Iteration 142, loss = 0.26058638\n",
      "Iteration 143, loss = 0.25978671\n",
      "Iteration 144, loss = 0.25941169\n",
      "Iteration 145, loss = 0.25862632\n",
      "Iteration 146, loss = 0.25757485\n",
      "Iteration 147, loss = 0.25688819\n",
      "Iteration 148, loss = 0.25597182\n",
      "Iteration 149, loss = 0.25559021\n",
      "Iteration 150, loss = 0.25452771\n",
      "Iteration 151, loss = 0.25428570\n",
      "Iteration 152, loss = 0.25313188\n",
      "Iteration 153, loss = 0.25266043\n",
      "Iteration 154, loss = 0.25246050\n",
      "Iteration 155, loss = 0.25096920\n",
      "Iteration 156, loss = 0.25067552\n",
      "Iteration 157, loss = 0.24985762\n",
      "Iteration 158, loss = 0.24906916\n",
      "Iteration 159, loss = 0.24825697\n",
      "Iteration 160, loss = 0.24763251\n",
      "Iteration 161, loss = 0.24721500\n",
      "Iteration 162, loss = 0.24700702\n",
      "Iteration 163, loss = 0.24596491\n",
      "Iteration 164, loss = 0.24507694\n",
      "Iteration 165, loss = 0.24459188\n",
      "Iteration 166, loss = 0.24460135\n",
      "Iteration 167, loss = 0.24384042\n",
      "Iteration 168, loss = 0.24330860\n",
      "Iteration 169, loss = 0.24278436\n",
      "Iteration 170, loss = 0.24157366\n",
      "Iteration 171, loss = 0.24107611\n",
      "Iteration 172, loss = 0.24053665\n",
      "Iteration 173, loss = 0.24010440\n",
      "Iteration 174, loss = 0.23947815\n",
      "Iteration 175, loss = 0.23909710\n",
      "Iteration 176, loss = 0.23906150\n",
      "Iteration 177, loss = 0.23816184\n",
      "Iteration 178, loss = 0.23765081\n",
      "Iteration 179, loss = 0.23690973\n",
      "Iteration 180, loss = 0.23657079\n",
      "Iteration 181, loss = 0.23590654\n",
      "Iteration 182, loss = 0.23586941\n",
      "Iteration 183, loss = 0.23495069\n",
      "Iteration 184, loss = 0.23451666\n",
      "Iteration 185, loss = 0.23411165\n",
      "Iteration 186, loss = 0.23425545\n",
      "Iteration 187, loss = 0.23299673\n",
      "Iteration 188, loss = 0.23297813\n",
      "Iteration 189, loss = 0.23198919\n",
      "Iteration 190, loss = 0.23226347\n",
      "Iteration 191, loss = 0.23132513\n",
      "Iteration 192, loss = 0.23091475\n",
      "Iteration 193, loss = 0.23055078\n",
      "Iteration 194, loss = 0.23030999\n",
      "Iteration 195, loss = 0.22963531\n",
      "Iteration 196, loss = 0.22895144\n",
      "Iteration 197, loss = 0.22859483\n",
      "Iteration 198, loss = 0.22823250\n",
      "Iteration 199, loss = 0.22832423\n",
      "Iteration 200, loss = 0.22720649\n",
      "Iteration 201, loss = 0.22721633\n",
      "Iteration 202, loss = 0.22706140\n",
      "Iteration 203, loss = 0.22659256\n",
      "Iteration 204, loss = 0.22607633\n",
      "Iteration 205, loss = 0.22549636\n",
      "Iteration 206, loss = 0.22529978\n",
      "Iteration 207, loss = 0.22507572\n",
      "Iteration 208, loss = 0.22475997\n",
      "Iteration 209, loss = 0.22402417\n",
      "Iteration 210, loss = 0.22371376\n",
      "Iteration 211, loss = 0.22404314\n",
      "Iteration 212, loss = 0.22317944\n",
      "Iteration 213, loss = 0.22483842\n",
      "Iteration 214, loss = 0.22240772\n",
      "Iteration 215, loss = 0.22308984\n",
      "Iteration 216, loss = 0.22184197\n",
      "Iteration 217, loss = 0.22172808\n",
      "Iteration 218, loss = 0.22162876\n",
      "Iteration 219, loss = 0.22168408\n",
      "Iteration 220, loss = 0.22057512\n",
      "Iteration 221, loss = 0.22033404\n",
      "Iteration 222, loss = 0.22046589\n",
      "Iteration 223, loss = 0.21977071\n",
      "Iteration 224, loss = 0.22000991\n",
      "Iteration 225, loss = 0.21980381\n",
      "Iteration 226, loss = 0.21911726\n",
      "Iteration 227, loss = 0.21856477\n",
      "Iteration 228, loss = 0.21855147\n",
      "Iteration 229, loss = 0.21821861\n",
      "Iteration 230, loss = 0.21821349\n",
      "Iteration 231, loss = 0.21756705\n",
      "Iteration 232, loss = 0.21753909\n",
      "Iteration 233, loss = 0.21752594\n",
      "Iteration 234, loss = 0.21674745\n",
      "Iteration 235, loss = 0.21619338\n",
      "Iteration 236, loss = 0.21636355\n",
      "Iteration 237, loss = 0.21600836\n",
      "Iteration 238, loss = 0.21601918\n",
      "Iteration 239, loss = 0.21593082\n",
      "Iteration 240, loss = 0.21540295\n",
      "Iteration 241, loss = 0.21503870\n",
      "Iteration 242, loss = 0.21521489\n",
      "Iteration 243, loss = 0.21535234\n",
      "Iteration 244, loss = 0.21430811\n",
      "Iteration 245, loss = 0.21534117\n",
      "Iteration 246, loss = 0.21531796\n",
      "Iteration 247, loss = 0.21391620\n",
      "Iteration 248, loss = 0.21347622\n",
      "Iteration 249, loss = 0.21378593\n",
      "Iteration 250, loss = 0.21337907\n",
      "Iteration 251, loss = 0.21325973\n",
      "Iteration 252, loss = 0.21260520\n",
      "Iteration 253, loss = 0.21229775\n",
      "Iteration 254, loss = 0.21210860\n",
      "Iteration 255, loss = 0.21195542\n",
      "Iteration 256, loss = 0.21171375\n",
      "Iteration 257, loss = 0.21170009\n",
      "Iteration 258, loss = 0.21135659\n",
      "Iteration 259, loss = 0.21104564\n",
      "Iteration 260, loss = 0.21073606\n",
      "Iteration 261, loss = 0.21092626\n",
      "Iteration 262, loss = 0.21049815\n",
      "Iteration 263, loss = 0.21021659\n",
      "Iteration 264, loss = 0.21045664\n",
      "Iteration 265, loss = 0.20975467\n",
      "Iteration 266, loss = 0.21034855\n",
      "Iteration 267, loss = 0.20967025\n",
      "Iteration 268, loss = 0.20968005\n",
      "Iteration 269, loss = 0.20918356\n",
      "Iteration 270, loss = 0.20919153\n",
      "Iteration 271, loss = 0.20893903\n",
      "Iteration 272, loss = 0.20855186\n",
      "Iteration 273, loss = 0.20836305\n",
      "Iteration 274, loss = 0.20811304\n",
      "Iteration 275, loss = 0.20831539\n",
      "Iteration 276, loss = 0.20786799\n",
      "Iteration 277, loss = 0.20763126\n",
      "Iteration 278, loss = 0.20743218\n",
      "Iteration 279, loss = 0.20730886\n",
      "Iteration 280, loss = 0.20732706\n",
      "Iteration 281, loss = 0.20704034\n",
      "Iteration 282, loss = 0.20692879\n",
      "Iteration 283, loss = 0.20678946\n",
      "Iteration 284, loss = 0.20654089\n",
      "Iteration 285, loss = 0.20669777\n",
      "Iteration 286, loss = 0.20617440\n",
      "Iteration 287, loss = 0.20598345\n",
      "Iteration 288, loss = 0.20571317\n",
      "Iteration 289, loss = 0.20562112\n",
      "Iteration 290, loss = 0.20678638\n",
      "Iteration 291, loss = 0.20521771\n",
      "Iteration 292, loss = 0.20531818\n",
      "Iteration 293, loss = 0.20467612\n",
      "Iteration 294, loss = 0.20487494\n",
      "Iteration 295, loss = 0.20492978\n",
      "Iteration 296, loss = 0.20444829\n",
      "Iteration 297, loss = 0.20509690\n",
      "Iteration 298, loss = 0.20477427\n",
      "Iteration 299, loss = 0.20480547\n",
      "Iteration 300, loss = 0.20430726\n",
      "Iteration 301, loss = 0.20390636\n",
      "Iteration 302, loss = 0.20372354\n",
      "Iteration 303, loss = 0.20350712\n",
      "Iteration 304, loss = 0.20395306\n",
      "Iteration 305, loss = 0.20351250\n",
      "Iteration 306, loss = 0.20302344\n",
      "Iteration 307, loss = 0.20308628\n",
      "Iteration 308, loss = 0.20291204\n",
      "Iteration 309, loss = 0.20260178\n",
      "Iteration 310, loss = 0.20283112\n",
      "Iteration 311, loss = 0.20220904\n",
      "Iteration 312, loss = 0.20221793\n",
      "Iteration 313, loss = 0.20247538\n",
      "Iteration 314, loss = 0.20202717\n",
      "Iteration 315, loss = 0.20182549\n",
      "Iteration 316, loss = 0.20185396\n",
      "Iteration 317, loss = 0.20144051\n",
      "Iteration 318, loss = 0.20112580\n",
      "Iteration 319, loss = 0.20160098\n",
      "Iteration 320, loss = 0.20110572\n",
      "Iteration 321, loss = 0.20098349\n",
      "Iteration 322, loss = 0.20048342\n",
      "Iteration 323, loss = 0.20074981\n",
      "Iteration 324, loss = 0.20066801\n",
      "Iteration 325, loss = 0.20017949\n",
      "Iteration 326, loss = 0.20037149\n",
      "Iteration 327, loss = 0.19992279\n",
      "Iteration 328, loss = 0.20003818\n",
      "Iteration 329, loss = 0.20041768\n",
      "Iteration 330, loss = 0.19991349\n",
      "Iteration 331, loss = 0.19954644\n",
      "Iteration 332, loss = 0.19956924\n",
      "Iteration 333, loss = 0.19945290\n",
      "Iteration 334, loss = 0.19945782\n",
      "Iteration 335, loss = 0.19959830\n",
      "Iteration 336, loss = 0.19912066\n",
      "Iteration 337, loss = 0.19916918\n",
      "Iteration 338, loss = 0.19929126\n",
      "Iteration 339, loss = 0.19932626\n",
      "Iteration 340, loss = 0.19923001\n",
      "Iteration 341, loss = 0.19854689\n",
      "Iteration 342, loss = 0.19872630\n",
      "Iteration 343, loss = 0.19839005\n",
      "Iteration 344, loss = 0.19869742\n",
      "Iteration 345, loss = 0.19827231\n",
      "Iteration 346, loss = 0.19820779\n",
      "Iteration 347, loss = 0.19830730\n",
      "Iteration 348, loss = 0.19852323\n",
      "Iteration 349, loss = 0.19788201\n",
      "Iteration 350, loss = 0.19809374\n",
      "Iteration 351, loss = 0.19779504\n",
      "Iteration 352, loss = 0.19811094\n",
      "Iteration 353, loss = 0.19854197\n",
      "Iteration 354, loss = 0.19758976\n",
      "Iteration 355, loss = 0.19768758\n",
      "Iteration 356, loss = 0.19721903\n",
      "Iteration 357, loss = 0.19745941\n",
      "Iteration 358, loss = 0.19740164\n",
      "Iteration 359, loss = 0.19701061\n",
      "Iteration 360, loss = 0.19727571\n",
      "Iteration 361, loss = 0.19722308\n",
      "Iteration 362, loss = 0.19708819\n",
      "Iteration 363, loss = 0.19679415\n",
      "Iteration 364, loss = 0.19670035\n",
      "Iteration 365, loss = 0.19656023\n",
      "Iteration 366, loss = 0.19642463\n",
      "Iteration 367, loss = 0.19673085\n",
      "Iteration 368, loss = 0.19666278\n",
      "Iteration 369, loss = 0.19632639\n",
      "Iteration 370, loss = 0.19605519\n",
      "Iteration 371, loss = 0.19689316\n",
      "Iteration 372, loss = 0.19608804\n",
      "Iteration 373, loss = 0.19579041\n",
      "Iteration 374, loss = 0.19597702\n",
      "Iteration 375, loss = 0.19578623\n",
      "Iteration 376, loss = 0.19596644\n",
      "Iteration 377, loss = 0.19603190\n",
      "Iteration 378, loss = 0.19570750\n",
      "Iteration 379, loss = 0.19549758\n",
      "Iteration 380, loss = 0.19551721\n",
      "Iteration 381, loss = 0.19536429\n",
      "Iteration 382, loss = 0.19554355\n",
      "Iteration 383, loss = 0.19498540\n",
      "Iteration 384, loss = 0.19522524\n",
      "Iteration 385, loss = 0.19572667\n",
      "Iteration 386, loss = 0.19536234\n",
      "Iteration 387, loss = 0.19521915\n",
      "Iteration 388, loss = 0.19503450\n",
      "Iteration 389, loss = 0.19523330\n",
      "Iteration 390, loss = 0.19512282\n",
      "Iteration 391, loss = 0.19521094\n",
      "Iteration 392, loss = 0.19481736\n",
      "Iteration 393, loss = 0.19490858\n",
      "Iteration 394, loss = 0.19462708\n",
      "Iteration 395, loss = 0.19486857\n",
      "Iteration 396, loss = 0.19506648\n",
      "Iteration 397, loss = 0.19457159\n",
      "Iteration 398, loss = 0.19452416\n",
      "Iteration 399, loss = 0.19546888\n",
      "Iteration 400, loss = 0.19435414\n",
      "Iteration 401, loss = 0.19565517\n",
      "Iteration 402, loss = 0.19437635\n",
      "Iteration 403, loss = 0.19433778\n",
      "Iteration 404, loss = 0.19527775\n",
      "Iteration 405, loss = 0.19483012\n",
      "Iteration 406, loss = 0.19415950\n",
      "Iteration 407, loss = 0.19382321\n",
      "Iteration 408, loss = 0.19411597\n",
      "Iteration 409, loss = 0.19382667\n",
      "Iteration 410, loss = 0.19408314\n",
      "Iteration 411, loss = 0.19351733\n",
      "Iteration 412, loss = 0.19368639\n",
      "Iteration 413, loss = 0.19376621\n",
      "Iteration 414, loss = 0.19370689\n",
      "Iteration 415, loss = 0.19363985\n",
      "Iteration 416, loss = 0.19340479\n",
      "Iteration 417, loss = 0.19350804\n",
      "Iteration 418, loss = 0.19323447\n",
      "Iteration 419, loss = 0.19358874\n",
      "Iteration 420, loss = 0.19310333\n",
      "Iteration 421, loss = 0.19327955\n",
      "Iteration 422, loss = 0.19272948\n",
      "Iteration 423, loss = 0.19352841\n",
      "Iteration 424, loss = 0.19331519\n",
      "Iteration 425, loss = 0.19322856\n",
      "Iteration 426, loss = 0.19360909\n",
      "Iteration 427, loss = 0.19323248\n",
      "Iteration 428, loss = 0.19276604\n",
      "Iteration 429, loss = 0.19283386\n",
      "Iteration 430, loss = 0.19253072\n",
      "Iteration 431, loss = 0.19233220\n",
      "Iteration 432, loss = 0.19246933\n",
      "Iteration 433, loss = 0.19291390\n",
      "Iteration 434, loss = 0.19245852\n",
      "Iteration 435, loss = 0.19212571\n",
      "Iteration 436, loss = 0.19251172\n",
      "Iteration 437, loss = 0.19217364\n",
      "Iteration 438, loss = 0.19232256\n",
      "Iteration 439, loss = 0.19174805\n",
      "Iteration 440, loss = 0.19206408\n",
      "Iteration 441, loss = 0.19164848\n",
      "Iteration 442, loss = 0.19184205\n",
      "Iteration 443, loss = 0.19192948\n",
      "Iteration 444, loss = 0.19210888\n",
      "Iteration 445, loss = 0.19164701\n",
      "Iteration 446, loss = 0.19164198\n",
      "Iteration 447, loss = 0.19123265\n",
      "Iteration 448, loss = 0.19160034\n",
      "Iteration 449, loss = 0.19190854\n",
      "Iteration 450, loss = 0.19227804\n",
      "Iteration 451, loss = 0.19151880\n",
      "Iteration 452, loss = 0.19154601\n",
      "Iteration 453, loss = 0.19093400\n",
      "Iteration 454, loss = 0.19149330\n",
      "Iteration 455, loss = 0.19077823\n",
      "Iteration 456, loss = 0.19086226\n",
      "Iteration 457, loss = 0.19166242\n",
      "Iteration 458, loss = 0.19074640\n",
      "Iteration 459, loss = 0.19070916\n",
      "Iteration 460, loss = 0.19045973\n",
      "Iteration 461, loss = 0.19083406\n",
      "Iteration 462, loss = 0.19053401\n",
      "Iteration 463, loss = 0.19046428\n",
      "Iteration 464, loss = 0.19035472\n",
      "Iteration 465, loss = 0.19217265\n",
      "Iteration 466, loss = 0.19083263\n",
      "Iteration 467, loss = 0.19086004\n",
      "Iteration 468, loss = 0.19044865\n",
      "Iteration 469, loss = 0.19067186\n",
      "Iteration 470, loss = 0.19036195\n",
      "Iteration 471, loss = 0.19011816\n",
      "Iteration 472, loss = 0.19111885\n",
      "Iteration 473, loss = 0.19071791\n",
      "Iteration 474, loss = 0.19049349\n",
      "Iteration 475, loss = 0.19016574\n",
      "Iteration 476, loss = 0.19002697\n",
      "Iteration 477, loss = 0.18998581\n",
      "Iteration 478, loss = 0.19004840\n",
      "Iteration 479, loss = 0.19005581\n",
      "Iteration 480, loss = 0.19064787\n",
      "Iteration 481, loss = 0.19002820\n",
      "Iteration 482, loss = 0.18950121\n",
      "Iteration 483, loss = 0.19006740\n",
      "Iteration 484, loss = 0.18980852\n",
      "Iteration 485, loss = 0.18951860\n",
      "Iteration 486, loss = 0.18935190\n",
      "Iteration 487, loss = 0.18941228\n",
      "Iteration 488, loss = 0.18977132\n",
      "Iteration 489, loss = 0.18986890\n",
      "Iteration 490, loss = 0.18942151\n",
      "Iteration 491, loss = 0.18927432\n",
      "Iteration 492, loss = 0.18951375\n",
      "Iteration 493, loss = 0.18938580\n",
      "Iteration 494, loss = 0.18926850\n",
      "Iteration 495, loss = 0.18905482\n",
      "Iteration 496, loss = 0.18916559\n",
      "Iteration 497, loss = 0.18901207\n",
      "Iteration 498, loss = 0.18901839\n",
      "Iteration 499, loss = 0.18994776\n",
      "Iteration 500, loss = 0.18909316\n",
      "Iteration 501, loss = 0.18899545\n",
      "Iteration 502, loss = 0.18919967\n",
      "Iteration 503, loss = 0.18854070\n",
      "Iteration 504, loss = 0.18894916\n",
      "Iteration 505, loss = 0.18864829\n",
      "Iteration 506, loss = 0.18941607\n",
      "Iteration 507, loss = 0.18945985\n",
      "Iteration 508, loss = 0.18838995\n",
      "Iteration 509, loss = 0.18875131\n",
      "Iteration 510, loss = 0.18904460\n",
      "Iteration 511, loss = 0.18842947\n",
      "Iteration 512, loss = 0.18828768\n",
      "Iteration 513, loss = 0.18837709\n",
      "Iteration 514, loss = 0.18883963\n",
      "Iteration 515, loss = 0.18878644\n",
      "Iteration 516, loss = 0.18820123\n",
      "Iteration 517, loss = 0.18886715\n",
      "Iteration 518, loss = 0.18839114\n",
      "Iteration 519, loss = 0.18820843\n",
      "Iteration 520, loss = 0.18813102\n",
      "Iteration 521, loss = 0.18892661\n",
      "Iteration 522, loss = 0.18757722\n",
      "Iteration 523, loss = 0.18898827\n",
      "Iteration 524, loss = 0.18805291\n",
      "Iteration 525, loss = 0.18797922\n",
      "Iteration 526, loss = 0.18767374\n",
      "Iteration 527, loss = 0.18818699\n",
      "Iteration 528, loss = 0.18798826\n",
      "Iteration 529, loss = 0.18765295\n",
      "Iteration 530, loss = 0.18803433\n",
      "Iteration 531, loss = 0.18781767\n",
      "Iteration 532, loss = 0.18791228\n",
      "Iteration 533, loss = 0.18808438\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77094554\n",
      "Iteration 2, loss = 0.74504430\n",
      "Iteration 3, loss = 0.72313826\n",
      "Iteration 4, loss = 0.70566924\n",
      "Iteration 5, loss = 0.69145085\n",
      "Iteration 6, loss = 0.67841677\n",
      "Iteration 7, loss = 0.66491290\n",
      "Iteration 8, loss = 0.64925462\n",
      "Iteration 9, loss = 0.63144386\n",
      "Iteration 10, loss = 0.61275936\n",
      "Iteration 11, loss = 0.59323090\n",
      "Iteration 12, loss = 0.57524303\n",
      "Iteration 13, loss = 0.55874360\n",
      "Iteration 14, loss = 0.54451548\n",
      "Iteration 15, loss = 0.53182085\n",
      "Iteration 16, loss = 0.52132814\n",
      "Iteration 17, loss = 0.51196705\n",
      "Iteration 18, loss = 0.50404897\n",
      "Iteration 19, loss = 0.49693015\n",
      "Iteration 20, loss = 0.49037291\n",
      "Iteration 21, loss = 0.48388278\n",
      "Iteration 22, loss = 0.47790186\n",
      "Iteration 23, loss = 0.47243660\n",
      "Iteration 24, loss = 0.46735739\n",
      "Iteration 25, loss = 0.46256178\n",
      "Iteration 26, loss = 0.45788673\n",
      "Iteration 27, loss = 0.45368956\n",
      "Iteration 28, loss = 0.44952444\n",
      "Iteration 29, loss = 0.44563015\n",
      "Iteration 30, loss = 0.44179351\n",
      "Iteration 31, loss = 0.43783969\n",
      "Iteration 32, loss = 0.43426442\n",
      "Iteration 33, loss = 0.43076902\n",
      "Iteration 34, loss = 0.42732612\n",
      "Iteration 35, loss = 0.42409487\n",
      "Iteration 36, loss = 0.42078005\n",
      "Iteration 37, loss = 0.41773954\n",
      "Iteration 38, loss = 0.41444347\n",
      "Iteration 39, loss = 0.41152728\n",
      "Iteration 40, loss = 0.40819897\n",
      "Iteration 41, loss = 0.40540262\n",
      "Iteration 42, loss = 0.40250181\n",
      "Iteration 43, loss = 0.39973597\n",
      "Iteration 44, loss = 0.39699446\n",
      "Iteration 45, loss = 0.39408089\n",
      "Iteration 46, loss = 0.39159606\n",
      "Iteration 47, loss = 0.38897644\n",
      "Iteration 48, loss = 0.38663498\n",
      "Iteration 49, loss = 0.38409183\n",
      "Iteration 50, loss = 0.38162755\n",
      "Iteration 51, loss = 0.37945253\n",
      "Iteration 52, loss = 0.37708409\n",
      "Iteration 53, loss = 0.37446314\n",
      "Iteration 54, loss = 0.37229413\n",
      "Iteration 55, loss = 0.37011385\n",
      "Iteration 56, loss = 0.36777217\n",
      "Iteration 57, loss = 0.36552794\n",
      "Iteration 58, loss = 0.36335475\n",
      "Iteration 59, loss = 0.36132445\n",
      "Iteration 60, loss = 0.35948825\n",
      "Iteration 61, loss = 0.35758245\n",
      "Iteration 62, loss = 0.35543768\n",
      "Iteration 63, loss = 0.35366357\n",
      "Iteration 64, loss = 0.35181418\n",
      "Iteration 65, loss = 0.34995248\n",
      "Iteration 66, loss = 0.34811550\n",
      "Iteration 67, loss = 0.34645662\n",
      "Iteration 68, loss = 0.34460506\n",
      "Iteration 69, loss = 0.34308532\n",
      "Iteration 70, loss = 0.34155259\n",
      "Iteration 71, loss = 0.33993822\n",
      "Iteration 72, loss = 0.33812132\n",
      "Iteration 73, loss = 0.33662696\n",
      "Iteration 74, loss = 0.33495926\n",
      "Iteration 75, loss = 0.33329428\n",
      "Iteration 76, loss = 0.33171625\n",
      "Iteration 77, loss = 0.33009575\n",
      "Iteration 78, loss = 0.32847077\n",
      "Iteration 79, loss = 0.32709289\n",
      "Iteration 80, loss = 0.32536087\n",
      "Iteration 81, loss = 0.32397137\n",
      "Iteration 82, loss = 0.32256809\n",
      "Iteration 83, loss = 0.32125030\n",
      "Iteration 84, loss = 0.32003944\n",
      "Iteration 85, loss = 0.31818837\n",
      "Iteration 86, loss = 0.31698783\n",
      "Iteration 87, loss = 0.31553392\n",
      "Iteration 88, loss = 0.31386245\n",
      "Iteration 89, loss = 0.31271113\n",
      "Iteration 90, loss = 0.31127442\n",
      "Iteration 91, loss = 0.30986970\n",
      "Iteration 92, loss = 0.30885213\n",
      "Iteration 93, loss = 0.30750057\n",
      "Iteration 94, loss = 0.30660524\n",
      "Iteration 95, loss = 0.30524701\n",
      "Iteration 96, loss = 0.30366826\n",
      "Iteration 97, loss = 0.30262490\n",
      "Iteration 98, loss = 0.30153384\n",
      "Iteration 99, loss = 0.30018396\n",
      "Iteration 100, loss = 0.29906662\n",
      "Iteration 101, loss = 0.29778696\n",
      "Iteration 102, loss = 0.29665251\n",
      "Iteration 103, loss = 0.29590788\n",
      "Iteration 104, loss = 0.29463946\n",
      "Iteration 105, loss = 0.29343541\n",
      "Iteration 106, loss = 0.29241496\n",
      "Iteration 107, loss = 0.29109981\n",
      "Iteration 108, loss = 0.29027899\n",
      "Iteration 109, loss = 0.28916488\n",
      "Iteration 110, loss = 0.28819005\n",
      "Iteration 111, loss = 0.28706238\n",
      "Iteration 112, loss = 0.28648497\n",
      "Iteration 113, loss = 0.28543643\n",
      "Iteration 114, loss = 0.28411048\n",
      "Iteration 115, loss = 0.28330876\n",
      "Iteration 116, loss = 0.28233950\n",
      "Iteration 117, loss = 0.28140486\n",
      "Iteration 118, loss = 0.28029163\n",
      "Iteration 119, loss = 0.27941678\n",
      "Iteration 120, loss = 0.27877310\n",
      "Iteration 121, loss = 0.27777330\n",
      "Iteration 122, loss = 0.27672221\n",
      "Iteration 123, loss = 0.27629343\n",
      "Iteration 124, loss = 0.27517513\n",
      "Iteration 125, loss = 0.27465386\n",
      "Iteration 126, loss = 0.27361091\n",
      "Iteration 127, loss = 0.27278405\n",
      "Iteration 128, loss = 0.27212664\n",
      "Iteration 129, loss = 0.27130943\n",
      "Iteration 130, loss = 0.27063319\n",
      "Iteration 131, loss = 0.26949111\n",
      "Iteration 132, loss = 0.26903502\n",
      "Iteration 133, loss = 0.26805651\n",
      "Iteration 134, loss = 0.26737077\n",
      "Iteration 135, loss = 0.26660462\n",
      "Iteration 136, loss = 0.26601954\n",
      "Iteration 137, loss = 0.26515552\n",
      "Iteration 138, loss = 0.26434169\n",
      "Iteration 139, loss = 0.26358525\n",
      "Iteration 140, loss = 0.26298789\n",
      "Iteration 141, loss = 0.26241929\n",
      "Iteration 142, loss = 0.26178050\n",
      "Iteration 143, loss = 0.26081144\n",
      "Iteration 144, loss = 0.26020527\n",
      "Iteration 145, loss = 0.26045811\n",
      "Iteration 146, loss = 0.25937093\n",
      "Iteration 147, loss = 0.25828332\n",
      "Iteration 148, loss = 0.25769913\n",
      "Iteration 149, loss = 0.25712428\n",
      "Iteration 150, loss = 0.25595436\n",
      "Iteration 151, loss = 0.25567447\n",
      "Iteration 152, loss = 0.25473917\n",
      "Iteration 153, loss = 0.25430773\n",
      "Iteration 154, loss = 0.25379523\n",
      "Iteration 155, loss = 0.25315015\n",
      "Iteration 156, loss = 0.25252556\n",
      "Iteration 157, loss = 0.25209259\n",
      "Iteration 158, loss = 0.25168296\n",
      "Iteration 159, loss = 0.25065150\n",
      "Iteration 160, loss = 0.24995666\n",
      "Iteration 161, loss = 0.24938461\n",
      "Iteration 162, loss = 0.24872441\n",
      "Iteration 163, loss = 0.24838829\n",
      "Iteration 164, loss = 0.24786216\n",
      "Iteration 165, loss = 0.24722078\n",
      "Iteration 166, loss = 0.24649804\n",
      "Iteration 167, loss = 0.24636105\n",
      "Iteration 168, loss = 0.24591622\n",
      "Iteration 169, loss = 0.24557643\n",
      "Iteration 170, loss = 0.24477970\n",
      "Iteration 171, loss = 0.24419923\n",
      "Iteration 172, loss = 0.24374716\n",
      "Iteration 173, loss = 0.24335034\n",
      "Iteration 174, loss = 0.24302598\n",
      "Iteration 175, loss = 0.24285329\n",
      "Iteration 176, loss = 0.24195152\n",
      "Iteration 177, loss = 0.24184895\n",
      "Iteration 178, loss = 0.24139780\n",
      "Iteration 179, loss = 0.24078325\n",
      "Iteration 180, loss = 0.24044769\n",
      "Iteration 181, loss = 0.23972033\n",
      "Iteration 182, loss = 0.23944988\n",
      "Iteration 183, loss = 0.23877639\n",
      "Iteration 184, loss = 0.23861555\n",
      "Iteration 185, loss = 0.23814336\n",
      "Iteration 186, loss = 0.23767201\n",
      "Iteration 187, loss = 0.23727181\n",
      "Iteration 188, loss = 0.23693592\n",
      "Iteration 189, loss = 0.23660763\n",
      "Iteration 190, loss = 0.23766959\n",
      "Iteration 191, loss = 0.23589988\n",
      "Iteration 192, loss = 0.23542060\n",
      "Iteration 193, loss = 0.23490266\n",
      "Iteration 194, loss = 0.23447614\n",
      "Iteration 195, loss = 0.23430415\n",
      "Iteration 196, loss = 0.23386828\n",
      "Iteration 197, loss = 0.23333201\n",
      "Iteration 198, loss = 0.23275851\n",
      "Iteration 199, loss = 0.23271561\n",
      "Iteration 200, loss = 0.23164571\n",
      "Iteration 201, loss = 0.23189932\n",
      "Iteration 202, loss = 0.23065041\n",
      "Iteration 203, loss = 0.23068921\n",
      "Iteration 204, loss = 0.23022176\n",
      "Iteration 205, loss = 0.23007654\n",
      "Iteration 206, loss = 0.22944307\n",
      "Iteration 207, loss = 0.22907847\n",
      "Iteration 208, loss = 0.22921114\n",
      "Iteration 209, loss = 0.22912212\n",
      "Iteration 210, loss = 0.22839737\n",
      "Iteration 211, loss = 0.22765000\n",
      "Iteration 212, loss = 0.22708876\n",
      "Iteration 213, loss = 0.22672259\n",
      "Iteration 214, loss = 0.22677488\n",
      "Iteration 215, loss = 0.22653032\n",
      "Iteration 216, loss = 0.22668929\n",
      "Iteration 217, loss = 0.22546212\n",
      "Iteration 218, loss = 0.22503790\n",
      "Iteration 219, loss = 0.22465675\n",
      "Iteration 220, loss = 0.22450756\n",
      "Iteration 221, loss = 0.22403159\n",
      "Iteration 222, loss = 0.22362632\n",
      "Iteration 223, loss = 0.22340069\n",
      "Iteration 224, loss = 0.22344521\n",
      "Iteration 225, loss = 0.22250946\n",
      "Iteration 226, loss = 0.22272343\n",
      "Iteration 227, loss = 0.22304207\n",
      "Iteration 228, loss = 0.22227177\n",
      "Iteration 229, loss = 0.22169556\n",
      "Iteration 230, loss = 0.22140062\n",
      "Iteration 231, loss = 0.22136095\n",
      "Iteration 232, loss = 0.22111622\n",
      "Iteration 233, loss = 0.22044078\n",
      "Iteration 234, loss = 0.22017035\n",
      "Iteration 235, loss = 0.21995064\n",
      "Iteration 236, loss = 0.21986503\n",
      "Iteration 237, loss = 0.21975421\n",
      "Iteration 238, loss = 0.21931920\n",
      "Iteration 239, loss = 0.21975747\n",
      "Iteration 240, loss = 0.21839949\n",
      "Iteration 241, loss = 0.21870598\n",
      "Iteration 242, loss = 0.21847018\n",
      "Iteration 243, loss = 0.21846108\n",
      "Iteration 244, loss = 0.21779238\n",
      "Iteration 245, loss = 0.21803356\n",
      "Iteration 246, loss = 0.21846429\n",
      "Iteration 247, loss = 0.21755583\n",
      "Iteration 248, loss = 0.21683318\n",
      "Iteration 249, loss = 0.21667599\n",
      "Iteration 250, loss = 0.21721803\n",
      "Iteration 251, loss = 0.21767803\n",
      "Iteration 252, loss = 0.21672387\n",
      "Iteration 253, loss = 0.21668937\n",
      "Iteration 254, loss = 0.21577892\n",
      "Iteration 255, loss = 0.21503783\n",
      "Iteration 256, loss = 0.21522869\n",
      "Iteration 257, loss = 0.21452534\n",
      "Iteration 258, loss = 0.21438718\n",
      "Iteration 259, loss = 0.21424417\n",
      "Iteration 260, loss = 0.21383076\n",
      "Iteration 261, loss = 0.21358364\n",
      "Iteration 262, loss = 0.21323210\n",
      "Iteration 263, loss = 0.21323794\n",
      "Iteration 264, loss = 0.21356727\n",
      "Iteration 265, loss = 0.21313382\n",
      "Iteration 266, loss = 0.21294011\n",
      "Iteration 267, loss = 0.21284297\n",
      "Iteration 268, loss = 0.21244008\n",
      "Iteration 269, loss = 0.21211985\n",
      "Iteration 270, loss = 0.21192998\n",
      "Iteration 271, loss = 0.21267275\n",
      "Iteration 272, loss = 0.21187541\n",
      "Iteration 273, loss = 0.21114531\n",
      "Iteration 274, loss = 0.21200909\n",
      "Iteration 275, loss = 0.21325058\n",
      "Iteration 276, loss = 0.21081460\n",
      "Iteration 277, loss = 0.21013901\n",
      "Iteration 278, loss = 0.21053822\n",
      "Iteration 279, loss = 0.21032300\n",
      "Iteration 280, loss = 0.20989256\n",
      "Iteration 281, loss = 0.20967908\n",
      "Iteration 282, loss = 0.21029066\n",
      "Iteration 283, loss = 0.20963963\n",
      "Iteration 284, loss = 0.20922587\n",
      "Iteration 285, loss = 0.20913966\n",
      "Iteration 286, loss = 0.20928017\n",
      "Iteration 287, loss = 0.20927388\n",
      "Iteration 288, loss = 0.20937906\n",
      "Iteration 289, loss = 0.21024328\n",
      "Iteration 290, loss = 0.20941921\n",
      "Iteration 291, loss = 0.20860810\n",
      "Iteration 292, loss = 0.20830647\n",
      "Iteration 293, loss = 0.20785429\n",
      "Iteration 294, loss = 0.20798848\n",
      "Iteration 295, loss = 0.20839199\n",
      "Iteration 296, loss = 0.20792248\n",
      "Iteration 297, loss = 0.20825317\n",
      "Iteration 298, loss = 0.20739784\n",
      "Iteration 299, loss = 0.20726464\n",
      "Iteration 300, loss = 0.20739469\n",
      "Iteration 301, loss = 0.20753130\n",
      "Iteration 302, loss = 0.20710422\n",
      "Iteration 303, loss = 0.20677829\n",
      "Iteration 304, loss = 0.20729186\n",
      "Iteration 305, loss = 0.20710948\n",
      "Iteration 306, loss = 0.20619578\n",
      "Iteration 307, loss = 0.20610927\n",
      "Iteration 308, loss = 0.20626001\n",
      "Iteration 309, loss = 0.20601319\n",
      "Iteration 310, loss = 0.20593782\n",
      "Iteration 311, loss = 0.20589817\n",
      "Iteration 312, loss = 0.20625421\n",
      "Iteration 313, loss = 0.20578223\n",
      "Iteration 314, loss = 0.20611025\n",
      "Iteration 315, loss = 0.20516438\n",
      "Iteration 316, loss = 0.20513177\n",
      "Iteration 317, loss = 0.20522289\n",
      "Iteration 318, loss = 0.20570879\n",
      "Iteration 319, loss = 0.20497267\n",
      "Iteration 320, loss = 0.20496856\n",
      "Iteration 321, loss = 0.20450495\n",
      "Iteration 322, loss = 0.20434468\n",
      "Iteration 323, loss = 0.20451071\n",
      "Iteration 324, loss = 0.20435226\n",
      "Iteration 325, loss = 0.20399045\n",
      "Iteration 326, loss = 0.20382817\n",
      "Iteration 327, loss = 0.20385081\n",
      "Iteration 328, loss = 0.20389521\n",
      "Iteration 329, loss = 0.20362058\n",
      "Iteration 330, loss = 0.20370674\n",
      "Iteration 331, loss = 0.20344247\n",
      "Iteration 332, loss = 0.20333430\n",
      "Iteration 333, loss = 0.20380524\n",
      "Iteration 334, loss = 0.20327277\n",
      "Iteration 335, loss = 0.20302534\n",
      "Iteration 336, loss = 0.20265623\n",
      "Iteration 337, loss = 0.20244577\n",
      "Iteration 338, loss = 0.20243743\n",
      "Iteration 339, loss = 0.20269697\n",
      "Iteration 340, loss = 0.20246898\n",
      "Iteration 341, loss = 0.20284931\n",
      "Iteration 342, loss = 0.20209760\n",
      "Iteration 343, loss = 0.20221479\n",
      "Iteration 344, loss = 0.20174030\n",
      "Iteration 345, loss = 0.20234777\n",
      "Iteration 346, loss = 0.20197253\n",
      "Iteration 347, loss = 0.20169864\n",
      "Iteration 348, loss = 0.20171681\n",
      "Iteration 349, loss = 0.20145628\n",
      "Iteration 350, loss = 0.20141072\n",
      "Iteration 351, loss = 0.20127006\n",
      "Iteration 352, loss = 0.20208950\n",
      "Iteration 353, loss = 0.20133603\n",
      "Iteration 354, loss = 0.20131973\n",
      "Iteration 355, loss = 0.20093690\n",
      "Iteration 356, loss = 0.20071221\n",
      "Iteration 357, loss = 0.20129349\n",
      "Iteration 358, loss = 0.20121213\n",
      "Iteration 359, loss = 0.20037417\n",
      "Iteration 360, loss = 0.20083081\n",
      "Iteration 361, loss = 0.20102742\n",
      "Iteration 362, loss = 0.20043302\n",
      "Iteration 363, loss = 0.20026279\n",
      "Iteration 364, loss = 0.20073187\n",
      "Iteration 365, loss = 0.19995004\n",
      "Iteration 366, loss = 0.20067730\n",
      "Iteration 367, loss = 0.19967359\n",
      "Iteration 368, loss = 0.20007195\n",
      "Iteration 369, loss = 0.19955531\n",
      "Iteration 370, loss = 0.19951283\n",
      "Iteration 371, loss = 0.19964646\n",
      "Iteration 372, loss = 0.19935518\n",
      "Iteration 373, loss = 0.19946508\n",
      "Iteration 374, loss = 0.19936890\n",
      "Iteration 375, loss = 0.19950253\n",
      "Iteration 376, loss = 0.19936238\n",
      "Iteration 377, loss = 0.19916486\n",
      "Iteration 378, loss = 0.19883227\n",
      "Iteration 379, loss = 0.19895955\n",
      "Iteration 380, loss = 0.19960382\n",
      "Iteration 381, loss = 0.19893629\n",
      "Iteration 382, loss = 0.19856980\n",
      "Iteration 383, loss = 0.19863950\n",
      "Iteration 384, loss = 0.19909810\n",
      "Iteration 385, loss = 0.20026179\n",
      "Iteration 386, loss = 0.19863747\n",
      "Iteration 387, loss = 0.19851317\n",
      "Iteration 388, loss = 0.19844627\n",
      "Iteration 389, loss = 0.19853395\n",
      "Iteration 390, loss = 0.19804173\n",
      "Iteration 391, loss = 0.19836032\n",
      "Iteration 392, loss = 0.19863997\n",
      "Iteration 393, loss = 0.19869321\n",
      "Iteration 394, loss = 0.19981609\n",
      "Iteration 395, loss = 0.19802776\n",
      "Iteration 396, loss = 0.19806833\n",
      "Iteration 397, loss = 0.19819591\n",
      "Iteration 398, loss = 0.19791620\n",
      "Iteration 399, loss = 0.19794508\n",
      "Iteration 400, loss = 0.19770793\n",
      "Iteration 401, loss = 0.19765917\n",
      "Iteration 402, loss = 0.19755526\n",
      "Iteration 403, loss = 0.19746637\n",
      "Iteration 404, loss = 0.19752465\n",
      "Iteration 405, loss = 0.19757019\n",
      "Iteration 406, loss = 0.19778796\n",
      "Iteration 407, loss = 0.19782533\n",
      "Iteration 408, loss = 0.19755981\n",
      "Iteration 409, loss = 0.19828784\n",
      "Iteration 410, loss = 0.19723648\n",
      "Iteration 411, loss = 0.19699155\n",
      "Iteration 412, loss = 0.19706389\n",
      "Iteration 413, loss = 0.19704311\n",
      "Iteration 414, loss = 0.19713378\n",
      "Iteration 415, loss = 0.19727166\n",
      "Iteration 416, loss = 0.19666819\n",
      "Iteration 417, loss = 0.19754940\n",
      "Iteration 418, loss = 0.19702192\n",
      "Iteration 419, loss = 0.19706387\n",
      "Iteration 420, loss = 0.19712448\n",
      "Iteration 421, loss = 0.19702849\n",
      "Iteration 422, loss = 0.19677655\n",
      "Iteration 423, loss = 0.19642360\n",
      "Iteration 424, loss = 0.19636552\n",
      "Iteration 425, loss = 0.19702670\n",
      "Iteration 426, loss = 0.19681548\n",
      "Iteration 427, loss = 0.19690284\n",
      "Iteration 428, loss = 0.19649819\n",
      "Iteration 429, loss = 0.19696316\n",
      "Iteration 430, loss = 0.19649022\n",
      "Iteration 431, loss = 0.19641036\n",
      "Iteration 432, loss = 0.19635935\n",
      "Iteration 433, loss = 0.19671835\n",
      "Iteration 434, loss = 0.19696655\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76987375\n",
      "Iteration 2, loss = 0.74365352\n",
      "Iteration 3, loss = 0.72181592\n",
      "Iteration 4, loss = 0.70427059\n",
      "Iteration 5, loss = 0.69052405\n",
      "Iteration 6, loss = 0.67753309\n",
      "Iteration 7, loss = 0.66378522\n",
      "Iteration 8, loss = 0.64826490\n",
      "Iteration 9, loss = 0.63027534\n",
      "Iteration 10, loss = 0.61147120\n",
      "Iteration 11, loss = 0.59192337\n",
      "Iteration 12, loss = 0.57368420\n",
      "Iteration 13, loss = 0.55682073\n",
      "Iteration 14, loss = 0.54242721\n",
      "Iteration 15, loss = 0.53014177\n",
      "Iteration 16, loss = 0.51970866\n",
      "Iteration 17, loss = 0.51019840\n",
      "Iteration 18, loss = 0.50214874\n",
      "Iteration 19, loss = 0.49491835\n",
      "Iteration 20, loss = 0.48839707\n",
      "Iteration 21, loss = 0.48198654\n",
      "Iteration 22, loss = 0.47624061\n",
      "Iteration 23, loss = 0.47045669\n",
      "Iteration 24, loss = 0.46551376\n",
      "Iteration 25, loss = 0.46040174\n",
      "Iteration 26, loss = 0.45596154\n",
      "Iteration 27, loss = 0.45154235\n",
      "Iteration 28, loss = 0.44725631\n",
      "Iteration 29, loss = 0.44329409\n",
      "Iteration 30, loss = 0.43948710\n",
      "Iteration 31, loss = 0.43561336\n",
      "Iteration 32, loss = 0.43193551\n",
      "Iteration 33, loss = 0.42842465\n",
      "Iteration 34, loss = 0.42510937\n",
      "Iteration 35, loss = 0.42166812\n",
      "Iteration 36, loss = 0.41838884\n",
      "Iteration 37, loss = 0.41494532\n",
      "Iteration 38, loss = 0.41177606\n",
      "Iteration 39, loss = 0.40863367\n",
      "Iteration 40, loss = 0.40551831\n",
      "Iteration 41, loss = 0.40234468\n",
      "Iteration 42, loss = 0.39950150\n",
      "Iteration 43, loss = 0.39671021\n",
      "Iteration 44, loss = 0.39398651\n",
      "Iteration 45, loss = 0.39120320\n",
      "Iteration 46, loss = 0.38850626\n",
      "Iteration 47, loss = 0.38585622\n",
      "Iteration 48, loss = 0.38346911\n",
      "Iteration 49, loss = 0.38110022\n",
      "Iteration 50, loss = 0.37847137\n",
      "Iteration 51, loss = 0.37592849\n",
      "Iteration 52, loss = 0.37355546\n",
      "Iteration 53, loss = 0.37139673\n",
      "Iteration 54, loss = 0.36909103\n",
      "Iteration 55, loss = 0.36682350\n",
      "Iteration 56, loss = 0.36476379\n",
      "Iteration 57, loss = 0.36226803\n",
      "Iteration 58, loss = 0.36022591\n",
      "Iteration 59, loss = 0.35828182\n",
      "Iteration 60, loss = 0.35611305\n",
      "Iteration 61, loss = 0.35444679\n",
      "Iteration 62, loss = 0.35221562\n",
      "Iteration 63, loss = 0.35035468\n",
      "Iteration 64, loss = 0.34847337\n",
      "Iteration 65, loss = 0.34677862\n",
      "Iteration 66, loss = 0.34496380\n",
      "Iteration 67, loss = 0.34314485\n",
      "Iteration 68, loss = 0.34144397\n",
      "Iteration 69, loss = 0.33963533\n",
      "Iteration 70, loss = 0.33814762\n",
      "Iteration 71, loss = 0.33629327\n",
      "Iteration 72, loss = 0.33487973\n",
      "Iteration 73, loss = 0.33326832\n",
      "Iteration 74, loss = 0.33172611\n",
      "Iteration 75, loss = 0.33035938\n",
      "Iteration 76, loss = 0.32875263\n",
      "Iteration 77, loss = 0.32728552\n",
      "Iteration 78, loss = 0.32586730\n",
      "Iteration 79, loss = 0.32446777\n",
      "Iteration 80, loss = 0.32307119\n",
      "Iteration 81, loss = 0.32170499\n",
      "Iteration 82, loss = 0.32021685\n",
      "Iteration 83, loss = 0.31891351\n",
      "Iteration 84, loss = 0.31742133\n",
      "Iteration 85, loss = 0.31614247\n",
      "Iteration 86, loss = 0.31490189\n",
      "Iteration 87, loss = 0.31359967\n",
      "Iteration 88, loss = 0.31229369\n",
      "Iteration 89, loss = 0.31127424\n",
      "Iteration 90, loss = 0.30983312\n",
      "Iteration 91, loss = 0.30913115\n",
      "Iteration 92, loss = 0.30763714\n",
      "Iteration 93, loss = 0.30656074\n",
      "Iteration 94, loss = 0.30535273\n",
      "Iteration 95, loss = 0.30425234\n",
      "Iteration 96, loss = 0.30307470\n",
      "Iteration 97, loss = 0.30212716\n",
      "Iteration 98, loss = 0.30103442\n",
      "Iteration 99, loss = 0.30005731\n",
      "Iteration 100, loss = 0.29906138\n",
      "Iteration 101, loss = 0.29798533\n",
      "Iteration 102, loss = 0.29683652\n",
      "Iteration 103, loss = 0.29600462\n",
      "Iteration 104, loss = 0.29514865\n",
      "Iteration 105, loss = 0.29407410\n",
      "Iteration 106, loss = 0.29308958\n",
      "Iteration 107, loss = 0.29189047\n",
      "Iteration 108, loss = 0.29109866\n",
      "Iteration 109, loss = 0.29012312\n",
      "Iteration 110, loss = 0.28954952\n",
      "Iteration 111, loss = 0.28829303\n",
      "Iteration 112, loss = 0.28741888\n",
      "Iteration 113, loss = 0.28650571\n",
      "Iteration 114, loss = 0.28561700\n",
      "Iteration 115, loss = 0.28467544\n",
      "Iteration 116, loss = 0.28347738\n",
      "Iteration 117, loss = 0.28258568\n",
      "Iteration 118, loss = 0.28136254\n",
      "Iteration 119, loss = 0.28074027\n",
      "Iteration 120, loss = 0.27903509\n",
      "Iteration 121, loss = 0.27841889\n",
      "Iteration 122, loss = 0.27723413\n",
      "Iteration 123, loss = 0.27630551\n",
      "Iteration 124, loss = 0.27549243\n",
      "Iteration 125, loss = 0.27464546\n",
      "Iteration 126, loss = 0.27364464\n",
      "Iteration 127, loss = 0.27325515\n",
      "Iteration 128, loss = 0.27202150\n",
      "Iteration 129, loss = 0.27130302\n",
      "Iteration 130, loss = 0.27046671\n",
      "Iteration 131, loss = 0.26994142\n",
      "Iteration 132, loss = 0.26916169\n",
      "Iteration 133, loss = 0.26891282\n",
      "Iteration 134, loss = 0.26766335\n",
      "Iteration 135, loss = 0.26677696\n",
      "Iteration 136, loss = 0.26647729\n",
      "Iteration 137, loss = 0.26535916\n",
      "Iteration 138, loss = 0.26528229\n",
      "Iteration 139, loss = 0.26396986\n",
      "Iteration 140, loss = 0.26330551\n",
      "Iteration 141, loss = 0.26299298\n",
      "Iteration 142, loss = 0.26289404\n",
      "Iteration 143, loss = 0.26146389\n",
      "Iteration 144, loss = 0.26070367\n",
      "Iteration 145, loss = 0.26078741\n",
      "Iteration 146, loss = 0.25948712\n",
      "Iteration 147, loss = 0.25866908\n",
      "Iteration 148, loss = 0.25826762\n",
      "Iteration 149, loss = 0.25774236\n",
      "Iteration 150, loss = 0.25684996\n",
      "Iteration 151, loss = 0.25640282\n",
      "Iteration 152, loss = 0.25560593\n",
      "Iteration 153, loss = 0.25503688\n",
      "Iteration 154, loss = 0.25502483\n",
      "Iteration 155, loss = 0.25399675\n",
      "Iteration 156, loss = 0.25350832\n",
      "Iteration 157, loss = 0.25286337\n",
      "Iteration 158, loss = 0.25236378\n",
      "Iteration 159, loss = 0.25154000\n",
      "Iteration 160, loss = 0.25109381\n",
      "Iteration 161, loss = 0.25045699\n",
      "Iteration 162, loss = 0.25065607\n",
      "Iteration 163, loss = 0.24987584\n",
      "Iteration 164, loss = 0.24870280\n",
      "Iteration 165, loss = 0.24904072\n",
      "Iteration 166, loss = 0.24804642\n",
      "Iteration 167, loss = 0.24746109\n",
      "Iteration 168, loss = 0.24715432\n",
      "Iteration 169, loss = 0.24724816\n",
      "Iteration 170, loss = 0.24666546\n",
      "Iteration 171, loss = 0.24532278\n",
      "Iteration 172, loss = 0.24514310\n",
      "Iteration 173, loss = 0.24440405\n",
      "Iteration 174, loss = 0.24390908\n",
      "Iteration 175, loss = 0.24358405\n",
      "Iteration 176, loss = 0.24294092\n",
      "Iteration 177, loss = 0.24245000\n",
      "Iteration 178, loss = 0.24190672\n",
      "Iteration 179, loss = 0.24175949\n",
      "Iteration 180, loss = 0.24137820\n",
      "Iteration 181, loss = 0.24119523\n",
      "Iteration 182, loss = 0.24033416\n",
      "Iteration 183, loss = 0.23983071\n",
      "Iteration 184, loss = 0.23969934\n",
      "Iteration 185, loss = 0.23920284\n",
      "Iteration 186, loss = 0.23864806\n",
      "Iteration 187, loss = 0.23815356\n",
      "Iteration 188, loss = 0.23830424\n",
      "Iteration 189, loss = 0.23746618\n",
      "Iteration 190, loss = 0.23672019\n",
      "Iteration 191, loss = 0.23641918\n",
      "Iteration 192, loss = 0.23612136\n",
      "Iteration 193, loss = 0.23569883\n",
      "Iteration 194, loss = 0.23522999\n",
      "Iteration 195, loss = 0.23466650\n",
      "Iteration 196, loss = 0.23511001\n",
      "Iteration 197, loss = 0.23417472\n",
      "Iteration 198, loss = 0.23406222\n",
      "Iteration 199, loss = 0.23319106\n",
      "Iteration 200, loss = 0.23249667\n",
      "Iteration 201, loss = 0.23229572\n",
      "Iteration 202, loss = 0.23209014\n",
      "Iteration 203, loss = 0.23207836\n",
      "Iteration 204, loss = 0.23130642\n",
      "Iteration 205, loss = 0.23064302\n",
      "Iteration 206, loss = 0.23031230\n",
      "Iteration 207, loss = 0.23034109\n",
      "Iteration 208, loss = 0.22982834\n",
      "Iteration 209, loss = 0.22936564\n",
      "Iteration 210, loss = 0.22919902\n",
      "Iteration 211, loss = 0.22870631\n",
      "Iteration 212, loss = 0.22824557\n",
      "Iteration 213, loss = 0.22851517\n",
      "Iteration 214, loss = 0.22782772\n",
      "Iteration 215, loss = 0.22851096\n",
      "Iteration 216, loss = 0.22661547\n",
      "Iteration 217, loss = 0.22675465\n",
      "Iteration 218, loss = 0.22633411\n",
      "Iteration 219, loss = 0.22609391\n",
      "Iteration 220, loss = 0.22568796\n",
      "Iteration 221, loss = 0.22500502\n",
      "Iteration 222, loss = 0.22493469\n",
      "Iteration 223, loss = 0.22434931\n",
      "Iteration 224, loss = 0.22461544\n",
      "Iteration 225, loss = 0.22429794\n",
      "Iteration 226, loss = 0.22348582\n",
      "Iteration 227, loss = 0.22298810\n",
      "Iteration 228, loss = 0.22361157\n",
      "Iteration 229, loss = 0.22342929\n",
      "Iteration 230, loss = 0.22362785\n",
      "Iteration 231, loss = 0.22242509\n",
      "Iteration 232, loss = 0.22202440\n",
      "Iteration 233, loss = 0.22167082\n",
      "Iteration 234, loss = 0.22098535\n",
      "Iteration 235, loss = 0.22090432\n",
      "Iteration 236, loss = 0.22059923\n",
      "Iteration 237, loss = 0.22020002\n",
      "Iteration 238, loss = 0.22055162\n",
      "Iteration 239, loss = 0.22021411\n",
      "Iteration 240, loss = 0.21944868\n",
      "Iteration 241, loss = 0.21929257\n",
      "Iteration 242, loss = 0.21902930\n",
      "Iteration 243, loss = 0.21961156\n",
      "Iteration 244, loss = 0.21848757\n",
      "Iteration 245, loss = 0.21824463\n",
      "Iteration 246, loss = 0.21801295\n",
      "Iteration 247, loss = 0.21794483\n",
      "Iteration 248, loss = 0.21758806\n",
      "Iteration 249, loss = 0.21768781\n",
      "Iteration 250, loss = 0.21686547\n",
      "Iteration 251, loss = 0.21647223\n",
      "Iteration 252, loss = 0.21654119\n",
      "Iteration 253, loss = 0.21681994\n",
      "Iteration 254, loss = 0.21592340\n",
      "Iteration 255, loss = 0.21605381\n",
      "Iteration 256, loss = 0.21597799\n",
      "Iteration 257, loss = 0.21530212\n",
      "Iteration 258, loss = 0.21516905\n",
      "Iteration 259, loss = 0.21480236\n",
      "Iteration 260, loss = 0.21459118\n",
      "Iteration 261, loss = 0.21456473\n",
      "Iteration 262, loss = 0.21449086\n",
      "Iteration 263, loss = 0.21416390\n",
      "Iteration 264, loss = 0.21365121\n",
      "Iteration 265, loss = 0.21357259\n",
      "Iteration 266, loss = 0.21359603\n",
      "Iteration 267, loss = 0.21352794\n",
      "Iteration 268, loss = 0.21292869\n",
      "Iteration 269, loss = 0.21318518\n",
      "Iteration 270, loss = 0.21255108\n",
      "Iteration 271, loss = 0.21259715\n",
      "Iteration 272, loss = 0.21303519\n",
      "Iteration 273, loss = 0.21259437\n",
      "Iteration 274, loss = 0.21209768\n",
      "Iteration 275, loss = 0.21189895\n",
      "Iteration 276, loss = 0.21159460\n",
      "Iteration 277, loss = 0.21197752\n",
      "Iteration 278, loss = 0.21120417\n",
      "Iteration 279, loss = 0.21142021\n",
      "Iteration 280, loss = 0.21097176\n",
      "Iteration 281, loss = 0.21069578\n",
      "Iteration 282, loss = 0.21045297\n",
      "Iteration 283, loss = 0.21018655\n",
      "Iteration 284, loss = 0.21008452\n",
      "Iteration 285, loss = 0.21038329\n",
      "Iteration 286, loss = 0.21044581\n",
      "Iteration 287, loss = 0.20990860\n",
      "Iteration 288, loss = 0.20946802\n",
      "Iteration 289, loss = 0.20933580\n",
      "Iteration 290, loss = 0.20991504\n",
      "Iteration 291, loss = 0.20912583\n",
      "Iteration 292, loss = 0.20895672\n",
      "Iteration 293, loss = 0.20871830\n",
      "Iteration 294, loss = 0.20890051\n",
      "Iteration 295, loss = 0.20833791\n",
      "Iteration 296, loss = 0.20838545\n",
      "Iteration 297, loss = 0.20871975\n",
      "Iteration 298, loss = 0.20840547\n",
      "Iteration 299, loss = 0.20773792\n",
      "Iteration 300, loss = 0.20797031\n",
      "Iteration 301, loss = 0.20682365\n",
      "Iteration 302, loss = 0.20726655\n",
      "Iteration 303, loss = 0.20682300\n",
      "Iteration 304, loss = 0.20641992\n",
      "Iteration 305, loss = 0.20613151\n",
      "Iteration 306, loss = 0.20667586\n",
      "Iteration 307, loss = 0.20637579\n",
      "Iteration 308, loss = 0.20595055\n",
      "Iteration 309, loss = 0.20614703\n",
      "Iteration 310, loss = 0.20543020\n",
      "Iteration 311, loss = 0.20558224\n",
      "Iteration 312, loss = 0.20545507\n",
      "Iteration 313, loss = 0.20512089\n",
      "Iteration 314, loss = 0.20494601\n",
      "Iteration 315, loss = 0.20469883\n",
      "Iteration 316, loss = 0.20450714\n",
      "Iteration 317, loss = 0.20457883\n",
      "Iteration 318, loss = 0.20418396\n",
      "Iteration 319, loss = 0.20433429\n",
      "Iteration 320, loss = 0.20372930\n",
      "Iteration 321, loss = 0.20368051\n",
      "Iteration 322, loss = 0.20357808\n",
      "Iteration 323, loss = 0.20372636\n",
      "Iteration 324, loss = 0.20363493\n",
      "Iteration 325, loss = 0.20358791\n",
      "Iteration 326, loss = 0.20346124\n",
      "Iteration 327, loss = 0.20330720\n",
      "Iteration 328, loss = 0.20266285\n",
      "Iteration 329, loss = 0.20267372\n",
      "Iteration 330, loss = 0.20253025\n",
      "Iteration 331, loss = 0.20265020\n",
      "Iteration 332, loss = 0.20277920\n",
      "Iteration 333, loss = 0.20213675\n",
      "Iteration 334, loss = 0.20256334\n",
      "Iteration 335, loss = 0.20183315\n",
      "Iteration 336, loss = 0.20223559\n",
      "Iteration 337, loss = 0.20157532\n",
      "Iteration 338, loss = 0.20219061\n",
      "Iteration 339, loss = 0.20192237\n",
      "Iteration 340, loss = 0.20165677\n",
      "Iteration 341, loss = 0.20142396\n",
      "Iteration 342, loss = 0.20109566\n",
      "Iteration 343, loss = 0.20124963\n",
      "Iteration 344, loss = 0.20117812\n",
      "Iteration 345, loss = 0.20079303\n",
      "Iteration 346, loss = 0.20071370\n",
      "Iteration 347, loss = 0.20065631\n",
      "Iteration 348, loss = 0.20084831\n",
      "Iteration 349, loss = 0.20089554\n",
      "Iteration 350, loss = 0.20090420\n",
      "Iteration 351, loss = 0.20008225\n",
      "Iteration 352, loss = 0.20051987\n",
      "Iteration 353, loss = 0.19988391\n",
      "Iteration 354, loss = 0.19977478\n",
      "Iteration 355, loss = 0.20051424\n",
      "Iteration 356, loss = 0.20074559\n",
      "Iteration 357, loss = 0.19963489\n",
      "Iteration 358, loss = 0.19948753\n",
      "Iteration 359, loss = 0.19936843\n",
      "Iteration 360, loss = 0.19967992\n",
      "Iteration 361, loss = 0.19914968\n",
      "Iteration 362, loss = 0.19910919\n",
      "Iteration 363, loss = 0.19883438\n",
      "Iteration 364, loss = 0.19896512\n",
      "Iteration 365, loss = 0.19924196\n",
      "Iteration 366, loss = 0.19885876\n",
      "Iteration 367, loss = 0.19914903\n",
      "Iteration 368, loss = 0.19844893\n",
      "Iteration 369, loss = 0.19840959\n",
      "Iteration 370, loss = 0.19866325\n",
      "Iteration 371, loss = 0.19836650\n",
      "Iteration 372, loss = 0.19806543\n",
      "Iteration 373, loss = 0.19773441\n",
      "Iteration 374, loss = 0.19802893\n",
      "Iteration 375, loss = 0.19753981\n",
      "Iteration 376, loss = 0.19783383\n",
      "Iteration 377, loss = 0.19761811\n",
      "Iteration 378, loss = 0.19735912\n",
      "Iteration 379, loss = 0.19755714\n",
      "Iteration 380, loss = 0.19736114\n",
      "Iteration 381, loss = 0.19692966\n",
      "Iteration 382, loss = 0.19702671\n",
      "Iteration 383, loss = 0.19654446\n",
      "Iteration 384, loss = 0.19677575\n",
      "Iteration 385, loss = 0.19722241\n",
      "Iteration 386, loss = 0.19675067\n",
      "Iteration 387, loss = 0.19639678\n",
      "Iteration 388, loss = 0.19656065\n",
      "Iteration 389, loss = 0.19676067\n",
      "Iteration 390, loss = 0.19645273\n",
      "Iteration 391, loss = 0.19600808\n",
      "Iteration 392, loss = 0.19581068\n",
      "Iteration 393, loss = 0.19572742\n",
      "Iteration 394, loss = 0.19596878\n",
      "Iteration 395, loss = 0.19560833\n",
      "Iteration 396, loss = 0.19587119\n",
      "Iteration 397, loss = 0.19530186\n",
      "Iteration 398, loss = 0.19562795\n",
      "Iteration 399, loss = 0.19576629\n",
      "Iteration 400, loss = 0.19519216\n",
      "Iteration 401, loss = 0.19721738\n",
      "Iteration 402, loss = 0.19527346\n",
      "Iteration 403, loss = 0.19501640\n",
      "Iteration 404, loss = 0.19500871\n",
      "Iteration 405, loss = 0.19529979\n",
      "Iteration 406, loss = 0.19442442\n",
      "Iteration 407, loss = 0.19459602\n",
      "Iteration 408, loss = 0.19464001\n",
      "Iteration 409, loss = 0.19436612\n",
      "Iteration 410, loss = 0.19447770\n",
      "Iteration 411, loss = 0.19417957\n",
      "Iteration 412, loss = 0.19417774\n",
      "Iteration 413, loss = 0.19426124\n",
      "Iteration 414, loss = 0.19402372\n",
      "Iteration 415, loss = 0.19398020\n",
      "Iteration 416, loss = 0.19365262\n",
      "Iteration 417, loss = 0.19361730\n",
      "Iteration 418, loss = 0.19394307\n",
      "Iteration 419, loss = 0.19395944\n",
      "Iteration 420, loss = 0.19359485\n",
      "Iteration 421, loss = 0.19337583\n",
      "Iteration 422, loss = 0.19337904\n",
      "Iteration 423, loss = 0.19394754\n",
      "Iteration 424, loss = 0.19396964\n",
      "Iteration 425, loss = 0.19423819\n",
      "Iteration 426, loss = 0.19378119\n",
      "Iteration 427, loss = 0.19270495\n",
      "Iteration 428, loss = 0.19298727\n",
      "Iteration 429, loss = 0.19282122\n",
      "Iteration 430, loss = 0.19264781\n",
      "Iteration 431, loss = 0.19277064\n",
      "Iteration 432, loss = 0.19255765\n",
      "Iteration 433, loss = 0.19218551\n",
      "Iteration 434, loss = 0.19253169\n",
      "Iteration 435, loss = 0.19267298\n",
      "Iteration 436, loss = 0.19254998\n",
      "Iteration 437, loss = 0.19214854\n",
      "Iteration 438, loss = 0.19233491\n",
      "Iteration 439, loss = 0.19175226\n",
      "Iteration 440, loss = 0.19178526\n",
      "Iteration 441, loss = 0.19191528\n",
      "Iteration 442, loss = 0.19167370\n",
      "Iteration 443, loss = 0.19134530\n",
      "Iteration 444, loss = 0.19197775\n",
      "Iteration 445, loss = 0.19189970\n",
      "Iteration 446, loss = 0.19171839\n",
      "Iteration 447, loss = 0.19218876\n",
      "Iteration 448, loss = 0.19104697\n",
      "Iteration 449, loss = 0.19154175\n",
      "Iteration 450, loss = 0.19206208\n",
      "Iteration 451, loss = 0.19092355\n",
      "Iteration 452, loss = 0.19113482\n",
      "Iteration 453, loss = 0.19092577\n",
      "Iteration 454, loss = 0.19092063\n",
      "Iteration 455, loss = 0.19127872\n",
      "Iteration 456, loss = 0.19090910\n",
      "Iteration 457, loss = 0.19084009\n",
      "Iteration 458, loss = 0.19058010\n",
      "Iteration 459, loss = 0.19027770\n",
      "Iteration 460, loss = 0.19046182\n",
      "Iteration 461, loss = 0.19039066\n",
      "Iteration 462, loss = 0.19037787\n",
      "Iteration 463, loss = 0.18993534\n",
      "Iteration 464, loss = 0.19027164\n",
      "Iteration 465, loss = 0.18990666\n",
      "Iteration 466, loss = 0.19027411\n",
      "Iteration 467, loss = 0.19147407\n",
      "Iteration 468, loss = 0.19052258\n",
      "Iteration 469, loss = 0.18975110\n",
      "Iteration 470, loss = 0.18974711\n",
      "Iteration 471, loss = 0.18939355\n",
      "Iteration 472, loss = 0.18950267\n",
      "Iteration 473, loss = 0.18922481\n",
      "Iteration 474, loss = 0.18899915\n",
      "Iteration 475, loss = 0.18914119\n",
      "Iteration 476, loss = 0.18982681\n",
      "Iteration 477, loss = 0.18906473\n",
      "Iteration 478, loss = 0.18884306\n",
      "Iteration 479, loss = 0.18909734\n",
      "Iteration 480, loss = 0.18937651\n",
      "Iteration 481, loss = 0.18861312\n",
      "Iteration 482, loss = 0.18920269\n",
      "Iteration 483, loss = 0.18857297\n",
      "Iteration 484, loss = 0.18950266\n",
      "Iteration 485, loss = 0.18905044\n",
      "Iteration 486, loss = 0.18881680\n",
      "Iteration 487, loss = 0.18849497\n",
      "Iteration 488, loss = 0.18828162\n",
      "Iteration 489, loss = 0.18881929\n",
      "Iteration 490, loss = 0.18893139\n",
      "Iteration 491, loss = 0.18775721\n",
      "Iteration 492, loss = 0.18850254\n",
      "Iteration 493, loss = 0.18780590\n",
      "Iteration 494, loss = 0.18759423\n",
      "Iteration 495, loss = 0.18840566\n",
      "Iteration 496, loss = 0.18802000\n",
      "Iteration 497, loss = 0.18886263\n",
      "Iteration 498, loss = 0.18767819\n",
      "Iteration 499, loss = 0.18781226\n",
      "Iteration 500, loss = 0.18769439\n",
      "Iteration 501, loss = 0.18724522\n",
      "Iteration 502, loss = 0.18732749\n",
      "Iteration 503, loss = 0.18719145\n",
      "Iteration 504, loss = 0.18795834\n",
      "Iteration 505, loss = 0.18915851\n",
      "Iteration 506, loss = 0.18781037\n",
      "Iteration 507, loss = 0.18722441\n",
      "Iteration 508, loss = 0.18829440\n",
      "Iteration 509, loss = 0.18792660\n",
      "Iteration 510, loss = 0.18756089\n",
      "Iteration 511, loss = 0.18702480\n",
      "Iteration 512, loss = 0.18670729\n",
      "Iteration 513, loss = 0.18668974\n",
      "Iteration 514, loss = 0.18673271\n",
      "Iteration 515, loss = 0.18749866\n",
      "Iteration 516, loss = 0.18667964\n",
      "Iteration 517, loss = 0.18646352\n",
      "Iteration 518, loss = 0.18599847\n",
      "Iteration 519, loss = 0.18657002\n",
      "Iteration 520, loss = 0.18625718\n",
      "Iteration 521, loss = 0.18614468\n",
      "Iteration 522, loss = 0.18677039\n",
      "Iteration 523, loss = 0.18642792\n",
      "Iteration 524, loss = 0.18667222\n",
      "Iteration 525, loss = 0.18589810\n",
      "Iteration 526, loss = 0.18587139\n",
      "Iteration 527, loss = 0.18606445\n",
      "Iteration 528, loss = 0.18601366\n",
      "Iteration 529, loss = 0.18584067\n",
      "Iteration 530, loss = 0.18606611\n",
      "Iteration 531, loss = 0.18574983\n",
      "Iteration 532, loss = 0.18567620\n",
      "Iteration 533, loss = 0.18610798\n",
      "Iteration 534, loss = 0.18642223\n",
      "Iteration 535, loss = 0.18554376\n",
      "Iteration 536, loss = 0.18552464\n",
      "Iteration 537, loss = 0.18682971\n",
      "Iteration 538, loss = 0.18557676\n",
      "Iteration 539, loss = 0.18557953\n",
      "Iteration 540, loss = 0.18515039\n",
      "Iteration 541, loss = 0.18500453\n",
      "Iteration 542, loss = 0.18556666\n",
      "Iteration 543, loss = 0.18625118\n",
      "Iteration 544, loss = 0.18506943\n",
      "Iteration 545, loss = 0.18504480\n",
      "Iteration 546, loss = 0.18526641\n",
      "Iteration 547, loss = 0.18479463\n",
      "Iteration 548, loss = 0.18533868\n",
      "Iteration 549, loss = 0.18593651\n",
      "Iteration 550, loss = 0.18491551\n",
      "Iteration 551, loss = 0.18516839\n",
      "Iteration 552, loss = 0.18461145\n",
      "Iteration 553, loss = 0.18487349\n",
      "Iteration 554, loss = 0.18470852\n",
      "Iteration 555, loss = 0.18454691\n",
      "Iteration 556, loss = 0.18448875\n",
      "Iteration 557, loss = 0.18533032\n",
      "Iteration 558, loss = 0.18474847\n",
      "Iteration 559, loss = 0.18437427\n",
      "Iteration 560, loss = 0.18475420\n",
      "Iteration 561, loss = 0.18456751\n",
      "Iteration 562, loss = 0.18568967\n",
      "Iteration 563, loss = 0.18445367\n",
      "Iteration 564, loss = 0.18427312\n",
      "Iteration 565, loss = 0.18415653\n",
      "Iteration 566, loss = 0.18414800\n",
      "Iteration 567, loss = 0.18471591\n",
      "Iteration 568, loss = 0.18459695\n",
      "Iteration 569, loss = 0.18383251\n",
      "Iteration 570, loss = 0.18393666\n",
      "Iteration 571, loss = 0.18380737\n",
      "Iteration 572, loss = 0.18397149\n",
      "Iteration 573, loss = 0.18382199\n",
      "Iteration 574, loss = 0.18413298\n",
      "Iteration 575, loss = 0.18372916\n",
      "Iteration 576, loss = 0.18386039\n",
      "Iteration 577, loss = 0.18391361\n",
      "Iteration 578, loss = 0.18370138\n",
      "Iteration 579, loss = 0.18413156\n",
      "Iteration 580, loss = 0.18400977\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77048576\n",
      "Iteration 2, loss = 0.74442503\n",
      "Iteration 3, loss = 0.72237915\n",
      "Iteration 4, loss = 0.70529988\n",
      "Iteration 5, loss = 0.69114658\n",
      "Iteration 6, loss = 0.67729802\n",
      "Iteration 7, loss = 0.66218576\n",
      "Iteration 8, loss = 0.64547675\n",
      "Iteration 9, loss = 0.62665634\n",
      "Iteration 10, loss = 0.60692146\n",
      "Iteration 11, loss = 0.58741804\n",
      "Iteration 12, loss = 0.56911444\n",
      "Iteration 13, loss = 0.55337859\n",
      "Iteration 14, loss = 0.53946842\n",
      "Iteration 15, loss = 0.52697337\n",
      "Iteration 16, loss = 0.51723418\n",
      "Iteration 17, loss = 0.50844485\n",
      "Iteration 18, loss = 0.50088875\n",
      "Iteration 19, loss = 0.49407157\n",
      "Iteration 20, loss = 0.48745663\n",
      "Iteration 21, loss = 0.48138354\n",
      "Iteration 22, loss = 0.47528859\n",
      "Iteration 23, loss = 0.47019288\n",
      "Iteration 24, loss = 0.46499980\n",
      "Iteration 25, loss = 0.46035284\n",
      "Iteration 26, loss = 0.45623845\n",
      "Iteration 27, loss = 0.45187476\n",
      "Iteration 28, loss = 0.44776103\n",
      "Iteration 29, loss = 0.44388599\n",
      "Iteration 30, loss = 0.44007909\n",
      "Iteration 31, loss = 0.43620016\n",
      "Iteration 32, loss = 0.43253782\n",
      "Iteration 33, loss = 0.42892806\n",
      "Iteration 34, loss = 0.42552232\n",
      "Iteration 35, loss = 0.42213388\n",
      "Iteration 36, loss = 0.41911207\n",
      "Iteration 37, loss = 0.41554419\n",
      "Iteration 38, loss = 0.41230585\n",
      "Iteration 39, loss = 0.40918277\n",
      "Iteration 40, loss = 0.40630317\n",
      "Iteration 41, loss = 0.40327278\n",
      "Iteration 42, loss = 0.40017427\n",
      "Iteration 43, loss = 0.39736609\n",
      "Iteration 44, loss = 0.39486534\n",
      "Iteration 45, loss = 0.39208823\n",
      "Iteration 46, loss = 0.38933412\n",
      "Iteration 47, loss = 0.38681298\n",
      "Iteration 48, loss = 0.38421245\n",
      "Iteration 49, loss = 0.38190995\n",
      "Iteration 50, loss = 0.37941684\n",
      "Iteration 51, loss = 0.37684939\n",
      "Iteration 52, loss = 0.37458362\n",
      "Iteration 53, loss = 0.37219997\n",
      "Iteration 54, loss = 0.36993805\n",
      "Iteration 55, loss = 0.36762031\n",
      "Iteration 56, loss = 0.36526402\n",
      "Iteration 57, loss = 0.36332206\n",
      "Iteration 58, loss = 0.36093313\n",
      "Iteration 59, loss = 0.35894296\n",
      "Iteration 60, loss = 0.35697758\n",
      "Iteration 61, loss = 0.35478109\n",
      "Iteration 62, loss = 0.35280296\n",
      "Iteration 63, loss = 0.35098105\n",
      "Iteration 64, loss = 0.34895861\n",
      "Iteration 65, loss = 0.34752290\n",
      "Iteration 66, loss = 0.34513893\n",
      "Iteration 67, loss = 0.34329680\n",
      "Iteration 68, loss = 0.34160348\n",
      "Iteration 69, loss = 0.33963505\n",
      "Iteration 70, loss = 0.33799653\n",
      "Iteration 71, loss = 0.33627931\n",
      "Iteration 72, loss = 0.33422225\n",
      "Iteration 73, loss = 0.33259565\n",
      "Iteration 74, loss = 0.33062197\n",
      "Iteration 75, loss = 0.32880387\n",
      "Iteration 76, loss = 0.32755496\n",
      "Iteration 77, loss = 0.32604545\n",
      "Iteration 78, loss = 0.32418161\n",
      "Iteration 79, loss = 0.32258007\n",
      "Iteration 80, loss = 0.32119263\n",
      "Iteration 81, loss = 0.31964574\n",
      "Iteration 82, loss = 0.31820603\n",
      "Iteration 83, loss = 0.31681970\n",
      "Iteration 84, loss = 0.31518611\n",
      "Iteration 85, loss = 0.31366820\n",
      "Iteration 86, loss = 0.31216746\n",
      "Iteration 87, loss = 0.31073214\n",
      "Iteration 88, loss = 0.30936615\n",
      "Iteration 89, loss = 0.30825836\n",
      "Iteration 90, loss = 0.30659353\n",
      "Iteration 91, loss = 0.30521339\n",
      "Iteration 92, loss = 0.30381639\n",
      "Iteration 93, loss = 0.30299628\n",
      "Iteration 94, loss = 0.30135266\n",
      "Iteration 95, loss = 0.30006960\n",
      "Iteration 96, loss = 0.29924352\n",
      "Iteration 97, loss = 0.29831190\n",
      "Iteration 98, loss = 0.29635253\n",
      "Iteration 99, loss = 0.29519556\n",
      "Iteration 100, loss = 0.29402304\n",
      "Iteration 101, loss = 0.29257559\n",
      "Iteration 102, loss = 0.29160396\n",
      "Iteration 103, loss = 0.29081264\n",
      "Iteration 104, loss = 0.28929310\n",
      "Iteration 105, loss = 0.28837856\n",
      "Iteration 106, loss = 0.28714927\n",
      "Iteration 107, loss = 0.28661962\n",
      "Iteration 108, loss = 0.28500380\n",
      "Iteration 109, loss = 0.28397484\n",
      "Iteration 110, loss = 0.28297201\n",
      "Iteration 111, loss = 0.28196292\n",
      "Iteration 112, loss = 0.28121387\n",
      "Iteration 113, loss = 0.28020361\n",
      "Iteration 114, loss = 0.27956197\n",
      "Iteration 115, loss = 0.27804708\n",
      "Iteration 116, loss = 0.27775575\n",
      "Iteration 117, loss = 0.27653007\n",
      "Iteration 118, loss = 0.27546843\n",
      "Iteration 119, loss = 0.27451094\n",
      "Iteration 120, loss = 0.27388483\n",
      "Iteration 121, loss = 0.27337284\n",
      "Iteration 122, loss = 0.27195455\n",
      "Iteration 123, loss = 0.27100470\n",
      "Iteration 124, loss = 0.27017999\n",
      "Iteration 125, loss = 0.26926934\n",
      "Iteration 126, loss = 0.26864737\n",
      "Iteration 127, loss = 0.26769607\n",
      "Iteration 128, loss = 0.26716457\n",
      "Iteration 129, loss = 0.26617757\n",
      "Iteration 130, loss = 0.26569700\n",
      "Iteration 131, loss = 0.26464631\n",
      "Iteration 132, loss = 0.26406579\n",
      "Iteration 133, loss = 0.26331536\n",
      "Iteration 134, loss = 0.26217105\n",
      "Iteration 135, loss = 0.26178500\n",
      "Iteration 136, loss = 0.26069038\n",
      "Iteration 137, loss = 0.26008177\n",
      "Iteration 138, loss = 0.26041984\n",
      "Iteration 139, loss = 0.25861152\n",
      "Iteration 140, loss = 0.25783254\n",
      "Iteration 141, loss = 0.25739917\n",
      "Iteration 142, loss = 0.25664985\n",
      "Iteration 143, loss = 0.25607612\n",
      "Iteration 144, loss = 0.25523338\n",
      "Iteration 145, loss = 0.25481150\n",
      "Iteration 146, loss = 0.25390246\n",
      "Iteration 147, loss = 0.25314654\n",
      "Iteration 148, loss = 0.25247004\n",
      "Iteration 149, loss = 0.25222781\n",
      "Iteration 150, loss = 0.25156349\n",
      "Iteration 151, loss = 0.25098751\n",
      "Iteration 152, loss = 0.25025917\n",
      "Iteration 153, loss = 0.24987559\n",
      "Iteration 154, loss = 0.24922942\n",
      "Iteration 155, loss = 0.24853123\n",
      "Iteration 156, loss = 0.24849714\n",
      "Iteration 157, loss = 0.24725854\n",
      "Iteration 158, loss = 0.24767822\n",
      "Iteration 159, loss = 0.24642457\n",
      "Iteration 160, loss = 0.24618638\n",
      "Iteration 161, loss = 0.24515152\n",
      "Iteration 162, loss = 0.24475472\n",
      "Iteration 163, loss = 0.24428332\n",
      "Iteration 164, loss = 0.24358983\n",
      "Iteration 165, loss = 0.24339994\n",
      "Iteration 166, loss = 0.24267284\n",
      "Iteration 167, loss = 0.24214370\n",
      "Iteration 168, loss = 0.24146643\n",
      "Iteration 169, loss = 0.24125060\n",
      "Iteration 170, loss = 0.24060125\n",
      "Iteration 171, loss = 0.23996936\n",
      "Iteration 172, loss = 0.23944122\n",
      "Iteration 173, loss = 0.23917116\n",
      "Iteration 174, loss = 0.23897513\n",
      "Iteration 175, loss = 0.23792740\n",
      "Iteration 176, loss = 0.23760753\n",
      "Iteration 177, loss = 0.23676443\n",
      "Iteration 178, loss = 0.23709820\n",
      "Iteration 179, loss = 0.23587445\n",
      "Iteration 180, loss = 0.23550953\n",
      "Iteration 181, loss = 0.23493585\n",
      "Iteration 182, loss = 0.23409176\n",
      "Iteration 183, loss = 0.23331542\n",
      "Iteration 184, loss = 0.23312012\n",
      "Iteration 185, loss = 0.23243847\n",
      "Iteration 186, loss = 0.23163121\n",
      "Iteration 187, loss = 0.23107293\n",
      "Iteration 188, loss = 0.23095474\n",
      "Iteration 189, loss = 0.23024916\n",
      "Iteration 190, loss = 0.23009643\n",
      "Iteration 191, loss = 0.22933666\n",
      "Iteration 192, loss = 0.22918393\n",
      "Iteration 193, loss = 0.22895924\n",
      "Iteration 194, loss = 0.22837068\n",
      "Iteration 195, loss = 0.22770773\n",
      "Iteration 196, loss = 0.22764589\n",
      "Iteration 197, loss = 0.22691406\n",
      "Iteration 198, loss = 0.22706691\n",
      "Iteration 199, loss = 0.22730672\n",
      "Iteration 200, loss = 0.22616047\n",
      "Iteration 201, loss = 0.22585669\n",
      "Iteration 202, loss = 0.22509980\n",
      "Iteration 203, loss = 0.22526533\n",
      "Iteration 204, loss = 0.22480143\n",
      "Iteration 205, loss = 0.22453833\n",
      "Iteration 206, loss = 0.22390728\n",
      "Iteration 207, loss = 0.22348303\n",
      "Iteration 208, loss = 0.22318697\n",
      "Iteration 209, loss = 0.22285285\n",
      "Iteration 210, loss = 0.22312749\n",
      "Iteration 211, loss = 0.22236723\n",
      "Iteration 212, loss = 0.22167829\n",
      "Iteration 213, loss = 0.22160072\n",
      "Iteration 214, loss = 0.22178332\n",
      "Iteration 215, loss = 0.22097574\n",
      "Iteration 216, loss = 0.22079152\n",
      "Iteration 217, loss = 0.22037989\n",
      "Iteration 218, loss = 0.21998957\n",
      "Iteration 219, loss = 0.21986472\n",
      "Iteration 220, loss = 0.21972200\n",
      "Iteration 221, loss = 0.21928083\n",
      "Iteration 222, loss = 0.21887023\n",
      "Iteration 223, loss = 0.21859664\n",
      "Iteration 224, loss = 0.21823229\n",
      "Iteration 225, loss = 0.21850340\n",
      "Iteration 226, loss = 0.21840952\n",
      "Iteration 227, loss = 0.21761030\n",
      "Iteration 228, loss = 0.21738796\n",
      "Iteration 229, loss = 0.21732646\n",
      "Iteration 230, loss = 0.21698468\n",
      "Iteration 231, loss = 0.21634210\n",
      "Iteration 232, loss = 0.21643164\n",
      "Iteration 233, loss = 0.21603425\n",
      "Iteration 234, loss = 0.21555095\n",
      "Iteration 235, loss = 0.21544846\n",
      "Iteration 236, loss = 0.21520081\n",
      "Iteration 237, loss = 0.21488004\n",
      "Iteration 238, loss = 0.21449684\n",
      "Iteration 239, loss = 0.21484257\n",
      "Iteration 240, loss = 0.21418515\n",
      "Iteration 241, loss = 0.21444390\n",
      "Iteration 242, loss = 0.21358889\n",
      "Iteration 243, loss = 0.21350641\n",
      "Iteration 244, loss = 0.21472182\n",
      "Iteration 245, loss = 0.21328793\n",
      "Iteration 246, loss = 0.21267922\n",
      "Iteration 247, loss = 0.21227616\n",
      "Iteration 248, loss = 0.21224712\n",
      "Iteration 249, loss = 0.21187776\n",
      "Iteration 250, loss = 0.21138847\n",
      "Iteration 251, loss = 0.21149725\n",
      "Iteration 252, loss = 0.21138663\n",
      "Iteration 253, loss = 0.21096382\n",
      "Iteration 254, loss = 0.21071637\n",
      "Iteration 255, loss = 0.21072285\n",
      "Iteration 256, loss = 0.21073714\n",
      "Iteration 257, loss = 0.21013208\n",
      "Iteration 258, loss = 0.21007229\n",
      "Iteration 259, loss = 0.20975694\n",
      "Iteration 260, loss = 0.20954782\n",
      "Iteration 261, loss = 0.20926041\n",
      "Iteration 262, loss = 0.21009389\n",
      "Iteration 263, loss = 0.20897512\n",
      "Iteration 264, loss = 0.20875008\n",
      "Iteration 265, loss = 0.20869063\n",
      "Iteration 266, loss = 0.20876821\n",
      "Iteration 267, loss = 0.20851435\n",
      "Iteration 268, loss = 0.20751769\n",
      "Iteration 269, loss = 0.20721699\n",
      "Iteration 270, loss = 0.20728527\n",
      "Iteration 271, loss = 0.20762022\n",
      "Iteration 272, loss = 0.20699083\n",
      "Iteration 273, loss = 0.20688104\n",
      "Iteration 274, loss = 0.20632022\n",
      "Iteration 275, loss = 0.20632481\n",
      "Iteration 276, loss = 0.20647980\n",
      "Iteration 277, loss = 0.20581857\n",
      "Iteration 278, loss = 0.20569307\n",
      "Iteration 279, loss = 0.20548227\n",
      "Iteration 280, loss = 0.20587085\n",
      "Iteration 281, loss = 0.20531372\n",
      "Iteration 282, loss = 0.20525149\n",
      "Iteration 283, loss = 0.20479592\n",
      "Iteration 284, loss = 0.20511284\n",
      "Iteration 285, loss = 0.20432910\n",
      "Iteration 286, loss = 0.20476530\n",
      "Iteration 287, loss = 0.20485760\n",
      "Iteration 288, loss = 0.20400188\n",
      "Iteration 289, loss = 0.20400923\n",
      "Iteration 290, loss = 0.20389419\n",
      "Iteration 291, loss = 0.20369691\n",
      "Iteration 292, loss = 0.20329900\n",
      "Iteration 293, loss = 0.20317333\n",
      "Iteration 294, loss = 0.20338248\n",
      "Iteration 295, loss = 0.20326097\n",
      "Iteration 296, loss = 0.20327221\n",
      "Iteration 297, loss = 0.20306045\n",
      "Iteration 298, loss = 0.20301710\n",
      "Iteration 299, loss = 0.20245881\n",
      "Iteration 300, loss = 0.20247831\n",
      "Iteration 301, loss = 0.20246182\n",
      "Iteration 302, loss = 0.20261437\n",
      "Iteration 303, loss = 0.20197843\n",
      "Iteration 304, loss = 0.20183626\n",
      "Iteration 305, loss = 0.20172914\n",
      "Iteration 306, loss = 0.20193374\n",
      "Iteration 307, loss = 0.20190598\n",
      "Iteration 308, loss = 0.20200183\n",
      "Iteration 309, loss = 0.20125128\n",
      "Iteration 310, loss = 0.20093439\n",
      "Iteration 311, loss = 0.20095047\n",
      "Iteration 312, loss = 0.20059118\n",
      "Iteration 313, loss = 0.20034677\n",
      "Iteration 314, loss = 0.20029399\n",
      "Iteration 315, loss = 0.20031693\n",
      "Iteration 316, loss = 0.20088139\n",
      "Iteration 317, loss = 0.20075100\n",
      "Iteration 318, loss = 0.20053494\n",
      "Iteration 319, loss = 0.19984806\n",
      "Iteration 320, loss = 0.19988559\n",
      "Iteration 321, loss = 0.19978807\n",
      "Iteration 322, loss = 0.19905823\n",
      "Iteration 323, loss = 0.19889830\n",
      "Iteration 324, loss = 0.19933402\n",
      "Iteration 325, loss = 0.19935224\n",
      "Iteration 326, loss = 0.19874729\n",
      "Iteration 327, loss = 0.19870339\n",
      "Iteration 328, loss = 0.19874789\n",
      "Iteration 329, loss = 0.19832832\n",
      "Iteration 330, loss = 0.19914199\n",
      "Iteration 331, loss = 0.19804361\n",
      "Iteration 332, loss = 0.19833628\n",
      "Iteration 333, loss = 0.19826054\n",
      "Iteration 334, loss = 0.19786451\n",
      "Iteration 335, loss = 0.19908615\n",
      "Iteration 336, loss = 0.19792729\n",
      "Iteration 337, loss = 0.19758937\n",
      "Iteration 338, loss = 0.19758843\n",
      "Iteration 339, loss = 0.19707433\n",
      "Iteration 340, loss = 0.19723789\n",
      "Iteration 341, loss = 0.19683151\n",
      "Iteration 342, loss = 0.19721208\n",
      "Iteration 343, loss = 0.19670676\n",
      "Iteration 344, loss = 0.19688291\n",
      "Iteration 345, loss = 0.19666987\n",
      "Iteration 346, loss = 0.19616723\n",
      "Iteration 347, loss = 0.19623597\n",
      "Iteration 348, loss = 0.19612902\n",
      "Iteration 349, loss = 0.19632033\n",
      "Iteration 350, loss = 0.19603704\n",
      "Iteration 351, loss = 0.19654042\n",
      "Iteration 352, loss = 0.19581591\n",
      "Iteration 353, loss = 0.19541005\n",
      "Iteration 354, loss = 0.19525190\n",
      "Iteration 355, loss = 0.19605563\n",
      "Iteration 356, loss = 0.19580896\n",
      "Iteration 357, loss = 0.19524742\n",
      "Iteration 358, loss = 0.19498517\n",
      "Iteration 359, loss = 0.19539990\n",
      "Iteration 360, loss = 0.19463767\n",
      "Iteration 361, loss = 0.19480523\n",
      "Iteration 362, loss = 0.19442782\n",
      "Iteration 363, loss = 0.19396144\n",
      "Iteration 364, loss = 0.19440810\n",
      "Iteration 365, loss = 0.19416993\n",
      "Iteration 366, loss = 0.19447587\n",
      "Iteration 367, loss = 0.19412660\n",
      "Iteration 368, loss = 0.19357380\n",
      "Iteration 369, loss = 0.19424379\n",
      "Iteration 370, loss = 0.19329693\n",
      "Iteration 371, loss = 0.19319297\n",
      "Iteration 372, loss = 0.19315681\n",
      "Iteration 373, loss = 0.19299157\n",
      "Iteration 374, loss = 0.19265211\n",
      "Iteration 375, loss = 0.19273120\n",
      "Iteration 376, loss = 0.19234901\n",
      "Iteration 377, loss = 0.19351540\n",
      "Iteration 378, loss = 0.19340499\n",
      "Iteration 379, loss = 0.19191108\n",
      "Iteration 380, loss = 0.19209005\n",
      "Iteration 381, loss = 0.19191776\n",
      "Iteration 382, loss = 0.19211328\n",
      "Iteration 383, loss = 0.19193500\n",
      "Iteration 384, loss = 0.19120851\n",
      "Iteration 385, loss = 0.19149681\n",
      "Iteration 386, loss = 0.19107896\n",
      "Iteration 387, loss = 0.19133004\n",
      "Iteration 388, loss = 0.19162788\n",
      "Iteration 389, loss = 0.19087563\n",
      "Iteration 390, loss = 0.19151007\n",
      "Iteration 391, loss = 0.19163409\n",
      "Iteration 392, loss = 0.19075129\n",
      "Iteration 393, loss = 0.19065535\n",
      "Iteration 394, loss = 0.19036328\n",
      "Iteration 395, loss = 0.19026360\n",
      "Iteration 396, loss = 0.19015621\n",
      "Iteration 397, loss = 0.19020933\n",
      "Iteration 398, loss = 0.19003435\n",
      "Iteration 399, loss = 0.19051270\n",
      "Iteration 400, loss = 0.18981082\n",
      "Iteration 401, loss = 0.18946401\n",
      "Iteration 402, loss = 0.18970075\n",
      "Iteration 403, loss = 0.19012186\n",
      "Iteration 404, loss = 0.18987345\n",
      "Iteration 405, loss = 0.18966258\n",
      "Iteration 406, loss = 0.18941154\n",
      "Iteration 407, loss = 0.18954220\n",
      "Iteration 408, loss = 0.18974281\n",
      "Iteration 409, loss = 0.18909752\n",
      "Iteration 410, loss = 0.18944122\n",
      "Iteration 411, loss = 0.18892389\n",
      "Iteration 412, loss = 0.18908748\n",
      "Iteration 413, loss = 0.18869696\n",
      "Iteration 414, loss = 0.18868013\n",
      "Iteration 415, loss = 0.18911492\n",
      "Iteration 416, loss = 0.18919667\n",
      "Iteration 417, loss = 0.18902693\n",
      "Iteration 418, loss = 0.18872243\n",
      "Iteration 419, loss = 0.18915201\n",
      "Iteration 420, loss = 0.18824396\n",
      "Iteration 421, loss = 0.18818063\n",
      "Iteration 422, loss = 0.18844009\n",
      "Iteration 423, loss = 0.18788096\n",
      "Iteration 424, loss = 0.18808465\n",
      "Iteration 425, loss = 0.18838746\n",
      "Iteration 426, loss = 0.18840307\n",
      "Iteration 427, loss = 0.18817237\n",
      "Iteration 428, loss = 0.18772352\n",
      "Iteration 429, loss = 0.18778028\n",
      "Iteration 430, loss = 0.18791006\n",
      "Iteration 431, loss = 0.18757150\n",
      "Iteration 432, loss = 0.18740998\n",
      "Iteration 433, loss = 0.18787624\n",
      "Iteration 434, loss = 0.18789951\n",
      "Iteration 435, loss = 0.18793059\n",
      "Iteration 436, loss = 0.18733104\n",
      "Iteration 437, loss = 0.18757946\n",
      "Iteration 438, loss = 0.18700732\n",
      "Iteration 439, loss = 0.18721465\n",
      "Iteration 440, loss = 0.18744565\n",
      "Iteration 441, loss = 0.18713830\n",
      "Iteration 442, loss = 0.18731553\n",
      "Iteration 443, loss = 0.18663346\n",
      "Iteration 444, loss = 0.18668107\n",
      "Iteration 445, loss = 0.18689009\n",
      "Iteration 446, loss = 0.18746652\n",
      "Iteration 447, loss = 0.18648810\n",
      "Iteration 448, loss = 0.18654574\n",
      "Iteration 449, loss = 0.18679066\n",
      "Iteration 450, loss = 0.18691395\n",
      "Iteration 451, loss = 0.18673093\n",
      "Iteration 452, loss = 0.18636473\n",
      "Iteration 453, loss = 0.18650205\n",
      "Iteration 454, loss = 0.18696637\n",
      "Iteration 455, loss = 0.18608330\n",
      "Iteration 456, loss = 0.18691692\n",
      "Iteration 457, loss = 0.18661131\n",
      "Iteration 458, loss = 0.18620746\n",
      "Iteration 459, loss = 0.18652514\n",
      "Iteration 460, loss = 0.18628086\n",
      "Iteration 461, loss = 0.18624940\n",
      "Iteration 462, loss = 0.18657475\n",
      "Iteration 463, loss = 0.18614668\n",
      "Iteration 464, loss = 0.18622686\n",
      "Iteration 465, loss = 0.18564008\n",
      "Iteration 466, loss = 0.18569400\n",
      "Iteration 467, loss = 0.18595163\n",
      "Iteration 468, loss = 0.18551920\n",
      "Iteration 469, loss = 0.18572594\n",
      "Iteration 470, loss = 0.18548796\n",
      "Iteration 471, loss = 0.18575702\n",
      "Iteration 472, loss = 0.18563135\n",
      "Iteration 473, loss = 0.18544359\n",
      "Iteration 474, loss = 0.18551759\n",
      "Iteration 475, loss = 0.18547934\n",
      "Iteration 476, loss = 0.18532499\n",
      "Iteration 477, loss = 0.18538691\n",
      "Iteration 478, loss = 0.18593098\n",
      "Iteration 479, loss = 0.18546247\n",
      "Iteration 480, loss = 0.18529661\n",
      "Iteration 481, loss = 0.18502460\n",
      "Iteration 482, loss = 0.18506290\n",
      "Iteration 483, loss = 0.18506495\n",
      "Iteration 484, loss = 0.18542416\n",
      "Iteration 485, loss = 0.18560027\n",
      "Iteration 486, loss = 0.18443511\n",
      "Iteration 487, loss = 0.18492135\n",
      "Iteration 488, loss = 0.18579499\n",
      "Iteration 489, loss = 0.18441454\n",
      "Iteration 490, loss = 0.18508293\n",
      "Iteration 491, loss = 0.18503652\n",
      "Iteration 492, loss = 0.18489652\n",
      "Iteration 493, loss = 0.18522954\n",
      "Iteration 494, loss = 0.18439811\n",
      "Iteration 495, loss = 0.18446740\n",
      "Iteration 496, loss = 0.18429466\n",
      "Iteration 497, loss = 0.18452972\n",
      "Iteration 498, loss = 0.18446229\n",
      "Iteration 499, loss = 0.18406170\n",
      "Iteration 500, loss = 0.18501766\n",
      "Iteration 501, loss = 0.18468059\n",
      "Iteration 502, loss = 0.18443859\n",
      "Iteration 503, loss = 0.18458747\n",
      "Iteration 504, loss = 0.18552624\n",
      "Iteration 505, loss = 0.18428022\n",
      "Iteration 506, loss = 0.18479410\n",
      "Iteration 507, loss = 0.18444303\n",
      "Iteration 508, loss = 0.18435131\n",
      "Iteration 509, loss = 0.18421777\n",
      "Iteration 510, loss = 0.18378602\n",
      "Iteration 511, loss = 0.18390071\n",
      "Iteration 512, loss = 0.18452170\n",
      "Iteration 513, loss = 0.18399371\n",
      "Iteration 514, loss = 0.18414241\n",
      "Iteration 515, loss = 0.18394242\n",
      "Iteration 516, loss = 0.18405416\n",
      "Iteration 517, loss = 0.18386042\n",
      "Iteration 518, loss = 0.18382667\n",
      "Iteration 519, loss = 0.18369676\n",
      "Iteration 520, loss = 0.18354746\n",
      "Iteration 521, loss = 0.18385978\n",
      "Iteration 522, loss = 0.18419836\n",
      "Iteration 523, loss = 0.18369616\n",
      "Iteration 524, loss = 0.18401704\n",
      "Iteration 525, loss = 0.18372118\n",
      "Iteration 526, loss = 0.18396376\n",
      "Iteration 527, loss = 0.18314387\n",
      "Iteration 528, loss = 0.18355352\n",
      "Iteration 529, loss = 0.18333611\n",
      "Iteration 530, loss = 0.18363437\n",
      "Iteration 531, loss = 0.18460657\n",
      "Iteration 532, loss = 0.18363409\n",
      "Iteration 533, loss = 0.18322018\n",
      "Iteration 534, loss = 0.18343777\n",
      "Iteration 535, loss = 0.18316565\n",
      "Iteration 536, loss = 0.18340782\n",
      "Iteration 537, loss = 0.18335307\n",
      "Iteration 538, loss = 0.18350151\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76997242\n",
      "Iteration 2, loss = 0.74438034\n",
      "Iteration 3, loss = 0.72270976\n",
      "Iteration 4, loss = 0.70559033\n",
      "Iteration 5, loss = 0.69096282\n",
      "Iteration 6, loss = 0.67857638\n",
      "Iteration 7, loss = 0.66628714\n",
      "Iteration 8, loss = 0.65237847\n",
      "Iteration 9, loss = 0.63602749\n",
      "Iteration 10, loss = 0.61805492\n",
      "Iteration 11, loss = 0.59898424\n",
      "Iteration 12, loss = 0.57980336\n",
      "Iteration 13, loss = 0.56390177\n",
      "Iteration 14, loss = 0.54895953\n",
      "Iteration 15, loss = 0.53652482\n",
      "Iteration 16, loss = 0.52637441\n",
      "Iteration 17, loss = 0.51628627\n",
      "Iteration 18, loss = 0.50775503\n",
      "Iteration 19, loss = 0.49975397\n",
      "Iteration 20, loss = 0.49274479\n",
      "Iteration 21, loss = 0.48615982\n",
      "Iteration 22, loss = 0.48002673\n",
      "Iteration 23, loss = 0.47476646\n",
      "Iteration 24, loss = 0.46934657\n",
      "Iteration 25, loss = 0.46456551\n",
      "Iteration 26, loss = 0.45967468\n",
      "Iteration 27, loss = 0.45524988\n",
      "Iteration 28, loss = 0.45077979\n",
      "Iteration 29, loss = 0.44677559\n",
      "Iteration 30, loss = 0.44270577\n",
      "Iteration 31, loss = 0.43876592\n",
      "Iteration 32, loss = 0.43499227\n",
      "Iteration 33, loss = 0.43127817\n",
      "Iteration 34, loss = 0.42782417\n",
      "Iteration 35, loss = 0.42422377\n",
      "Iteration 36, loss = 0.42082606\n",
      "Iteration 37, loss = 0.41757272\n",
      "Iteration 38, loss = 0.41397414\n",
      "Iteration 39, loss = 0.41067652\n",
      "Iteration 40, loss = 0.40772757\n",
      "Iteration 41, loss = 0.40448172\n",
      "Iteration 42, loss = 0.40170985\n",
      "Iteration 43, loss = 0.39854690\n",
      "Iteration 44, loss = 0.39572442\n",
      "Iteration 45, loss = 0.39273112\n",
      "Iteration 46, loss = 0.38995451\n",
      "Iteration 47, loss = 0.38741717\n",
      "Iteration 48, loss = 0.38474822\n",
      "Iteration 49, loss = 0.38239453\n",
      "Iteration 50, loss = 0.37987110\n",
      "Iteration 51, loss = 0.37741005\n",
      "Iteration 52, loss = 0.37515758\n",
      "Iteration 53, loss = 0.37255868\n",
      "Iteration 54, loss = 0.37040733\n",
      "Iteration 55, loss = 0.36810692\n",
      "Iteration 56, loss = 0.36602156\n",
      "Iteration 57, loss = 0.36388402\n",
      "Iteration 58, loss = 0.36178445\n",
      "Iteration 59, loss = 0.35983317\n",
      "Iteration 60, loss = 0.35775308\n",
      "Iteration 61, loss = 0.35599890\n",
      "Iteration 62, loss = 0.35375843\n",
      "Iteration 63, loss = 0.35241642\n",
      "Iteration 64, loss = 0.35039227\n",
      "Iteration 65, loss = 0.34836008\n",
      "Iteration 66, loss = 0.34650888\n",
      "Iteration 67, loss = 0.34476163\n",
      "Iteration 68, loss = 0.34296620\n",
      "Iteration 69, loss = 0.34113176\n",
      "Iteration 70, loss = 0.33925450\n",
      "Iteration 71, loss = 0.33724503\n",
      "Iteration 72, loss = 0.33563630\n",
      "Iteration 73, loss = 0.33381696\n",
      "Iteration 74, loss = 0.33208967\n",
      "Iteration 75, loss = 0.33055035\n",
      "Iteration 76, loss = 0.32921252\n",
      "Iteration 77, loss = 0.32736274\n",
      "Iteration 78, loss = 0.32581428\n",
      "Iteration 79, loss = 0.32471478\n",
      "Iteration 80, loss = 0.32237210\n",
      "Iteration 81, loss = 0.32094092\n",
      "Iteration 82, loss = 0.31933443\n",
      "Iteration 83, loss = 0.31790669\n",
      "Iteration 84, loss = 0.31631417\n",
      "Iteration 85, loss = 0.31479086\n",
      "Iteration 86, loss = 0.31354821\n",
      "Iteration 87, loss = 0.31198975\n",
      "Iteration 88, loss = 0.31061442\n",
      "Iteration 89, loss = 0.30889692\n",
      "Iteration 90, loss = 0.30780517\n",
      "Iteration 91, loss = 0.30682502\n",
      "Iteration 92, loss = 0.30602364\n",
      "Iteration 93, loss = 0.30359273\n",
      "Iteration 94, loss = 0.30280681\n",
      "Iteration 95, loss = 0.30113495\n",
      "Iteration 96, loss = 0.30004311\n",
      "Iteration 97, loss = 0.29890440\n",
      "Iteration 98, loss = 0.29786968\n",
      "Iteration 99, loss = 0.29662517\n",
      "Iteration 100, loss = 0.29481966\n",
      "Iteration 101, loss = 0.29417893\n",
      "Iteration 102, loss = 0.29266171\n",
      "Iteration 103, loss = 0.29157034\n",
      "Iteration 104, loss = 0.29041808\n",
      "Iteration 105, loss = 0.28942259\n",
      "Iteration 106, loss = 0.28810016\n",
      "Iteration 107, loss = 0.28702239\n",
      "Iteration 108, loss = 0.28638416\n",
      "Iteration 109, loss = 0.28512970\n",
      "Iteration 110, loss = 0.28405467\n",
      "Iteration 111, loss = 0.28281234\n",
      "Iteration 112, loss = 0.28169447\n",
      "Iteration 113, loss = 0.28071750\n",
      "Iteration 114, loss = 0.28003195\n",
      "Iteration 115, loss = 0.27852193\n",
      "Iteration 116, loss = 0.27837992\n",
      "Iteration 117, loss = 0.27685147\n",
      "Iteration 118, loss = 0.27602135\n",
      "Iteration 119, loss = 0.27523242\n",
      "Iteration 120, loss = 0.27412096\n",
      "Iteration 121, loss = 0.27327461\n",
      "Iteration 122, loss = 0.27251508\n",
      "Iteration 123, loss = 0.27155435\n",
      "Iteration 124, loss = 0.27092800\n",
      "Iteration 125, loss = 0.26964452\n",
      "Iteration 126, loss = 0.26879931\n",
      "Iteration 127, loss = 0.26813571\n",
      "Iteration 128, loss = 0.26720994\n",
      "Iteration 129, loss = 0.26647997\n",
      "Iteration 130, loss = 0.26580048\n",
      "Iteration 131, loss = 0.26527820\n",
      "Iteration 132, loss = 0.26427160\n",
      "Iteration 133, loss = 0.26361622\n",
      "Iteration 134, loss = 0.26290681\n",
      "Iteration 135, loss = 0.26247832\n",
      "Iteration 136, loss = 0.26239705\n",
      "Iteration 137, loss = 0.26072559\n",
      "Iteration 138, loss = 0.26081884\n",
      "Iteration 139, loss = 0.25967674\n",
      "Iteration 140, loss = 0.25917786\n",
      "Iteration 141, loss = 0.25805215\n",
      "Iteration 142, loss = 0.25753625\n",
      "Iteration 143, loss = 0.25680853\n",
      "Iteration 144, loss = 0.25635202\n",
      "Iteration 145, loss = 0.25576818\n",
      "Iteration 146, loss = 0.25550511\n",
      "Iteration 147, loss = 0.25475324\n",
      "Iteration 148, loss = 0.25382166\n",
      "Iteration 149, loss = 0.25350056\n",
      "Iteration 150, loss = 0.25274109\n",
      "Iteration 151, loss = 0.25224113\n",
      "Iteration 152, loss = 0.25160481\n",
      "Iteration 153, loss = 0.25104811\n",
      "Iteration 154, loss = 0.25074606\n",
      "Iteration 155, loss = 0.25008643\n",
      "Iteration 156, loss = 0.24975935\n",
      "Iteration 157, loss = 0.24924201\n",
      "Iteration 158, loss = 0.24850528\n",
      "Iteration 159, loss = 0.24791888\n",
      "Iteration 160, loss = 0.24734205\n",
      "Iteration 161, loss = 0.24687635\n",
      "Iteration 162, loss = 0.24691922\n",
      "Iteration 163, loss = 0.24599370\n",
      "Iteration 164, loss = 0.24552577\n",
      "Iteration 165, loss = 0.24533440\n",
      "Iteration 166, loss = 0.24496520\n",
      "Iteration 167, loss = 0.24474199\n",
      "Iteration 168, loss = 0.24422201\n",
      "Iteration 169, loss = 0.24372213\n",
      "Iteration 170, loss = 0.24307420\n",
      "Iteration 171, loss = 0.24257118\n",
      "Iteration 172, loss = 0.24198412\n",
      "Iteration 173, loss = 0.24185381\n",
      "Iteration 174, loss = 0.24107739\n",
      "Iteration 175, loss = 0.24086850\n",
      "Iteration 176, loss = 0.24073072\n",
      "Iteration 177, loss = 0.24043645\n",
      "Iteration 178, loss = 0.23969579\n",
      "Iteration 179, loss = 0.23942963\n",
      "Iteration 180, loss = 0.23903666\n",
      "Iteration 181, loss = 0.23884584\n",
      "Iteration 182, loss = 0.23797407\n",
      "Iteration 183, loss = 0.23770352\n",
      "Iteration 184, loss = 0.23709531\n",
      "Iteration 185, loss = 0.23661465\n",
      "Iteration 186, loss = 0.23652161\n",
      "Iteration 187, loss = 0.23590407\n",
      "Iteration 188, loss = 0.23566798\n",
      "Iteration 189, loss = 0.23521042\n",
      "Iteration 190, loss = 0.23484992\n",
      "Iteration 191, loss = 0.23440661\n",
      "Iteration 192, loss = 0.23440138\n",
      "Iteration 193, loss = 0.23385750\n",
      "Iteration 194, loss = 0.23320150\n",
      "Iteration 195, loss = 0.23323095\n",
      "Iteration 196, loss = 0.23273544\n",
      "Iteration 197, loss = 0.23215151\n",
      "Iteration 198, loss = 0.23174678\n",
      "Iteration 199, loss = 0.23197956\n",
      "Iteration 200, loss = 0.23098102\n",
      "Iteration 201, loss = 0.23083438\n",
      "Iteration 202, loss = 0.23051629\n",
      "Iteration 203, loss = 0.23057221\n",
      "Iteration 204, loss = 0.23003419\n",
      "Iteration 205, loss = 0.22954971\n",
      "Iteration 206, loss = 0.22923646\n",
      "Iteration 207, loss = 0.22894625\n",
      "Iteration 208, loss = 0.22906377\n",
      "Iteration 209, loss = 0.22828032\n",
      "Iteration 210, loss = 0.22783928\n",
      "Iteration 211, loss = 0.22767404\n",
      "Iteration 212, loss = 0.22713202\n",
      "Iteration 213, loss = 0.22666900\n",
      "Iteration 214, loss = 0.22687637\n",
      "Iteration 215, loss = 0.22716433\n",
      "Iteration 216, loss = 0.22629703\n",
      "Iteration 217, loss = 0.22657112\n",
      "Iteration 218, loss = 0.22527695\n",
      "Iteration 219, loss = 0.22566004\n",
      "Iteration 220, loss = 0.22489559\n",
      "Iteration 221, loss = 0.22443735\n",
      "Iteration 222, loss = 0.22459341\n",
      "Iteration 223, loss = 0.22387544\n",
      "Iteration 224, loss = 0.22389825\n",
      "Iteration 225, loss = 0.22313263\n",
      "Iteration 226, loss = 0.22334190\n",
      "Iteration 227, loss = 0.22306735\n",
      "Iteration 228, loss = 0.22245623\n",
      "Iteration 229, loss = 0.22252706\n",
      "Iteration 230, loss = 0.22312063\n",
      "Iteration 231, loss = 0.22214639\n",
      "Iteration 232, loss = 0.22129815\n",
      "Iteration 233, loss = 0.22158333\n",
      "Iteration 234, loss = 0.22065216\n",
      "Iteration 235, loss = 0.22060064\n",
      "Iteration 236, loss = 0.22015154\n",
      "Iteration 237, loss = 0.22017044\n",
      "Iteration 238, loss = 0.21966641\n",
      "Iteration 239, loss = 0.22140153\n",
      "Iteration 240, loss = 0.21930084\n",
      "Iteration 241, loss = 0.21938476\n",
      "Iteration 242, loss = 0.21851825\n",
      "Iteration 243, loss = 0.21854680\n",
      "Iteration 244, loss = 0.21839818\n",
      "Iteration 245, loss = 0.21780111\n",
      "Iteration 246, loss = 0.21778059\n",
      "Iteration 247, loss = 0.21768260\n",
      "Iteration 248, loss = 0.21742459\n",
      "Iteration 249, loss = 0.21712306\n",
      "Iteration 250, loss = 0.21690895\n",
      "Iteration 251, loss = 0.21669005\n",
      "Iteration 252, loss = 0.21627838\n",
      "Iteration 253, loss = 0.21661279\n",
      "Iteration 254, loss = 0.21624507\n",
      "Iteration 255, loss = 0.21617710\n",
      "Iteration 256, loss = 0.21558681\n",
      "Iteration 257, loss = 0.21556855\n",
      "Iteration 258, loss = 0.21519520\n",
      "Iteration 259, loss = 0.21491679\n",
      "Iteration 260, loss = 0.21505921\n",
      "Iteration 261, loss = 0.21478890\n",
      "Iteration 262, loss = 0.21431293\n",
      "Iteration 263, loss = 0.21436947\n",
      "Iteration 264, loss = 0.21415997\n",
      "Iteration 265, loss = 0.21355612\n",
      "Iteration 266, loss = 0.21372564\n",
      "Iteration 267, loss = 0.21367817\n",
      "Iteration 268, loss = 0.21336693\n",
      "Iteration 269, loss = 0.21309912\n",
      "Iteration 270, loss = 0.21279989\n",
      "Iteration 271, loss = 0.21281192\n",
      "Iteration 272, loss = 0.21268544\n",
      "Iteration 273, loss = 0.21236110\n",
      "Iteration 274, loss = 0.21224591\n",
      "Iteration 275, loss = 0.21206910\n",
      "Iteration 276, loss = 0.21173347\n",
      "Iteration 277, loss = 0.21170431\n",
      "Iteration 278, loss = 0.21122029\n",
      "Iteration 279, loss = 0.21098839\n",
      "Iteration 280, loss = 0.21114074\n",
      "Iteration 281, loss = 0.21070354\n",
      "Iteration 282, loss = 0.21034467\n",
      "Iteration 283, loss = 0.21030804\n",
      "Iteration 284, loss = 0.21021028\n",
      "Iteration 285, loss = 0.21019922\n",
      "Iteration 286, loss = 0.20960717\n",
      "Iteration 287, loss = 0.20940790\n",
      "Iteration 288, loss = 0.20921357\n",
      "Iteration 289, loss = 0.20908773\n",
      "Iteration 290, loss = 0.20905858\n",
      "Iteration 291, loss = 0.20853337\n",
      "Iteration 292, loss = 0.20883103\n",
      "Iteration 293, loss = 0.20836439\n",
      "Iteration 294, loss = 0.20805155\n",
      "Iteration 295, loss = 0.20851838\n",
      "Iteration 296, loss = 0.20817061\n",
      "Iteration 297, loss = 0.20787599\n",
      "Iteration 298, loss = 0.20836967\n",
      "Iteration 299, loss = 0.20787173\n",
      "Iteration 300, loss = 0.20768218\n",
      "Iteration 301, loss = 0.20691187\n",
      "Iteration 302, loss = 0.20735774\n",
      "Iteration 303, loss = 0.20663598\n",
      "Iteration 304, loss = 0.20721337\n",
      "Iteration 305, loss = 0.20623903\n",
      "Iteration 306, loss = 0.20621238\n",
      "Iteration 307, loss = 0.20644657\n",
      "Iteration 308, loss = 0.20659742\n",
      "Iteration 309, loss = 0.20603107\n",
      "Iteration 310, loss = 0.20573922\n",
      "Iteration 311, loss = 0.20547821\n",
      "Iteration 312, loss = 0.20541104\n",
      "Iteration 313, loss = 0.20543595\n",
      "Iteration 314, loss = 0.20530194\n",
      "Iteration 315, loss = 0.20501194\n",
      "Iteration 316, loss = 0.20562482\n",
      "Iteration 317, loss = 0.20479557\n",
      "Iteration 318, loss = 0.20448691\n",
      "Iteration 319, loss = 0.20465233\n",
      "Iteration 320, loss = 0.20423826\n",
      "Iteration 321, loss = 0.20402145\n",
      "Iteration 322, loss = 0.20433678\n",
      "Iteration 323, loss = 0.20428726\n",
      "Iteration 324, loss = 0.20382744\n",
      "Iteration 325, loss = 0.20374042\n",
      "Iteration 326, loss = 0.20411997\n",
      "Iteration 327, loss = 0.20368190\n",
      "Iteration 328, loss = 0.20330750\n",
      "Iteration 329, loss = 0.20318865\n",
      "Iteration 330, loss = 0.20277280\n",
      "Iteration 331, loss = 0.20248803\n",
      "Iteration 332, loss = 0.20270683\n",
      "Iteration 333, loss = 0.20245786\n",
      "Iteration 334, loss = 0.20237610\n",
      "Iteration 335, loss = 0.20245206\n",
      "Iteration 336, loss = 0.20207833\n",
      "Iteration 337, loss = 0.20212189\n",
      "Iteration 338, loss = 0.20174853\n",
      "Iteration 339, loss = 0.20155407\n",
      "Iteration 340, loss = 0.20152457\n",
      "Iteration 341, loss = 0.20107369\n",
      "Iteration 342, loss = 0.20122593\n",
      "Iteration 343, loss = 0.20079310\n",
      "Iteration 344, loss = 0.20121513\n",
      "Iteration 345, loss = 0.20082148\n",
      "Iteration 346, loss = 0.20043793\n",
      "Iteration 347, loss = 0.20018486\n",
      "Iteration 348, loss = 0.19983649\n",
      "Iteration 349, loss = 0.20009832\n",
      "Iteration 350, loss = 0.19987347\n",
      "Iteration 351, loss = 0.19932943\n",
      "Iteration 352, loss = 0.19984020\n",
      "Iteration 353, loss = 0.19911001\n",
      "Iteration 354, loss = 0.19917323\n",
      "Iteration 355, loss = 0.19933842\n",
      "Iteration 356, loss = 0.19907227\n",
      "Iteration 357, loss = 0.19901840\n",
      "Iteration 358, loss = 0.19882799\n",
      "Iteration 359, loss = 0.19835323\n",
      "Iteration 360, loss = 0.19845980\n",
      "Iteration 361, loss = 0.19844820\n",
      "Iteration 362, loss = 0.19823674\n",
      "Iteration 363, loss = 0.19793620\n",
      "Iteration 364, loss = 0.19784280\n",
      "Iteration 365, loss = 0.19765788\n",
      "Iteration 366, loss = 0.19750165\n",
      "Iteration 367, loss = 0.19739249\n",
      "Iteration 368, loss = 0.19750790\n",
      "Iteration 369, loss = 0.19742713\n",
      "Iteration 370, loss = 0.19710961\n",
      "Iteration 371, loss = 0.19690509\n",
      "Iteration 372, loss = 0.19700660\n",
      "Iteration 373, loss = 0.19717072\n",
      "Iteration 374, loss = 0.19657659\n",
      "Iteration 375, loss = 0.19674693\n",
      "Iteration 376, loss = 0.19645892\n",
      "Iteration 377, loss = 0.19661558\n",
      "Iteration 378, loss = 0.19601369\n",
      "Iteration 379, loss = 0.19607964\n",
      "Iteration 380, loss = 0.19569787\n",
      "Iteration 381, loss = 0.19582571\n",
      "Iteration 382, loss = 0.19562735\n",
      "Iteration 383, loss = 0.19578863\n",
      "Iteration 384, loss = 0.19627166\n",
      "Iteration 385, loss = 0.19554972\n",
      "Iteration 386, loss = 0.19519364\n",
      "Iteration 387, loss = 0.19535744\n",
      "Iteration 388, loss = 0.19528565\n",
      "Iteration 389, loss = 0.19504194\n",
      "Iteration 390, loss = 0.19461568\n",
      "Iteration 391, loss = 0.19469887\n",
      "Iteration 392, loss = 0.19485811\n",
      "Iteration 393, loss = 0.19470570\n",
      "Iteration 394, loss = 0.19440550\n",
      "Iteration 395, loss = 0.19421599\n",
      "Iteration 396, loss = 0.19415003\n",
      "Iteration 397, loss = 0.19373822\n",
      "Iteration 398, loss = 0.19409027\n",
      "Iteration 399, loss = 0.19374453\n",
      "Iteration 400, loss = 0.19378346\n",
      "Iteration 401, loss = 0.19388630\n",
      "Iteration 402, loss = 0.19331158\n",
      "Iteration 403, loss = 0.19342269\n",
      "Iteration 404, loss = 0.19395958\n",
      "Iteration 405, loss = 0.19314359\n",
      "Iteration 406, loss = 0.19282124\n",
      "Iteration 407, loss = 0.19283675\n",
      "Iteration 408, loss = 0.19258903\n",
      "Iteration 409, loss = 0.19232325\n",
      "Iteration 410, loss = 0.19226250\n",
      "Iteration 411, loss = 0.19228566\n",
      "Iteration 412, loss = 0.19202718\n",
      "Iteration 413, loss = 0.19174835\n",
      "Iteration 414, loss = 0.19195262\n",
      "Iteration 415, loss = 0.19158226\n",
      "Iteration 416, loss = 0.19131728\n",
      "Iteration 417, loss = 0.19150316\n",
      "Iteration 418, loss = 0.19103868\n",
      "Iteration 419, loss = 0.19126526\n",
      "Iteration 420, loss = 0.19088688\n",
      "Iteration 421, loss = 0.19087306\n",
      "Iteration 422, loss = 0.19064228\n",
      "Iteration 423, loss = 0.19078299\n",
      "Iteration 424, loss = 0.19048530\n",
      "Iteration 425, loss = 0.19084825\n",
      "Iteration 426, loss = 0.19117421\n",
      "Iteration 427, loss = 0.19085715\n",
      "Iteration 428, loss = 0.19007078\n",
      "Iteration 429, loss = 0.19013867\n",
      "Iteration 430, loss = 0.18977255\n",
      "Iteration 431, loss = 0.19018139\n",
      "Iteration 432, loss = 0.18975459\n",
      "Iteration 433, loss = 0.19025965\n",
      "Iteration 434, loss = 0.18935117\n",
      "Iteration 435, loss = 0.18951555\n",
      "Iteration 436, loss = 0.18954522\n",
      "Iteration 437, loss = 0.18908001\n",
      "Iteration 438, loss = 0.18924096\n",
      "Iteration 439, loss = 0.18916923\n",
      "Iteration 440, loss = 0.18907762\n",
      "Iteration 441, loss = 0.18877477\n",
      "Iteration 442, loss = 0.18871574\n",
      "Iteration 443, loss = 0.18870327\n",
      "Iteration 444, loss = 0.18961469\n",
      "Iteration 445, loss = 0.18880106\n",
      "Iteration 446, loss = 0.19049261\n",
      "Iteration 447, loss = 0.18874848\n",
      "Iteration 448, loss = 0.18833452\n",
      "Iteration 449, loss = 0.19011648\n",
      "Iteration 450, loss = 0.18978606\n",
      "Iteration 451, loss = 0.18861935\n",
      "Iteration 452, loss = 0.18777246\n",
      "Iteration 453, loss = 0.18790548\n",
      "Iteration 454, loss = 0.18749647\n",
      "Iteration 455, loss = 0.18733222\n",
      "Iteration 456, loss = 0.18698072\n",
      "Iteration 457, loss = 0.18723529\n",
      "Iteration 458, loss = 0.18670330\n",
      "Iteration 459, loss = 0.18789129\n",
      "Iteration 460, loss = 0.18687709\n",
      "Iteration 461, loss = 0.18670009\n",
      "Iteration 462, loss = 0.18690696\n",
      "Iteration 463, loss = 0.18633858\n",
      "Iteration 464, loss = 0.18640914\n",
      "Iteration 465, loss = 0.18654205\n",
      "Iteration 466, loss = 0.18629986\n",
      "Iteration 467, loss = 0.18643180\n",
      "Iteration 468, loss = 0.18639444\n",
      "Iteration 469, loss = 0.18651587\n",
      "Iteration 470, loss = 0.18658221\n",
      "Iteration 471, loss = 0.18592469\n",
      "Iteration 472, loss = 0.18594510\n",
      "Iteration 473, loss = 0.18573972\n",
      "Iteration 474, loss = 0.18617048\n",
      "Iteration 475, loss = 0.18524252\n",
      "Iteration 476, loss = 0.18614927\n",
      "Iteration 477, loss = 0.18569679\n",
      "Iteration 478, loss = 0.18508223\n",
      "Iteration 479, loss = 0.18530263\n",
      "Iteration 480, loss = 0.18525692\n",
      "Iteration 481, loss = 0.18500972\n",
      "Iteration 482, loss = 0.18514257\n",
      "Iteration 483, loss = 0.18569999\n",
      "Iteration 484, loss = 0.18498762\n",
      "Iteration 485, loss = 0.18480792\n",
      "Iteration 486, loss = 0.18458010\n",
      "Iteration 487, loss = 0.18458223\n",
      "Iteration 488, loss = 0.18460162\n",
      "Iteration 489, loss = 0.18491813\n",
      "Iteration 490, loss = 0.18433816\n",
      "Iteration 491, loss = 0.18437763\n",
      "Iteration 492, loss = 0.18438558\n",
      "Iteration 493, loss = 0.18492668\n",
      "Iteration 494, loss = 0.18484848\n",
      "Iteration 495, loss = 0.18419146\n",
      "Iteration 496, loss = 0.18458410\n",
      "Iteration 497, loss = 0.18411696\n",
      "Iteration 498, loss = 0.18392355\n",
      "Iteration 499, loss = 0.18554323\n",
      "Iteration 500, loss = 0.18350456\n",
      "Iteration 501, loss = 0.18362745\n",
      "Iteration 502, loss = 0.18373809\n",
      "Iteration 503, loss = 0.18357931\n",
      "Iteration 504, loss = 0.18353863\n",
      "Iteration 505, loss = 0.18349789\n",
      "Iteration 506, loss = 0.18352503\n",
      "Iteration 507, loss = 0.18336892\n",
      "Iteration 508, loss = 0.18339042\n",
      "Iteration 509, loss = 0.18419931\n",
      "Iteration 510, loss = 0.18345273\n",
      "Iteration 511, loss = 0.18338119\n",
      "Iteration 512, loss = 0.18297201\n",
      "Iteration 513, loss = 0.18309850\n",
      "Iteration 514, loss = 0.18270606\n",
      "Iteration 515, loss = 0.18299582\n",
      "Iteration 516, loss = 0.18259315\n",
      "Iteration 517, loss = 0.18260793\n",
      "Iteration 518, loss = 0.18239752\n",
      "Iteration 519, loss = 0.18247898\n",
      "Iteration 520, loss = 0.18262176\n",
      "Iteration 521, loss = 0.18244281\n",
      "Iteration 522, loss = 0.18196926\n",
      "Iteration 523, loss = 0.18238802\n",
      "Iteration 524, loss = 0.18206341\n",
      "Iteration 525, loss = 0.18207430\n",
      "Iteration 526, loss = 0.18181599\n",
      "Iteration 527, loss = 0.18197073\n",
      "Iteration 528, loss = 0.18228330\n",
      "Iteration 529, loss = 0.18179027\n",
      "Iteration 530, loss = 0.18195736\n",
      "Iteration 531, loss = 0.18196428\n",
      "Iteration 532, loss = 0.18224974\n",
      "Iteration 533, loss = 0.18226377\n",
      "Iteration 534, loss = 0.18174516\n",
      "Iteration 535, loss = 0.18126226\n",
      "Iteration 536, loss = 0.18151939\n",
      "Iteration 537, loss = 0.18147638\n",
      "Iteration 538, loss = 0.18163911\n",
      "Iteration 539, loss = 0.18125617\n",
      "Iteration 540, loss = 0.18164330\n",
      "Iteration 541, loss = 0.18094633\n",
      "Iteration 542, loss = 0.18101885\n",
      "Iteration 543, loss = 0.18101102\n",
      "Iteration 544, loss = 0.18082094\n",
      "Iteration 545, loss = 0.18087494\n",
      "Iteration 546, loss = 0.18094901\n",
      "Iteration 547, loss = 0.18089047\n",
      "Iteration 548, loss = 0.18059965\n",
      "Iteration 549, loss = 0.18047536\n",
      "Iteration 550, loss = 0.18046830\n",
      "Iteration 551, loss = 0.18069415\n",
      "Iteration 552, loss = 0.18079559\n",
      "Iteration 553, loss = 0.18098661\n",
      "Iteration 554, loss = 0.18093498\n",
      "Iteration 555, loss = 0.18027628\n",
      "Iteration 556, loss = 0.18062508\n",
      "Iteration 557, loss = 0.18015372\n",
      "Iteration 558, loss = 0.18035575\n",
      "Iteration 559, loss = 0.18056745\n",
      "Iteration 560, loss = 0.18081930\n",
      "Iteration 561, loss = 0.18018295\n",
      "Iteration 562, loss = 0.18061941\n",
      "Iteration 563, loss = 0.18029318\n",
      "Iteration 564, loss = 0.17962885\n",
      "Iteration 565, loss = 0.18032855\n",
      "Iteration 566, loss = 0.17975550\n",
      "Iteration 567, loss = 0.18101006\n",
      "Iteration 568, loss = 0.18145455\n",
      "Iteration 569, loss = 0.17988817\n",
      "Iteration 570, loss = 0.17989588\n",
      "Iteration 571, loss = 0.18010104\n",
      "Iteration 572, loss = 0.17985902\n",
      "Iteration 573, loss = 0.17950293\n",
      "Iteration 574, loss = 0.17981399\n",
      "Iteration 575, loss = 0.17956757\n",
      "Iteration 576, loss = 0.17937354\n",
      "Iteration 577, loss = 0.17965255\n",
      "Iteration 578, loss = 0.18048592\n",
      "Iteration 579, loss = 0.18016914\n",
      "Iteration 580, loss = 0.17965650\n",
      "Iteration 581, loss = 0.17985523\n",
      "Iteration 582, loss = 0.17943292\n",
      "Iteration 583, loss = 0.17935181\n",
      "Iteration 584, loss = 0.17976740\n",
      "Iteration 585, loss = 0.17960406\n",
      "Iteration 586, loss = 0.17914472\n",
      "Iteration 587, loss = 0.17910457\n",
      "Iteration 588, loss = 0.17908297\n",
      "Iteration 589, loss = 0.17897194\n",
      "Iteration 590, loss = 0.17968145\n",
      "Iteration 591, loss = 0.17913221\n",
      "Iteration 592, loss = 0.17860687\n",
      "Iteration 593, loss = 0.17845692\n",
      "Iteration 594, loss = 0.17841160\n",
      "Iteration 595, loss = 0.17833923\n",
      "Iteration 596, loss = 0.17822756\n",
      "Iteration 597, loss = 0.17885836\n",
      "Iteration 598, loss = 0.17819786\n",
      "Iteration 599, loss = 0.17860538\n",
      "Iteration 600, loss = 0.17973807\n",
      "Iteration 601, loss = 0.17857974\n",
      "Iteration 602, loss = 0.17839910\n",
      "Iteration 603, loss = 0.17849136\n",
      "Iteration 604, loss = 0.17883361\n",
      "Iteration 605, loss = 0.17866110\n",
      "Iteration 606, loss = 0.17842214\n",
      "Iteration 607, loss = 0.17801666\n",
      "Iteration 608, loss = 0.17815694\n",
      "Iteration 609, loss = 0.17812392\n",
      "Iteration 610, loss = 0.17777492\n",
      "Iteration 611, loss = 0.17773702\n",
      "Iteration 612, loss = 0.17817790\n",
      "Iteration 613, loss = 0.17833136\n",
      "Iteration 614, loss = 0.17839025\n",
      "Iteration 615, loss = 0.17770041\n",
      "Iteration 616, loss = 0.17807804\n",
      "Iteration 617, loss = 0.17792293\n",
      "Iteration 618, loss = 0.17823540\n",
      "Iteration 619, loss = 0.17787248\n",
      "Iteration 620, loss = 0.17764925\n",
      "Iteration 621, loss = 0.17741781\n",
      "Iteration 622, loss = 0.17769156\n",
      "Iteration 623, loss = 0.17744158\n",
      "Iteration 624, loss = 0.17759542\n",
      "Iteration 625, loss = 0.17725765\n",
      "Iteration 626, loss = 0.17738241\n",
      "Iteration 627, loss = 0.17787004\n",
      "Iteration 628, loss = 0.17764205\n",
      "Iteration 629, loss = 0.17714823\n",
      "Iteration 630, loss = 0.17719697\n",
      "Iteration 631, loss = 0.17745482\n",
      "Iteration 632, loss = 0.17746287\n",
      "Iteration 633, loss = 0.17693867\n",
      "Iteration 634, loss = 0.17715543\n",
      "Iteration 635, loss = 0.17821922\n",
      "Iteration 636, loss = 0.17759003\n",
      "Iteration 637, loss = 0.17748099\n",
      "Iteration 638, loss = 0.17707911\n",
      "Iteration 639, loss = 0.17718425\n",
      "Iteration 640, loss = 0.17714036\n",
      "Iteration 641, loss = 0.17692990\n",
      "Iteration 642, loss = 0.17690477\n",
      "Iteration 643, loss = 0.17704564\n",
      "Iteration 644, loss = 0.17624208\n",
      "Iteration 645, loss = 0.17651093\n",
      "Iteration 646, loss = 0.17665644\n",
      "Iteration 647, loss = 0.17659891\n",
      "Iteration 648, loss = 0.17640799\n",
      "Iteration 649, loss = 0.17619235\n",
      "Iteration 650, loss = 0.17635078\n",
      "Iteration 651, loss = 0.17632493\n",
      "Iteration 652, loss = 0.17601541\n",
      "Iteration 653, loss = 0.17651483\n",
      "Iteration 654, loss = 0.17644534\n",
      "Iteration 655, loss = 0.17669103\n",
      "Iteration 656, loss = 0.17590094\n",
      "Iteration 657, loss = 0.17605877\n",
      "Iteration 658, loss = 0.17562394\n",
      "Iteration 659, loss = 0.17621430\n",
      "Iteration 660, loss = 0.17565936\n",
      "Iteration 661, loss = 0.17607727\n",
      "Iteration 662, loss = 0.17561525\n",
      "Iteration 663, loss = 0.17562687\n",
      "Iteration 664, loss = 0.17549562\n",
      "Iteration 665, loss = 0.17558312\n",
      "Iteration 666, loss = 0.17617937\n",
      "Iteration 667, loss = 0.17558755\n",
      "Iteration 668, loss = 0.17537423\n",
      "Iteration 669, loss = 0.17556622\n",
      "Iteration 670, loss = 0.17527350\n",
      "Iteration 671, loss = 0.17509038\n",
      "Iteration 672, loss = 0.17572692\n",
      "Iteration 673, loss = 0.17519736\n",
      "Iteration 674, loss = 0.17554050\n",
      "Iteration 675, loss = 0.17534589\n",
      "Iteration 676, loss = 0.17522800\n",
      "Iteration 677, loss = 0.17542860\n",
      "Iteration 678, loss = 0.17506610\n",
      "Iteration 679, loss = 0.17503953\n",
      "Iteration 680, loss = 0.17503924\n",
      "Iteration 681, loss = 0.17547277\n",
      "Iteration 682, loss = 0.17557676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76910871\n",
      "Iteration 2, loss = 0.74306575\n",
      "Iteration 3, loss = 0.72095487\n",
      "Iteration 4, loss = 0.70335612\n",
      "Iteration 5, loss = 0.68923089\n",
      "Iteration 6, loss = 0.67585552\n",
      "Iteration 7, loss = 0.66148509\n",
      "Iteration 8, loss = 0.64526784\n",
      "Iteration 9, loss = 0.62658895\n",
      "Iteration 10, loss = 0.60708849\n",
      "Iteration 11, loss = 0.58762980\n",
      "Iteration 12, loss = 0.56987824\n",
      "Iteration 13, loss = 0.55351729\n",
      "Iteration 14, loss = 0.53969544\n",
      "Iteration 15, loss = 0.52759061\n",
      "Iteration 16, loss = 0.51687852\n",
      "Iteration 17, loss = 0.50729016\n",
      "Iteration 18, loss = 0.49882755\n",
      "Iteration 19, loss = 0.49144357\n",
      "Iteration 20, loss = 0.48458107\n",
      "Iteration 21, loss = 0.47845748\n",
      "Iteration 22, loss = 0.47267016\n",
      "Iteration 23, loss = 0.46767991\n",
      "Iteration 24, loss = 0.46260822\n",
      "Iteration 25, loss = 0.45787647\n",
      "Iteration 26, loss = 0.45287887\n",
      "Iteration 27, loss = 0.44855388\n",
      "Iteration 28, loss = 0.44425947\n",
      "Iteration 29, loss = 0.44035892\n",
      "Iteration 30, loss = 0.43632120\n",
      "Iteration 31, loss = 0.43237798\n",
      "Iteration 32, loss = 0.42880888\n",
      "Iteration 33, loss = 0.42517132\n",
      "Iteration 34, loss = 0.42177549\n",
      "Iteration 35, loss = 0.41796042\n",
      "Iteration 36, loss = 0.41467169\n",
      "Iteration 37, loss = 0.41143225\n",
      "Iteration 38, loss = 0.40808711\n",
      "Iteration 39, loss = 0.40493233\n",
      "Iteration 40, loss = 0.40177160\n",
      "Iteration 41, loss = 0.39876228\n",
      "Iteration 42, loss = 0.39610743\n",
      "Iteration 43, loss = 0.39309233\n",
      "Iteration 44, loss = 0.39052533\n",
      "Iteration 45, loss = 0.38769118\n",
      "Iteration 46, loss = 0.38510632\n",
      "Iteration 47, loss = 0.38242199\n",
      "Iteration 48, loss = 0.37988073\n",
      "Iteration 49, loss = 0.37746821\n",
      "Iteration 50, loss = 0.37490991\n",
      "Iteration 51, loss = 0.37255303\n",
      "Iteration 52, loss = 0.37017546\n",
      "Iteration 53, loss = 0.36788107\n",
      "Iteration 54, loss = 0.36557195\n",
      "Iteration 55, loss = 0.36343410\n",
      "Iteration 56, loss = 0.36109579\n",
      "Iteration 57, loss = 0.35905904\n",
      "Iteration 58, loss = 0.35686648\n",
      "Iteration 59, loss = 0.35487097\n",
      "Iteration 60, loss = 0.35285107\n",
      "Iteration 61, loss = 0.35100233\n",
      "Iteration 62, loss = 0.34908160\n",
      "Iteration 63, loss = 0.34735569\n",
      "Iteration 64, loss = 0.34488570\n",
      "Iteration 65, loss = 0.34308489\n",
      "Iteration 66, loss = 0.34091126\n",
      "Iteration 67, loss = 0.33905379\n",
      "Iteration 68, loss = 0.33733930\n",
      "Iteration 69, loss = 0.33524824\n",
      "Iteration 70, loss = 0.33346972\n",
      "Iteration 71, loss = 0.33160106\n",
      "Iteration 72, loss = 0.33020735\n",
      "Iteration 73, loss = 0.32819454\n",
      "Iteration 74, loss = 0.32648184\n",
      "Iteration 75, loss = 0.32477116\n",
      "Iteration 76, loss = 0.32317959\n",
      "Iteration 77, loss = 0.32149748\n",
      "Iteration 78, loss = 0.31981694\n",
      "Iteration 79, loss = 0.31851283\n",
      "Iteration 80, loss = 0.31685197\n",
      "Iteration 81, loss = 0.31513471\n",
      "Iteration 82, loss = 0.31368968\n",
      "Iteration 83, loss = 0.31214293\n",
      "Iteration 84, loss = 0.31082387\n",
      "Iteration 85, loss = 0.30914359\n",
      "Iteration 86, loss = 0.30783793\n",
      "Iteration 87, loss = 0.30649335\n",
      "Iteration 88, loss = 0.30504633\n",
      "Iteration 89, loss = 0.30352970\n",
      "Iteration 90, loss = 0.30234669\n",
      "Iteration 91, loss = 0.30141508\n",
      "Iteration 92, loss = 0.29972932\n",
      "Iteration 93, loss = 0.29825483\n",
      "Iteration 94, loss = 0.29719519\n",
      "Iteration 95, loss = 0.29577247\n",
      "Iteration 96, loss = 0.29455900\n",
      "Iteration 97, loss = 0.29341323\n",
      "Iteration 98, loss = 0.29208006\n",
      "Iteration 99, loss = 0.29102437\n",
      "Iteration 100, loss = 0.28976419\n",
      "Iteration 101, loss = 0.28901212\n",
      "Iteration 102, loss = 0.28768709\n",
      "Iteration 103, loss = 0.28651508\n",
      "Iteration 104, loss = 0.28579143\n",
      "Iteration 105, loss = 0.28438786\n",
      "Iteration 106, loss = 0.28348901\n",
      "Iteration 107, loss = 0.28234176\n",
      "Iteration 108, loss = 0.28141460\n",
      "Iteration 109, loss = 0.28050485\n",
      "Iteration 110, loss = 0.27995204\n",
      "Iteration 111, loss = 0.27852121\n",
      "Iteration 112, loss = 0.27763217\n",
      "Iteration 113, loss = 0.27699054\n",
      "Iteration 114, loss = 0.27599231\n",
      "Iteration 115, loss = 0.27524645\n",
      "Iteration 116, loss = 0.27432333\n",
      "Iteration 117, loss = 0.27313688\n",
      "Iteration 118, loss = 0.27245362\n",
      "Iteration 119, loss = 0.27163003\n",
      "Iteration 120, loss = 0.27063752\n",
      "Iteration 121, loss = 0.27027175\n",
      "Iteration 122, loss = 0.26927814\n",
      "Iteration 123, loss = 0.26849318\n",
      "Iteration 124, loss = 0.26814765\n",
      "Iteration 125, loss = 0.26688509\n",
      "Iteration 126, loss = 0.26604708\n",
      "Iteration 127, loss = 0.26558341\n",
      "Iteration 128, loss = 0.26472596\n",
      "Iteration 129, loss = 0.26354348\n",
      "Iteration 130, loss = 0.26271267\n",
      "Iteration 131, loss = 0.26179432\n",
      "Iteration 132, loss = 0.26108429\n",
      "Iteration 133, loss = 0.26088027\n",
      "Iteration 134, loss = 0.25958379\n",
      "Iteration 135, loss = 0.25890349\n",
      "Iteration 136, loss = 0.25800331\n",
      "Iteration 137, loss = 0.25748269\n",
      "Iteration 138, loss = 0.25681210\n",
      "Iteration 139, loss = 0.25606740\n",
      "Iteration 140, loss = 0.25538657\n",
      "Iteration 141, loss = 0.25468773\n",
      "Iteration 142, loss = 0.25411122\n",
      "Iteration 143, loss = 0.25331381\n",
      "Iteration 144, loss = 0.25308878\n",
      "Iteration 145, loss = 0.25230259\n",
      "Iteration 146, loss = 0.25148558\n",
      "Iteration 147, loss = 0.25066171\n",
      "Iteration 148, loss = 0.25029611\n",
      "Iteration 149, loss = 0.24930471\n",
      "Iteration 150, loss = 0.24887863\n",
      "Iteration 151, loss = 0.24835153\n",
      "Iteration 152, loss = 0.24791725\n",
      "Iteration 153, loss = 0.24789444\n",
      "Iteration 154, loss = 0.24680285\n",
      "Iteration 155, loss = 0.24599278\n",
      "Iteration 156, loss = 0.24519607\n",
      "Iteration 157, loss = 0.24492737\n",
      "Iteration 158, loss = 0.24455711\n",
      "Iteration 159, loss = 0.24357129\n",
      "Iteration 160, loss = 0.24302409\n",
      "Iteration 161, loss = 0.24245780\n",
      "Iteration 162, loss = 0.24195207\n",
      "Iteration 163, loss = 0.24141164\n",
      "Iteration 164, loss = 0.24055983\n",
      "Iteration 165, loss = 0.24002866\n",
      "Iteration 166, loss = 0.23942306\n",
      "Iteration 167, loss = 0.23868897\n",
      "Iteration 168, loss = 0.23900623\n",
      "Iteration 169, loss = 0.23801946\n",
      "Iteration 170, loss = 0.23760796\n",
      "Iteration 171, loss = 0.23671408\n",
      "Iteration 172, loss = 0.23643461\n",
      "Iteration 173, loss = 0.23587475\n",
      "Iteration 174, loss = 0.23525212\n",
      "Iteration 175, loss = 0.23474274\n",
      "Iteration 176, loss = 0.23419065\n",
      "Iteration 177, loss = 0.23371086\n",
      "Iteration 178, loss = 0.23434804\n",
      "Iteration 179, loss = 0.23290901\n",
      "Iteration 180, loss = 0.23226162\n",
      "Iteration 181, loss = 0.23199581\n",
      "Iteration 182, loss = 0.23137464\n",
      "Iteration 183, loss = 0.23103744\n",
      "Iteration 184, loss = 0.23030424\n",
      "Iteration 185, loss = 0.23012296\n",
      "Iteration 186, loss = 0.22962705\n",
      "Iteration 187, loss = 0.22907743\n",
      "Iteration 188, loss = 0.22875727\n",
      "Iteration 189, loss = 0.22818497\n",
      "Iteration 190, loss = 0.22768365\n",
      "Iteration 191, loss = 0.22771513\n",
      "Iteration 192, loss = 0.22653320\n",
      "Iteration 193, loss = 0.22626539\n",
      "Iteration 194, loss = 0.22705036\n",
      "Iteration 195, loss = 0.22550210\n",
      "Iteration 196, loss = 0.22473981\n",
      "Iteration 197, loss = 0.22460157\n",
      "Iteration 198, loss = 0.22495749\n",
      "Iteration 199, loss = 0.22364858\n",
      "Iteration 200, loss = 0.22368627\n",
      "Iteration 201, loss = 0.22294181\n",
      "Iteration 202, loss = 0.22234439\n",
      "Iteration 203, loss = 0.22187916\n",
      "Iteration 204, loss = 0.22146686\n",
      "Iteration 205, loss = 0.22140992\n",
      "Iteration 206, loss = 0.22056784\n",
      "Iteration 207, loss = 0.22025655\n",
      "Iteration 208, loss = 0.21978745\n",
      "Iteration 209, loss = 0.21945306\n",
      "Iteration 210, loss = 0.21927536\n",
      "Iteration 211, loss = 0.21876032\n",
      "Iteration 212, loss = 0.21832107\n",
      "Iteration 213, loss = 0.21857388\n",
      "Iteration 214, loss = 0.21717302\n",
      "Iteration 215, loss = 0.21781508\n",
      "Iteration 216, loss = 0.21710326\n",
      "Iteration 217, loss = 0.21683758\n",
      "Iteration 218, loss = 0.21616346\n",
      "Iteration 219, loss = 0.21566609\n",
      "Iteration 220, loss = 0.21564366\n",
      "Iteration 221, loss = 0.21506925\n",
      "Iteration 222, loss = 0.21519972\n",
      "Iteration 223, loss = 0.21448998\n",
      "Iteration 224, loss = 0.21439926\n",
      "Iteration 225, loss = 0.21423120\n",
      "Iteration 226, loss = 0.21438082\n",
      "Iteration 227, loss = 0.21283116\n",
      "Iteration 228, loss = 0.21277465\n",
      "Iteration 229, loss = 0.21270303\n",
      "Iteration 230, loss = 0.21220759\n",
      "Iteration 231, loss = 0.21192157\n",
      "Iteration 232, loss = 0.21157716\n",
      "Iteration 233, loss = 0.21114804\n",
      "Iteration 234, loss = 0.21102430\n",
      "Iteration 235, loss = 0.21047129\n",
      "Iteration 236, loss = 0.21059041\n",
      "Iteration 237, loss = 0.21014408\n",
      "Iteration 238, loss = 0.20975072\n",
      "Iteration 239, loss = 0.20979478\n",
      "Iteration 240, loss = 0.20932741\n",
      "Iteration 241, loss = 0.20953645\n",
      "Iteration 242, loss = 0.20861957\n",
      "Iteration 243, loss = 0.20896693\n",
      "Iteration 244, loss = 0.20836545\n",
      "Iteration 245, loss = 0.20828533\n",
      "Iteration 246, loss = 0.20786468\n",
      "Iteration 247, loss = 0.20735822\n",
      "Iteration 248, loss = 0.20790802\n",
      "Iteration 249, loss = 0.20703233\n",
      "Iteration 250, loss = 0.20694451\n",
      "Iteration 251, loss = 0.20665616\n",
      "Iteration 252, loss = 0.20670995\n",
      "Iteration 253, loss = 0.20643569\n",
      "Iteration 254, loss = 0.20622418\n",
      "Iteration 255, loss = 0.20554177\n",
      "Iteration 256, loss = 0.20525328\n",
      "Iteration 257, loss = 0.20533558\n",
      "Iteration 258, loss = 0.20515852\n",
      "Iteration 259, loss = 0.20489850\n",
      "Iteration 260, loss = 0.20456628\n",
      "Iteration 261, loss = 0.20425631\n",
      "Iteration 262, loss = 0.20411669\n",
      "Iteration 263, loss = 0.20393743\n",
      "Iteration 264, loss = 0.20398169\n",
      "Iteration 265, loss = 0.20368270\n",
      "Iteration 266, loss = 0.20351133\n",
      "Iteration 267, loss = 0.20309573\n",
      "Iteration 268, loss = 0.20335468\n",
      "Iteration 269, loss = 0.20288257\n",
      "Iteration 270, loss = 0.20259740\n",
      "Iteration 271, loss = 0.20252427\n",
      "Iteration 272, loss = 0.20227136\n",
      "Iteration 273, loss = 0.20220117\n",
      "Iteration 274, loss = 0.20179185\n",
      "Iteration 275, loss = 0.20193020\n",
      "Iteration 276, loss = 0.20113311\n",
      "Iteration 277, loss = 0.20126064\n",
      "Iteration 278, loss = 0.20129888\n",
      "Iteration 279, loss = 0.20083791\n",
      "Iteration 280, loss = 0.20063801\n",
      "Iteration 281, loss = 0.20010638\n",
      "Iteration 282, loss = 0.20034267\n",
      "Iteration 283, loss = 0.20022644\n",
      "Iteration 284, loss = 0.19982355\n",
      "Iteration 285, loss = 0.19997095\n",
      "Iteration 286, loss = 0.19888927\n",
      "Iteration 287, loss = 0.19951226\n",
      "Iteration 288, loss = 0.19957422\n",
      "Iteration 289, loss = 0.19938935\n",
      "Iteration 290, loss = 0.20003893\n",
      "Iteration 291, loss = 0.19866456\n",
      "Iteration 292, loss = 0.19863215\n",
      "Iteration 293, loss = 0.19845088\n",
      "Iteration 294, loss = 0.19802537\n",
      "Iteration 295, loss = 0.19814283\n",
      "Iteration 296, loss = 0.19764413\n",
      "Iteration 297, loss = 0.19753066\n",
      "Iteration 298, loss = 0.19755116\n",
      "Iteration 299, loss = 0.19727205\n",
      "Iteration 300, loss = 0.19727118\n",
      "Iteration 301, loss = 0.19709848\n",
      "Iteration 302, loss = 0.19729725\n",
      "Iteration 303, loss = 0.19686915\n",
      "Iteration 304, loss = 0.19681791\n",
      "Iteration 305, loss = 0.19654885\n",
      "Iteration 306, loss = 0.19670604\n",
      "Iteration 307, loss = 0.19627167\n",
      "Iteration 308, loss = 0.19676409\n",
      "Iteration 309, loss = 0.19662686\n",
      "Iteration 310, loss = 0.19625083\n",
      "Iteration 311, loss = 0.19595061\n",
      "Iteration 312, loss = 0.19608829\n",
      "Iteration 313, loss = 0.19627356\n",
      "Iteration 314, loss = 0.19588860\n",
      "Iteration 315, loss = 0.19607332\n",
      "Iteration 316, loss = 0.19577963\n",
      "Iteration 317, loss = 0.19530696\n",
      "Iteration 318, loss = 0.19573117\n",
      "Iteration 319, loss = 0.19559399\n",
      "Iteration 320, loss = 0.19534757\n",
      "Iteration 321, loss = 0.19544476\n",
      "Iteration 322, loss = 0.19535082\n",
      "Iteration 323, loss = 0.19527596\n",
      "Iteration 324, loss = 0.19469532\n",
      "Iteration 325, loss = 0.19483259\n",
      "Iteration 326, loss = 0.19457834\n",
      "Iteration 327, loss = 0.19455557\n",
      "Iteration 328, loss = 0.19499788\n",
      "Iteration 329, loss = 0.19435380\n",
      "Iteration 330, loss = 0.19468579\n",
      "Iteration 331, loss = 0.19490259\n",
      "Iteration 332, loss = 0.19471457\n",
      "Iteration 333, loss = 0.19472178\n",
      "Iteration 334, loss = 0.19528070\n",
      "Iteration 335, loss = 0.19412726\n",
      "Iteration 336, loss = 0.19399676\n",
      "Iteration 337, loss = 0.19409074\n",
      "Iteration 338, loss = 0.19429094\n",
      "Iteration 339, loss = 0.19359000\n",
      "Iteration 340, loss = 0.19339356\n",
      "Iteration 341, loss = 0.19372959\n",
      "Iteration 342, loss = 0.19348533\n",
      "Iteration 343, loss = 0.19364259\n",
      "Iteration 344, loss = 0.19353333\n",
      "Iteration 345, loss = 0.19293914\n",
      "Iteration 346, loss = 0.19312450\n",
      "Iteration 347, loss = 0.19286570\n",
      "Iteration 348, loss = 0.19363565\n",
      "Iteration 349, loss = 0.19537475\n",
      "Iteration 350, loss = 0.19340848\n",
      "Iteration 351, loss = 0.19277707\n",
      "Iteration 352, loss = 0.19294780\n",
      "Iteration 353, loss = 0.19249304\n",
      "Iteration 354, loss = 0.19263267\n",
      "Iteration 355, loss = 0.19333168\n",
      "Iteration 356, loss = 0.19239719\n",
      "Iteration 357, loss = 0.19262194\n",
      "Iteration 358, loss = 0.19205954\n",
      "Iteration 359, loss = 0.19256737\n",
      "Iteration 360, loss = 0.19241919\n",
      "Iteration 361, loss = 0.19253185\n",
      "Iteration 362, loss = 0.19230221\n",
      "Iteration 363, loss = 0.19209552\n",
      "Iteration 364, loss = 0.19221944\n",
      "Iteration 365, loss = 0.19185813\n",
      "Iteration 366, loss = 0.19204890\n",
      "Iteration 367, loss = 0.19195585\n",
      "Iteration 368, loss = 0.19228028\n",
      "Iteration 369, loss = 0.19192899\n",
      "Iteration 370, loss = 0.19156966\n",
      "Iteration 371, loss = 0.19195340\n",
      "Iteration 372, loss = 0.19217296\n",
      "Iteration 373, loss = 0.19194197\n",
      "Iteration 374, loss = 0.19167514\n",
      "Iteration 375, loss = 0.19150571\n",
      "Iteration 376, loss = 0.19172125\n",
      "Iteration 377, loss = 0.19145866\n",
      "Iteration 378, loss = 0.19195325\n",
      "Iteration 379, loss = 0.19141164\n",
      "Iteration 380, loss = 0.19110716\n",
      "Iteration 381, loss = 0.19117938\n",
      "Iteration 382, loss = 0.19096846\n",
      "Iteration 383, loss = 0.19141170\n",
      "Iteration 384, loss = 0.19131152\n",
      "Iteration 385, loss = 0.19152052\n",
      "Iteration 386, loss = 0.19103936\n",
      "Iteration 387, loss = 0.19110520\n",
      "Iteration 388, loss = 0.19062139\n",
      "Iteration 389, loss = 0.19089580\n",
      "Iteration 390, loss = 0.19082921\n",
      "Iteration 391, loss = 0.19087151\n",
      "Iteration 392, loss = 0.19047534\n",
      "Iteration 393, loss = 0.19081433\n",
      "Iteration 394, loss = 0.19068157\n",
      "Iteration 395, loss = 0.19065667\n",
      "Iteration 396, loss = 0.19071261\n",
      "Iteration 397, loss = 0.19062530\n",
      "Iteration 398, loss = 0.18996621\n",
      "Iteration 399, loss = 0.19136964\n",
      "Iteration 400, loss = 0.19053598\n",
      "Iteration 401, loss = 0.19057693\n",
      "Iteration 402, loss = 0.19043854\n",
      "Iteration 403, loss = 0.19028939\n",
      "Iteration 404, loss = 0.19056050\n",
      "Iteration 405, loss = 0.19017016\n",
      "Iteration 406, loss = 0.19006590\n",
      "Iteration 407, loss = 0.19012169\n",
      "Iteration 408, loss = 0.19041579\n",
      "Iteration 409, loss = 0.19020473\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76947034\n",
      "Iteration 2, loss = 0.74386837\n",
      "Iteration 3, loss = 0.72246918\n",
      "Iteration 4, loss = 0.70523938\n",
      "Iteration 5, loss = 0.69171699\n",
      "Iteration 6, loss = 0.67937949\n",
      "Iteration 7, loss = 0.66631109\n",
      "Iteration 8, loss = 0.65143797\n",
      "Iteration 9, loss = 0.63403670\n",
      "Iteration 10, loss = 0.61510393\n",
      "Iteration 11, loss = 0.59573462\n",
      "Iteration 12, loss = 0.57740658\n",
      "Iteration 13, loss = 0.56025603\n",
      "Iteration 14, loss = 0.54540653\n",
      "Iteration 15, loss = 0.53278696\n",
      "Iteration 16, loss = 0.52164152\n",
      "Iteration 17, loss = 0.51206103\n",
      "Iteration 18, loss = 0.50369550\n",
      "Iteration 19, loss = 0.49617434\n",
      "Iteration 20, loss = 0.48947265\n",
      "Iteration 21, loss = 0.48309880\n",
      "Iteration 22, loss = 0.47733853\n",
      "Iteration 23, loss = 0.47179033\n",
      "Iteration 24, loss = 0.46630614\n",
      "Iteration 25, loss = 0.46158181\n",
      "Iteration 26, loss = 0.45705334\n",
      "Iteration 27, loss = 0.45269493\n",
      "Iteration 28, loss = 0.44836075\n",
      "Iteration 29, loss = 0.44442879\n",
      "Iteration 30, loss = 0.44042474\n",
      "Iteration 31, loss = 0.43658734\n",
      "Iteration 32, loss = 0.43298079\n",
      "Iteration 33, loss = 0.42940992\n",
      "Iteration 34, loss = 0.42596983\n",
      "Iteration 35, loss = 0.42255517\n",
      "Iteration 36, loss = 0.41932022\n",
      "Iteration 37, loss = 0.41617751\n",
      "Iteration 38, loss = 0.41265916\n",
      "Iteration 39, loss = 0.40976333\n",
      "Iteration 40, loss = 0.40672882\n",
      "Iteration 41, loss = 0.40355556\n",
      "Iteration 42, loss = 0.40033634\n",
      "Iteration 43, loss = 0.39769406\n",
      "Iteration 44, loss = 0.39514008\n",
      "Iteration 45, loss = 0.39216299\n",
      "Iteration 46, loss = 0.38954562\n",
      "Iteration 47, loss = 0.38698568\n",
      "Iteration 48, loss = 0.38458945\n",
      "Iteration 49, loss = 0.38227892\n",
      "Iteration 50, loss = 0.37953060\n",
      "Iteration 51, loss = 0.37742845\n",
      "Iteration 52, loss = 0.37498451\n",
      "Iteration 53, loss = 0.37286085\n",
      "Iteration 54, loss = 0.37037225\n",
      "Iteration 55, loss = 0.36831071\n",
      "Iteration 56, loss = 0.36608077\n",
      "Iteration 57, loss = 0.36421502\n",
      "Iteration 58, loss = 0.36196848\n",
      "Iteration 59, loss = 0.36012462\n",
      "Iteration 60, loss = 0.35814735\n",
      "Iteration 61, loss = 0.35610455\n",
      "Iteration 62, loss = 0.35433669\n",
      "Iteration 63, loss = 0.35240330\n",
      "Iteration 64, loss = 0.35060358\n",
      "Iteration 65, loss = 0.34860023\n",
      "Iteration 66, loss = 0.34686191\n",
      "Iteration 67, loss = 0.34509998\n",
      "Iteration 68, loss = 0.34343814\n",
      "Iteration 69, loss = 0.34149491\n",
      "Iteration 70, loss = 0.33978679\n",
      "Iteration 71, loss = 0.33819666\n",
      "Iteration 72, loss = 0.33651676\n",
      "Iteration 73, loss = 0.33460131\n",
      "Iteration 74, loss = 0.33304556\n",
      "Iteration 75, loss = 0.33132162\n",
      "Iteration 76, loss = 0.32985691\n",
      "Iteration 77, loss = 0.32845946\n",
      "Iteration 78, loss = 0.32651183\n",
      "Iteration 79, loss = 0.32513597\n",
      "Iteration 80, loss = 0.32355733\n",
      "Iteration 81, loss = 0.32203855\n",
      "Iteration 82, loss = 0.32070510\n",
      "Iteration 83, loss = 0.31916785\n",
      "Iteration 84, loss = 0.31787376\n",
      "Iteration 85, loss = 0.31646000\n",
      "Iteration 86, loss = 0.31501547\n",
      "Iteration 87, loss = 0.31367447\n",
      "Iteration 88, loss = 0.31252365\n",
      "Iteration 89, loss = 0.31141214\n",
      "Iteration 90, loss = 0.30976407\n",
      "Iteration 91, loss = 0.30874945\n",
      "Iteration 92, loss = 0.30713293\n",
      "Iteration 93, loss = 0.30603235\n",
      "Iteration 94, loss = 0.30511137\n",
      "Iteration 95, loss = 0.30354041\n",
      "Iteration 96, loss = 0.30315192\n",
      "Iteration 97, loss = 0.30184512\n",
      "Iteration 98, loss = 0.30021560\n",
      "Iteration 99, loss = 0.29889984\n",
      "Iteration 100, loss = 0.29755179\n",
      "Iteration 101, loss = 0.29665020\n",
      "Iteration 102, loss = 0.29560985\n",
      "Iteration 103, loss = 0.29450898\n",
      "Iteration 104, loss = 0.29358486\n",
      "Iteration 105, loss = 0.29228693\n",
      "Iteration 106, loss = 0.29114050\n",
      "Iteration 107, loss = 0.29063027\n",
      "Iteration 108, loss = 0.28914210\n",
      "Iteration 109, loss = 0.28825090\n",
      "Iteration 110, loss = 0.28738272\n",
      "Iteration 111, loss = 0.28628172\n",
      "Iteration 112, loss = 0.28545209\n",
      "Iteration 113, loss = 0.28425804\n",
      "Iteration 114, loss = 0.28350889\n",
      "Iteration 115, loss = 0.28254310\n",
      "Iteration 116, loss = 0.28168596\n",
      "Iteration 117, loss = 0.28068799\n",
      "Iteration 118, loss = 0.28013480\n",
      "Iteration 119, loss = 0.27918889\n",
      "Iteration 120, loss = 0.27841175\n",
      "Iteration 121, loss = 0.27763195\n",
      "Iteration 122, loss = 0.27691911\n",
      "Iteration 123, loss = 0.27639063\n",
      "Iteration 124, loss = 0.27536290\n",
      "Iteration 125, loss = 0.27447325\n",
      "Iteration 126, loss = 0.27382023\n",
      "Iteration 127, loss = 0.27275831\n",
      "Iteration 128, loss = 0.27191696\n",
      "Iteration 129, loss = 0.27132046\n",
      "Iteration 130, loss = 0.27034257\n",
      "Iteration 131, loss = 0.26986262\n",
      "Iteration 132, loss = 0.26901448\n",
      "Iteration 133, loss = 0.26838793\n",
      "Iteration 134, loss = 0.26745285\n",
      "Iteration 135, loss = 0.26694484\n",
      "Iteration 136, loss = 0.26616911\n",
      "Iteration 137, loss = 0.26574030\n",
      "Iteration 138, loss = 0.26473823\n",
      "Iteration 139, loss = 0.26412632\n",
      "Iteration 140, loss = 0.26370458\n",
      "Iteration 141, loss = 0.26289239\n",
      "Iteration 142, loss = 0.26249722\n",
      "Iteration 143, loss = 0.26184006\n",
      "Iteration 144, loss = 0.26114092\n",
      "Iteration 145, loss = 0.26067750\n",
      "Iteration 146, loss = 0.25988497\n",
      "Iteration 147, loss = 0.25945300\n",
      "Iteration 148, loss = 0.25897735\n",
      "Iteration 149, loss = 0.25863910\n",
      "Iteration 150, loss = 0.25791546\n",
      "Iteration 151, loss = 0.25704139\n",
      "Iteration 152, loss = 0.25636746\n",
      "Iteration 153, loss = 0.25655672\n",
      "Iteration 154, loss = 0.25670644\n",
      "Iteration 155, loss = 0.25500762\n",
      "Iteration 156, loss = 0.25475837\n",
      "Iteration 157, loss = 0.25363534\n",
      "Iteration 158, loss = 0.25317656\n",
      "Iteration 159, loss = 0.25307871\n",
      "Iteration 160, loss = 0.25240605\n",
      "Iteration 161, loss = 0.25183061\n",
      "Iteration 162, loss = 0.25136477\n",
      "Iteration 163, loss = 0.25076475\n",
      "Iteration 164, loss = 0.25012913\n",
      "Iteration 165, loss = 0.24999989\n",
      "Iteration 166, loss = 0.24946250\n",
      "Iteration 167, loss = 0.24871865\n",
      "Iteration 168, loss = 0.24860219\n",
      "Iteration 169, loss = 0.24805601\n",
      "Iteration 170, loss = 0.24728551\n",
      "Iteration 171, loss = 0.24684087\n",
      "Iteration 172, loss = 0.24622331\n",
      "Iteration 173, loss = 0.24674459\n",
      "Iteration 174, loss = 0.24521118\n",
      "Iteration 175, loss = 0.24511741\n",
      "Iteration 176, loss = 0.24399250\n",
      "Iteration 177, loss = 0.24358554\n",
      "Iteration 178, loss = 0.24408248\n",
      "Iteration 179, loss = 0.24283021\n",
      "Iteration 180, loss = 0.24188760\n",
      "Iteration 181, loss = 0.24149300\n",
      "Iteration 182, loss = 0.24104747\n",
      "Iteration 183, loss = 0.24051805\n",
      "Iteration 184, loss = 0.24051462\n",
      "Iteration 185, loss = 0.23962932\n",
      "Iteration 186, loss = 0.23908505\n",
      "Iteration 187, loss = 0.23864672\n",
      "Iteration 188, loss = 0.23880821\n",
      "Iteration 189, loss = 0.23787644\n",
      "Iteration 190, loss = 0.23795374\n",
      "Iteration 191, loss = 0.23696479\n",
      "Iteration 192, loss = 0.23652881\n",
      "Iteration 193, loss = 0.23652583\n",
      "Iteration 194, loss = 0.23570880\n",
      "Iteration 195, loss = 0.23556023\n",
      "Iteration 196, loss = 0.23562556\n",
      "Iteration 197, loss = 0.23452731\n",
      "Iteration 198, loss = 0.23400038\n",
      "Iteration 199, loss = 0.23396310\n",
      "Iteration 200, loss = 0.23330325\n",
      "Iteration 201, loss = 0.23355418\n",
      "Iteration 202, loss = 0.23244340\n",
      "Iteration 203, loss = 0.23219767\n",
      "Iteration 204, loss = 0.23168436\n",
      "Iteration 205, loss = 0.23240657\n",
      "Iteration 206, loss = 0.23135164\n",
      "Iteration 207, loss = 0.23100317\n",
      "Iteration 208, loss = 0.23047212\n",
      "Iteration 209, loss = 0.22979790\n",
      "Iteration 210, loss = 0.22936696\n",
      "Iteration 211, loss = 0.22917130\n",
      "Iteration 212, loss = 0.22883443\n",
      "Iteration 213, loss = 0.22836497\n",
      "Iteration 214, loss = 0.22841124\n",
      "Iteration 215, loss = 0.22769985\n",
      "Iteration 216, loss = 0.22719030\n",
      "Iteration 217, loss = 0.22705429\n",
      "Iteration 218, loss = 0.22699093\n",
      "Iteration 219, loss = 0.22663684\n",
      "Iteration 220, loss = 0.22647953\n",
      "Iteration 221, loss = 0.22612495\n",
      "Iteration 222, loss = 0.22547071\n",
      "Iteration 223, loss = 0.22525998\n",
      "Iteration 224, loss = 0.22463388\n",
      "Iteration 225, loss = 0.22502939\n",
      "Iteration 226, loss = 0.22400649\n",
      "Iteration 227, loss = 0.22387234\n",
      "Iteration 228, loss = 0.22367555\n",
      "Iteration 229, loss = 0.22328210\n",
      "Iteration 230, loss = 0.22320988\n",
      "Iteration 231, loss = 0.22238870\n",
      "Iteration 232, loss = 0.22249888\n",
      "Iteration 233, loss = 0.22178335\n",
      "Iteration 234, loss = 0.22155221\n",
      "Iteration 235, loss = 0.22197623\n",
      "Iteration 236, loss = 0.22090163\n",
      "Iteration 237, loss = 0.22118798\n",
      "Iteration 238, loss = 0.22059504\n",
      "Iteration 239, loss = 0.22079772\n",
      "Iteration 240, loss = 0.21971025\n",
      "Iteration 241, loss = 0.21944885\n",
      "Iteration 242, loss = 0.21946159\n",
      "Iteration 243, loss = 0.21923531\n",
      "Iteration 244, loss = 0.21983788\n",
      "Iteration 245, loss = 0.21860676\n",
      "Iteration 246, loss = 0.21827334\n",
      "Iteration 247, loss = 0.21775937\n",
      "Iteration 248, loss = 0.21758401\n",
      "Iteration 249, loss = 0.21702033\n",
      "Iteration 250, loss = 0.21688481\n",
      "Iteration 251, loss = 0.21755366\n",
      "Iteration 252, loss = 0.21669968\n",
      "Iteration 253, loss = 0.21617894\n",
      "Iteration 254, loss = 0.21633825\n",
      "Iteration 255, loss = 0.21584788\n",
      "Iteration 256, loss = 0.21569864\n",
      "Iteration 257, loss = 0.21514517\n",
      "Iteration 258, loss = 0.21498167\n",
      "Iteration 259, loss = 0.21475416\n",
      "Iteration 260, loss = 0.21502999\n",
      "Iteration 261, loss = 0.21435503\n",
      "Iteration 262, loss = 0.21431479\n",
      "Iteration 263, loss = 0.21406958\n",
      "Iteration 264, loss = 0.21352071\n",
      "Iteration 265, loss = 0.21376073\n",
      "Iteration 266, loss = 0.21358221\n",
      "Iteration 267, loss = 0.21395749\n",
      "Iteration 268, loss = 0.21276991\n",
      "Iteration 269, loss = 0.21261988\n",
      "Iteration 270, loss = 0.21256525\n",
      "Iteration 271, loss = 0.21239191\n",
      "Iteration 272, loss = 0.21187874\n",
      "Iteration 273, loss = 0.21190840\n",
      "Iteration 274, loss = 0.21139604\n",
      "Iteration 275, loss = 0.21112532\n",
      "Iteration 276, loss = 0.21112990\n",
      "Iteration 277, loss = 0.21103405\n",
      "Iteration 278, loss = 0.21060638\n",
      "Iteration 279, loss = 0.21077527\n",
      "Iteration 280, loss = 0.21023413\n",
      "Iteration 281, loss = 0.21021537\n",
      "Iteration 282, loss = 0.21035233\n",
      "Iteration 283, loss = 0.20985815\n",
      "Iteration 284, loss = 0.21004184\n",
      "Iteration 285, loss = 0.21049759\n",
      "Iteration 286, loss = 0.20936237\n",
      "Iteration 287, loss = 0.20893995\n",
      "Iteration 288, loss = 0.20877235\n",
      "Iteration 289, loss = 0.20919794\n",
      "Iteration 290, loss = 0.20823485\n",
      "Iteration 291, loss = 0.20843276\n",
      "Iteration 292, loss = 0.20808866\n",
      "Iteration 293, loss = 0.20771799\n",
      "Iteration 294, loss = 0.20788750\n",
      "Iteration 295, loss = 0.20802734\n",
      "Iteration 296, loss = 0.20755991\n",
      "Iteration 297, loss = 0.20697320\n",
      "Iteration 298, loss = 0.20711262\n",
      "Iteration 299, loss = 0.20713551\n",
      "Iteration 300, loss = 0.20729383\n",
      "Iteration 301, loss = 0.20716197\n",
      "Iteration 302, loss = 0.20649802\n",
      "Iteration 303, loss = 0.20579866\n",
      "Iteration 304, loss = 0.20570384\n",
      "Iteration 305, loss = 0.20579326\n",
      "Iteration 306, loss = 0.20645329\n",
      "Iteration 307, loss = 0.20645040\n",
      "Iteration 308, loss = 0.20519802\n",
      "Iteration 309, loss = 0.20530737\n",
      "Iteration 310, loss = 0.20478184\n",
      "Iteration 311, loss = 0.20474799\n",
      "Iteration 312, loss = 0.20451172\n",
      "Iteration 313, loss = 0.20423926\n",
      "Iteration 314, loss = 0.20415851\n",
      "Iteration 315, loss = 0.20401578\n",
      "Iteration 316, loss = 0.20396876\n",
      "Iteration 317, loss = 0.20397525\n",
      "Iteration 318, loss = 0.20427118\n",
      "Iteration 319, loss = 0.20358831\n",
      "Iteration 320, loss = 0.20348555\n",
      "Iteration 321, loss = 0.20374216\n",
      "Iteration 322, loss = 0.20304535\n",
      "Iteration 323, loss = 0.20329089\n",
      "Iteration 324, loss = 0.20282852\n",
      "Iteration 325, loss = 0.20259785\n",
      "Iteration 326, loss = 0.20229766\n",
      "Iteration 327, loss = 0.20237557\n",
      "Iteration 328, loss = 0.20184517\n",
      "Iteration 329, loss = 0.20195632\n",
      "Iteration 330, loss = 0.20204973\n",
      "Iteration 331, loss = 0.20171121\n",
      "Iteration 332, loss = 0.20162566\n",
      "Iteration 333, loss = 0.20167667\n",
      "Iteration 334, loss = 0.20210315\n",
      "Iteration 335, loss = 0.20094681\n",
      "Iteration 336, loss = 0.20095533\n",
      "Iteration 337, loss = 0.20085083\n",
      "Iteration 338, loss = 0.20065864\n",
      "Iteration 339, loss = 0.20110715\n",
      "Iteration 340, loss = 0.20052354\n",
      "Iteration 341, loss = 0.20033822\n",
      "Iteration 342, loss = 0.20012103\n",
      "Iteration 343, loss = 0.20010662\n",
      "Iteration 344, loss = 0.20016604\n",
      "Iteration 345, loss = 0.19973732\n",
      "Iteration 346, loss = 0.19972074\n",
      "Iteration 347, loss = 0.19937486\n",
      "Iteration 348, loss = 0.19943930\n",
      "Iteration 349, loss = 0.19950870\n",
      "Iteration 350, loss = 0.19937666\n",
      "Iteration 351, loss = 0.19939664\n",
      "Iteration 352, loss = 0.19922459\n",
      "Iteration 353, loss = 0.19855456\n",
      "Iteration 354, loss = 0.19911507\n",
      "Iteration 355, loss = 0.19856608\n",
      "Iteration 356, loss = 0.19878666\n",
      "Iteration 357, loss = 0.19843207\n",
      "Iteration 358, loss = 0.19850293\n",
      "Iteration 359, loss = 0.19789125\n",
      "Iteration 360, loss = 0.19768687\n",
      "Iteration 361, loss = 0.19831115\n",
      "Iteration 362, loss = 0.19793917\n",
      "Iteration 363, loss = 0.19780108\n",
      "Iteration 364, loss = 0.19752608\n",
      "Iteration 365, loss = 0.19776118\n",
      "Iteration 366, loss = 0.19808622\n",
      "Iteration 367, loss = 0.19718362\n",
      "Iteration 368, loss = 0.19764678\n",
      "Iteration 369, loss = 0.19712831\n",
      "Iteration 370, loss = 0.19694991\n",
      "Iteration 371, loss = 0.19680215\n",
      "Iteration 372, loss = 0.19682893\n",
      "Iteration 373, loss = 0.19698561\n",
      "Iteration 374, loss = 0.19652198\n",
      "Iteration 375, loss = 0.19682696\n",
      "Iteration 376, loss = 0.19650642\n",
      "Iteration 377, loss = 0.19666675\n",
      "Iteration 378, loss = 0.19654575\n",
      "Iteration 379, loss = 0.19630214\n",
      "Iteration 380, loss = 0.19649931\n",
      "Iteration 381, loss = 0.19580995\n",
      "Iteration 382, loss = 0.19603444\n",
      "Iteration 383, loss = 0.19583453\n",
      "Iteration 384, loss = 0.19556870\n",
      "Iteration 385, loss = 0.19560399\n",
      "Iteration 386, loss = 0.19548806\n",
      "Iteration 387, loss = 0.19487553\n",
      "Iteration 388, loss = 0.19517990\n",
      "Iteration 389, loss = 0.19491752\n",
      "Iteration 390, loss = 0.19576126\n",
      "Iteration 391, loss = 0.19543938\n",
      "Iteration 392, loss = 0.19495976\n",
      "Iteration 393, loss = 0.19447844\n",
      "Iteration 394, loss = 0.19515989\n",
      "Iteration 395, loss = 0.19466369\n",
      "Iteration 396, loss = 0.19497144\n",
      "Iteration 397, loss = 0.19466432\n",
      "Iteration 398, loss = 0.19418755\n",
      "Iteration 399, loss = 0.19551632\n",
      "Iteration 400, loss = 0.19429831\n",
      "Iteration 401, loss = 0.19411071\n",
      "Iteration 402, loss = 0.19404485\n",
      "Iteration 403, loss = 0.19369196\n",
      "Iteration 404, loss = 0.19369131\n",
      "Iteration 405, loss = 0.19386781\n",
      "Iteration 406, loss = 0.19349124\n",
      "Iteration 407, loss = 0.19388817\n",
      "Iteration 408, loss = 0.19320758\n",
      "Iteration 409, loss = 0.19394622\n",
      "Iteration 410, loss = 0.19374325\n",
      "Iteration 411, loss = 0.19380513\n",
      "Iteration 412, loss = 0.19299796\n",
      "Iteration 413, loss = 0.19347756\n",
      "Iteration 414, loss = 0.19336980\n",
      "Iteration 415, loss = 0.19296303\n",
      "Iteration 416, loss = 0.19299398\n",
      "Iteration 417, loss = 0.19339950\n",
      "Iteration 418, loss = 0.19277755\n",
      "Iteration 419, loss = 0.19270796\n",
      "Iteration 420, loss = 0.19262147\n",
      "Iteration 421, loss = 0.19250290\n",
      "Iteration 422, loss = 0.19274923\n",
      "Iteration 423, loss = 0.19257489\n",
      "Iteration 424, loss = 0.19246315\n",
      "Iteration 425, loss = 0.19291513\n",
      "Iteration 426, loss = 0.19314923\n",
      "Iteration 427, loss = 0.19204170\n",
      "Iteration 428, loss = 0.19242756\n",
      "Iteration 429, loss = 0.19302482\n",
      "Iteration 430, loss = 0.19149299\n",
      "Iteration 431, loss = 0.19175684\n",
      "Iteration 432, loss = 0.19142638\n",
      "Iteration 433, loss = 0.19168415\n",
      "Iteration 434, loss = 0.19189460\n",
      "Iteration 435, loss = 0.19223813\n",
      "Iteration 436, loss = 0.19123803\n",
      "Iteration 437, loss = 0.19159829\n",
      "Iteration 438, loss = 0.19150311\n",
      "Iteration 439, loss = 0.19102931\n",
      "Iteration 440, loss = 0.19105447\n",
      "Iteration 441, loss = 0.19101885\n",
      "Iteration 442, loss = 0.19072963\n",
      "Iteration 443, loss = 0.19177058\n",
      "Iteration 444, loss = 0.19104639\n",
      "Iteration 445, loss = 0.19101281\n",
      "Iteration 446, loss = 0.19087366\n",
      "Iteration 447, loss = 0.19093724\n",
      "Iteration 448, loss = 0.19090797\n",
      "Iteration 449, loss = 0.19033347\n",
      "Iteration 450, loss = 0.19064953\n",
      "Iteration 451, loss = 0.19058882\n",
      "Iteration 452, loss = 0.19041446\n",
      "Iteration 453, loss = 0.19068695\n",
      "Iteration 454, loss = 0.19079318\n",
      "Iteration 455, loss = 0.18980828\n",
      "Iteration 456, loss = 0.19027277\n",
      "Iteration 457, loss = 0.19002626\n",
      "Iteration 458, loss = 0.18995816\n",
      "Iteration 459, loss = 0.18981389\n",
      "Iteration 460, loss = 0.19012429\n",
      "Iteration 461, loss = 0.18950938\n",
      "Iteration 462, loss = 0.18965905\n",
      "Iteration 463, loss = 0.18959650\n",
      "Iteration 464, loss = 0.19011641\n",
      "Iteration 465, loss = 0.18941672\n",
      "Iteration 466, loss = 0.18953864\n",
      "Iteration 467, loss = 0.18932951\n",
      "Iteration 468, loss = 0.18910837\n",
      "Iteration 469, loss = 0.18961872\n",
      "Iteration 470, loss = 0.18938677\n",
      "Iteration 471, loss = 0.18927452\n",
      "Iteration 472, loss = 0.18956797\n",
      "Iteration 473, loss = 0.18886649\n",
      "Iteration 474, loss = 0.18929045\n",
      "Iteration 475, loss = 0.18903245\n",
      "Iteration 476, loss = 0.18876559\n",
      "Iteration 477, loss = 0.18892076\n",
      "Iteration 478, loss = 0.18933400\n",
      "Iteration 479, loss = 0.18875766\n",
      "Iteration 480, loss = 0.18915748\n",
      "Iteration 481, loss = 0.18906415\n",
      "Iteration 482, loss = 0.18883709\n",
      "Iteration 483, loss = 0.18868378\n",
      "Iteration 484, loss = 0.18837212\n",
      "Iteration 485, loss = 0.18838370\n",
      "Iteration 486, loss = 0.18889735\n",
      "Iteration 487, loss = 0.18864188\n",
      "Iteration 488, loss = 0.18914715\n",
      "Iteration 489, loss = 0.18909105\n",
      "Iteration 490, loss = 0.18864781\n",
      "Iteration 491, loss = 0.18855863\n",
      "Iteration 492, loss = 0.18879443\n",
      "Iteration 493, loss = 0.18824945\n",
      "Iteration 494, loss = 0.18878461\n",
      "Iteration 495, loss = 0.18812263\n",
      "Iteration 496, loss = 0.18849273\n",
      "Iteration 497, loss = 0.18825385\n",
      "Iteration 498, loss = 0.18760072\n",
      "Iteration 499, loss = 0.18811740\n",
      "Iteration 500, loss = 0.18802005\n",
      "Iteration 501, loss = 0.18812373\n",
      "Iteration 502, loss = 0.18803644\n",
      "Iteration 503, loss = 0.18812770\n",
      "Iteration 504, loss = 0.18954112\n",
      "Iteration 505, loss = 0.18804529\n",
      "Iteration 506, loss = 0.18787332\n",
      "Iteration 507, loss = 0.18772849\n",
      "Iteration 508, loss = 0.18748474\n",
      "Iteration 509, loss = 0.18793108\n",
      "Iteration 510, loss = 0.18775061\n",
      "Iteration 511, loss = 0.18776301\n",
      "Iteration 512, loss = 0.18807709\n",
      "Iteration 513, loss = 0.18822923\n",
      "Iteration 514, loss = 0.18815378\n",
      "Iteration 515, loss = 0.18846788\n",
      "Iteration 516, loss = 0.18798557\n",
      "Iteration 517, loss = 0.18753465\n",
      "Iteration 518, loss = 0.18738299\n",
      "Iteration 519, loss = 0.18756301\n",
      "Iteration 520, loss = 0.18822262\n",
      "Iteration 521, loss = 0.18745929\n",
      "Iteration 522, loss = 0.18815971\n",
      "Iteration 523, loss = 0.18738075\n",
      "Iteration 524, loss = 0.18806394\n",
      "Iteration 525, loss = 0.18698681\n",
      "Iteration 526, loss = 0.18719178\n",
      "Iteration 527, loss = 0.18703393\n",
      "Iteration 528, loss = 0.18762032\n",
      "Iteration 529, loss = 0.18779497\n",
      "Iteration 530, loss = 0.18883037\n",
      "Iteration 531, loss = 0.18850949\n",
      "Iteration 532, loss = 0.18717506\n",
      "Iteration 533, loss = 0.18730156\n",
      "Iteration 534, loss = 0.18877035\n",
      "Iteration 535, loss = 0.18694411\n",
      "Iteration 536, loss = 0.18697467\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76880268\n",
      "Iteration 2, loss = 0.74250656\n",
      "Iteration 3, loss = 0.72079488\n",
      "Iteration 4, loss = 0.70400590\n",
      "Iteration 5, loss = 0.68978327\n",
      "Iteration 6, loss = 0.67648259\n",
      "Iteration 7, loss = 0.66208091\n",
      "Iteration 8, loss = 0.64619491\n",
      "Iteration 9, loss = 0.62756182\n",
      "Iteration 10, loss = 0.60863525\n",
      "Iteration 11, loss = 0.58955306\n",
      "Iteration 12, loss = 0.57183290\n",
      "Iteration 13, loss = 0.55570981\n",
      "Iteration 14, loss = 0.54203769\n",
      "Iteration 15, loss = 0.52968783\n",
      "Iteration 16, loss = 0.51943351\n",
      "Iteration 17, loss = 0.50978338\n",
      "Iteration 18, loss = 0.50173055\n",
      "Iteration 19, loss = 0.49424201\n",
      "Iteration 20, loss = 0.48754462\n",
      "Iteration 21, loss = 0.48106628\n",
      "Iteration 22, loss = 0.47521335\n",
      "Iteration 23, loss = 0.46939731\n",
      "Iteration 24, loss = 0.46418467\n",
      "Iteration 25, loss = 0.45932616\n",
      "Iteration 26, loss = 0.45484030\n",
      "Iteration 27, loss = 0.45072780\n",
      "Iteration 28, loss = 0.44633823\n",
      "Iteration 29, loss = 0.44242830\n",
      "Iteration 30, loss = 0.43857588\n",
      "Iteration 31, loss = 0.43474003\n",
      "Iteration 32, loss = 0.43115704\n",
      "Iteration 33, loss = 0.42723142\n",
      "Iteration 34, loss = 0.42385369\n",
      "Iteration 35, loss = 0.42042437\n",
      "Iteration 36, loss = 0.41683249\n",
      "Iteration 37, loss = 0.41368902\n",
      "Iteration 38, loss = 0.41036081\n",
      "Iteration 39, loss = 0.40732086\n",
      "Iteration 40, loss = 0.40423745\n",
      "Iteration 41, loss = 0.40143323\n",
      "Iteration 42, loss = 0.39886145\n",
      "Iteration 43, loss = 0.39588889\n",
      "Iteration 44, loss = 0.39325668\n",
      "Iteration 45, loss = 0.39062807\n",
      "Iteration 46, loss = 0.38788352\n",
      "Iteration 47, loss = 0.38530889\n",
      "Iteration 48, loss = 0.38293359\n",
      "Iteration 49, loss = 0.38040694\n",
      "Iteration 50, loss = 0.37790175\n",
      "Iteration 51, loss = 0.37573090\n",
      "Iteration 52, loss = 0.37321913\n",
      "Iteration 53, loss = 0.37101237\n",
      "Iteration 54, loss = 0.36895849\n",
      "Iteration 55, loss = 0.36670859\n",
      "Iteration 56, loss = 0.36445146\n",
      "Iteration 57, loss = 0.36241743\n",
      "Iteration 58, loss = 0.36021769\n",
      "Iteration 59, loss = 0.35823543\n",
      "Iteration 60, loss = 0.35621363\n",
      "Iteration 61, loss = 0.35420672\n",
      "Iteration 62, loss = 0.35255685\n",
      "Iteration 63, loss = 0.35043282\n",
      "Iteration 64, loss = 0.34845142\n",
      "Iteration 65, loss = 0.34654865\n",
      "Iteration 66, loss = 0.34463930\n",
      "Iteration 67, loss = 0.34293893\n",
      "Iteration 68, loss = 0.34102188\n",
      "Iteration 69, loss = 0.33930808\n",
      "Iteration 70, loss = 0.33765078\n",
      "Iteration 71, loss = 0.33590545\n",
      "Iteration 72, loss = 0.33433013\n",
      "Iteration 73, loss = 0.33262902\n",
      "Iteration 74, loss = 0.33112475\n",
      "Iteration 75, loss = 0.32927149\n",
      "Iteration 76, loss = 0.32757682\n",
      "Iteration 77, loss = 0.32608733\n",
      "Iteration 78, loss = 0.32462425\n",
      "Iteration 79, loss = 0.32300963\n",
      "Iteration 80, loss = 0.32188646\n",
      "Iteration 81, loss = 0.31995373\n",
      "Iteration 82, loss = 0.31884525\n",
      "Iteration 83, loss = 0.31774582\n",
      "Iteration 84, loss = 0.31612676\n",
      "Iteration 85, loss = 0.31424107\n",
      "Iteration 86, loss = 0.31337368\n",
      "Iteration 87, loss = 0.31168876\n",
      "Iteration 88, loss = 0.31022973\n",
      "Iteration 89, loss = 0.30861382\n",
      "Iteration 90, loss = 0.30708399\n",
      "Iteration 91, loss = 0.30636966\n",
      "Iteration 92, loss = 0.30449429\n",
      "Iteration 93, loss = 0.30303609\n",
      "Iteration 94, loss = 0.30223860\n",
      "Iteration 95, loss = 0.30185067\n",
      "Iteration 96, loss = 0.29958486\n",
      "Iteration 97, loss = 0.29841148\n",
      "Iteration 98, loss = 0.29720623\n",
      "Iteration 99, loss = 0.29605339\n",
      "Iteration 100, loss = 0.29493092\n",
      "Iteration 101, loss = 0.29394992\n",
      "Iteration 102, loss = 0.29253317\n",
      "Iteration 103, loss = 0.29160718\n",
      "Iteration 104, loss = 0.29070846\n",
      "Iteration 105, loss = 0.28962577\n",
      "Iteration 106, loss = 0.28868003\n",
      "Iteration 107, loss = 0.28750444\n",
      "Iteration 108, loss = 0.28718451\n",
      "Iteration 109, loss = 0.28543785\n",
      "Iteration 110, loss = 0.28455249\n",
      "Iteration 111, loss = 0.28382518\n",
      "Iteration 112, loss = 0.28256595\n",
      "Iteration 113, loss = 0.28166090\n",
      "Iteration 114, loss = 0.28076505\n",
      "Iteration 115, loss = 0.27990778\n",
      "Iteration 116, loss = 0.27896508\n",
      "Iteration 117, loss = 0.27822929\n",
      "Iteration 118, loss = 0.27704528\n",
      "Iteration 119, loss = 0.27710874\n",
      "Iteration 120, loss = 0.27555026\n",
      "Iteration 121, loss = 0.27431530\n",
      "Iteration 122, loss = 0.27406031\n",
      "Iteration 123, loss = 0.27305142\n",
      "Iteration 124, loss = 0.27228937\n",
      "Iteration 125, loss = 0.27157279\n",
      "Iteration 126, loss = 0.27060530\n",
      "Iteration 127, loss = 0.27007485\n",
      "Iteration 128, loss = 0.26907545\n",
      "Iteration 129, loss = 0.26809581\n",
      "Iteration 130, loss = 0.26727450\n",
      "Iteration 131, loss = 0.26638888\n",
      "Iteration 132, loss = 0.26591799\n",
      "Iteration 133, loss = 0.26494308\n",
      "Iteration 134, loss = 0.26415518\n",
      "Iteration 135, loss = 0.26350077\n",
      "Iteration 136, loss = 0.26262229\n",
      "Iteration 137, loss = 0.26189338\n",
      "Iteration 138, loss = 0.26108614\n",
      "Iteration 139, loss = 0.26039167\n",
      "Iteration 140, loss = 0.26005864\n",
      "Iteration 141, loss = 0.25900816\n",
      "Iteration 142, loss = 0.25803003\n",
      "Iteration 143, loss = 0.25735302\n",
      "Iteration 144, loss = 0.25703599\n",
      "Iteration 145, loss = 0.25579241\n",
      "Iteration 146, loss = 0.25528178\n",
      "Iteration 147, loss = 0.25468487\n",
      "Iteration 148, loss = 0.25386801\n",
      "Iteration 149, loss = 0.25328381\n",
      "Iteration 150, loss = 0.25289834\n",
      "Iteration 151, loss = 0.25190695\n",
      "Iteration 152, loss = 0.25133428\n",
      "Iteration 153, loss = 0.25066311\n",
      "Iteration 154, loss = 0.25011528\n",
      "Iteration 155, loss = 0.24917976\n",
      "Iteration 156, loss = 0.24834929\n",
      "Iteration 157, loss = 0.24802529\n",
      "Iteration 158, loss = 0.24722373\n",
      "Iteration 159, loss = 0.24656138\n",
      "Iteration 160, loss = 0.24600437\n",
      "Iteration 161, loss = 0.24535823\n",
      "Iteration 162, loss = 0.24472111\n",
      "Iteration 163, loss = 0.24437177\n",
      "Iteration 164, loss = 0.24363005\n",
      "Iteration 165, loss = 0.24309143\n",
      "Iteration 166, loss = 0.24283005\n",
      "Iteration 167, loss = 0.24294851\n",
      "Iteration 168, loss = 0.24145658\n",
      "Iteration 169, loss = 0.24089522\n",
      "Iteration 170, loss = 0.24059226\n",
      "Iteration 171, loss = 0.23967561\n",
      "Iteration 172, loss = 0.23944849\n",
      "Iteration 173, loss = 0.23867725\n",
      "Iteration 174, loss = 0.23877938\n",
      "Iteration 175, loss = 0.23784568\n",
      "Iteration 176, loss = 0.23719277\n",
      "Iteration 177, loss = 0.23656834\n",
      "Iteration 178, loss = 0.23653685\n",
      "Iteration 179, loss = 0.23582617\n",
      "Iteration 180, loss = 0.23524591\n",
      "Iteration 181, loss = 0.23478831\n",
      "Iteration 182, loss = 0.23428667\n",
      "Iteration 183, loss = 0.23386050\n",
      "Iteration 184, loss = 0.23330681\n",
      "Iteration 185, loss = 0.23285394\n",
      "Iteration 186, loss = 0.23238807\n",
      "Iteration 187, loss = 0.23185673\n",
      "Iteration 188, loss = 0.23165285\n",
      "Iteration 189, loss = 0.23150819\n",
      "Iteration 190, loss = 0.23094647\n",
      "Iteration 191, loss = 0.23040057\n",
      "Iteration 192, loss = 0.22961926\n",
      "Iteration 193, loss = 0.22951096\n",
      "Iteration 194, loss = 0.22917558\n",
      "Iteration 195, loss = 0.22873445\n",
      "Iteration 196, loss = 0.22813890\n",
      "Iteration 197, loss = 0.22778624\n",
      "Iteration 198, loss = 0.22753622\n",
      "Iteration 199, loss = 0.22694003\n",
      "Iteration 200, loss = 0.22669277\n",
      "Iteration 201, loss = 0.22642019\n",
      "Iteration 202, loss = 0.22603290\n",
      "Iteration 203, loss = 0.22560614\n",
      "Iteration 204, loss = 0.22574015\n",
      "Iteration 205, loss = 0.22553419\n",
      "Iteration 206, loss = 0.22498213\n",
      "Iteration 207, loss = 0.22437541\n",
      "Iteration 208, loss = 0.22380735\n",
      "Iteration 209, loss = 0.22382171\n",
      "Iteration 210, loss = 0.22307068\n",
      "Iteration 211, loss = 0.22277007\n",
      "Iteration 212, loss = 0.22256633\n",
      "Iteration 213, loss = 0.22232212\n",
      "Iteration 214, loss = 0.22191104\n",
      "Iteration 215, loss = 0.22188054\n",
      "Iteration 216, loss = 0.22110036\n",
      "Iteration 217, loss = 0.22114163\n",
      "Iteration 218, loss = 0.22058898\n",
      "Iteration 219, loss = 0.22073501\n",
      "Iteration 220, loss = 0.22002030\n",
      "Iteration 221, loss = 0.21966149\n",
      "Iteration 222, loss = 0.21989641\n",
      "Iteration 223, loss = 0.21945895\n",
      "Iteration 224, loss = 0.21889985\n",
      "Iteration 225, loss = 0.21876866\n",
      "Iteration 226, loss = 0.21824095\n",
      "Iteration 227, loss = 0.21793959\n",
      "Iteration 228, loss = 0.21777613\n",
      "Iteration 229, loss = 0.21750020\n",
      "Iteration 230, loss = 0.21738242\n",
      "Iteration 231, loss = 0.21681495\n",
      "Iteration 232, loss = 0.21701289\n",
      "Iteration 233, loss = 0.21629856\n",
      "Iteration 234, loss = 0.21555647\n",
      "Iteration 235, loss = 0.21538714\n",
      "Iteration 236, loss = 0.21519850\n",
      "Iteration 237, loss = 0.21482176\n",
      "Iteration 238, loss = 0.21504257\n",
      "Iteration 239, loss = 0.21466643\n",
      "Iteration 240, loss = 0.21394479\n",
      "Iteration 241, loss = 0.21365078\n",
      "Iteration 242, loss = 0.21337436\n",
      "Iteration 243, loss = 0.21355422\n",
      "Iteration 244, loss = 0.21285387\n",
      "Iteration 245, loss = 0.21328347\n",
      "Iteration 246, loss = 0.21283679\n",
      "Iteration 247, loss = 0.21230273\n",
      "Iteration 248, loss = 0.21180934\n",
      "Iteration 249, loss = 0.21188899\n",
      "Iteration 250, loss = 0.21120870\n",
      "Iteration 251, loss = 0.21077491\n",
      "Iteration 252, loss = 0.21059132\n",
      "Iteration 253, loss = 0.21058088\n",
      "Iteration 254, loss = 0.21019347\n",
      "Iteration 255, loss = 0.20979763\n",
      "Iteration 256, loss = 0.21018200\n",
      "Iteration 257, loss = 0.20929597\n",
      "Iteration 258, loss = 0.20939144\n",
      "Iteration 259, loss = 0.20895315\n",
      "Iteration 260, loss = 0.20865485\n",
      "Iteration 261, loss = 0.20860834\n",
      "Iteration 262, loss = 0.20804277\n",
      "Iteration 263, loss = 0.20787692\n",
      "Iteration 264, loss = 0.20713921\n",
      "Iteration 265, loss = 0.20673344\n",
      "Iteration 266, loss = 0.20647292\n",
      "Iteration 267, loss = 0.20599526\n",
      "Iteration 268, loss = 0.20500860\n",
      "Iteration 269, loss = 0.20523706\n",
      "Iteration 270, loss = 0.20421469\n",
      "Iteration 271, loss = 0.20464924\n",
      "Iteration 272, loss = 0.20351402\n",
      "Iteration 273, loss = 0.20355726\n",
      "Iteration 274, loss = 0.20386573\n",
      "Iteration 275, loss = 0.20277227\n",
      "Iteration 276, loss = 0.20210576\n",
      "Iteration 277, loss = 0.20218458\n",
      "Iteration 278, loss = 0.20147610\n",
      "Iteration 279, loss = 0.20238567\n",
      "Iteration 280, loss = 0.20107598\n",
      "Iteration 281, loss = 0.20081022\n",
      "Iteration 282, loss = 0.20067392\n",
      "Iteration 283, loss = 0.20072391\n",
      "Iteration 284, loss = 0.20005517\n",
      "Iteration 285, loss = 0.19996208\n",
      "Iteration 286, loss = 0.19932691\n",
      "Iteration 287, loss = 0.19930750\n",
      "Iteration 288, loss = 0.19863660\n",
      "Iteration 289, loss = 0.19895511\n",
      "Iteration 290, loss = 0.20004452\n",
      "Iteration 291, loss = 0.19773393\n",
      "Iteration 292, loss = 0.19857006\n",
      "Iteration 293, loss = 0.19814663\n",
      "Iteration 294, loss = 0.19713352\n",
      "Iteration 295, loss = 0.19692080\n",
      "Iteration 296, loss = 0.19675695\n",
      "Iteration 297, loss = 0.19724942\n",
      "Iteration 298, loss = 0.19683925\n",
      "Iteration 299, loss = 0.19634342\n",
      "Iteration 300, loss = 0.19594488\n",
      "Iteration 301, loss = 0.19582042\n",
      "Iteration 302, loss = 0.19564530\n",
      "Iteration 303, loss = 0.19540180\n",
      "Iteration 304, loss = 0.19501257\n",
      "Iteration 305, loss = 0.19491868\n",
      "Iteration 306, loss = 0.19478904\n",
      "Iteration 307, loss = 0.19447508\n",
      "Iteration 308, loss = 0.19417180\n",
      "Iteration 309, loss = 0.19417212\n",
      "Iteration 310, loss = 0.19468736\n",
      "Iteration 311, loss = 0.19419681\n",
      "Iteration 312, loss = 0.19352439\n",
      "Iteration 313, loss = 0.19424808\n",
      "Iteration 314, loss = 0.19326829\n",
      "Iteration 315, loss = 0.19350919\n",
      "Iteration 316, loss = 0.19293789\n",
      "Iteration 317, loss = 0.19328545\n",
      "Iteration 318, loss = 0.19309201\n",
      "Iteration 319, loss = 0.19245965\n",
      "Iteration 320, loss = 0.19229509\n",
      "Iteration 321, loss = 0.19222083\n",
      "Iteration 322, loss = 0.19222292\n",
      "Iteration 323, loss = 0.19216868\n",
      "Iteration 324, loss = 0.19206104\n",
      "Iteration 325, loss = 0.19173481\n",
      "Iteration 326, loss = 0.19163416\n",
      "Iteration 327, loss = 0.19164811\n",
      "Iteration 328, loss = 0.19240557\n",
      "Iteration 329, loss = 0.19192668\n",
      "Iteration 330, loss = 0.19112467\n",
      "Iteration 331, loss = 0.19082515\n",
      "Iteration 332, loss = 0.19132024\n",
      "Iteration 333, loss = 0.19057447\n",
      "Iteration 334, loss = 0.19058028\n",
      "Iteration 335, loss = 0.19040136\n",
      "Iteration 336, loss = 0.19006139\n",
      "Iteration 337, loss = 0.19005501\n",
      "Iteration 338, loss = 0.18999421\n",
      "Iteration 339, loss = 0.19007044\n",
      "Iteration 340, loss = 0.18973446\n",
      "Iteration 341, loss = 0.18942823\n",
      "Iteration 342, loss = 0.18935994\n",
      "Iteration 343, loss = 0.18928494\n",
      "Iteration 344, loss = 0.18926516\n",
      "Iteration 345, loss = 0.18951824\n",
      "Iteration 346, loss = 0.18916077\n",
      "Iteration 347, loss = 0.18910762\n",
      "Iteration 348, loss = 0.18892230\n",
      "Iteration 349, loss = 0.18854069\n",
      "Iteration 350, loss = 0.18845961\n",
      "Iteration 351, loss = 0.18898945\n",
      "Iteration 352, loss = 0.18851924\n",
      "Iteration 353, loss = 0.18816015\n",
      "Iteration 354, loss = 0.18799605\n",
      "Iteration 355, loss = 0.18798153\n",
      "Iteration 356, loss = 0.18776466\n",
      "Iteration 357, loss = 0.18829836\n",
      "Iteration 358, loss = 0.18780534\n",
      "Iteration 359, loss = 0.18751127\n",
      "Iteration 360, loss = 0.18769437\n",
      "Iteration 361, loss = 0.18745912\n",
      "Iteration 362, loss = 0.18772078\n",
      "Iteration 363, loss = 0.18728962\n",
      "Iteration 364, loss = 0.18665841\n",
      "Iteration 365, loss = 0.18692949\n",
      "Iteration 366, loss = 0.18719812\n",
      "Iteration 367, loss = 0.18671271\n",
      "Iteration 368, loss = 0.18644588\n",
      "Iteration 369, loss = 0.18674538\n",
      "Iteration 370, loss = 0.18656065\n",
      "Iteration 371, loss = 0.18609875\n",
      "Iteration 372, loss = 0.18627808\n",
      "Iteration 373, loss = 0.18563641\n",
      "Iteration 374, loss = 0.18607238\n",
      "Iteration 375, loss = 0.18567144\n",
      "Iteration 376, loss = 0.18580381\n",
      "Iteration 377, loss = 0.18558398\n",
      "Iteration 378, loss = 0.18538286\n",
      "Iteration 379, loss = 0.18562740\n",
      "Iteration 380, loss = 0.18535415\n",
      "Iteration 381, loss = 0.18528041\n",
      "Iteration 382, loss = 0.18553441\n",
      "Iteration 383, loss = 0.18514429\n",
      "Iteration 384, loss = 0.18504219\n",
      "Iteration 385, loss = 0.18503270\n",
      "Iteration 386, loss = 0.18510926\n",
      "Iteration 387, loss = 0.18437049\n",
      "Iteration 388, loss = 0.18438919\n",
      "Iteration 389, loss = 0.18460615\n",
      "Iteration 390, loss = 0.18441673\n",
      "Iteration 391, loss = 0.18442553\n",
      "Iteration 392, loss = 0.18475878\n",
      "Iteration 393, loss = 0.18479589\n",
      "Iteration 394, loss = 0.18375009\n",
      "Iteration 395, loss = 0.18392904\n",
      "Iteration 396, loss = 0.18383527\n",
      "Iteration 397, loss = 0.18366995\n",
      "Iteration 398, loss = 0.18340061\n",
      "Iteration 399, loss = 0.18392590\n",
      "Iteration 400, loss = 0.18351842\n",
      "Iteration 401, loss = 0.18319905\n",
      "Iteration 402, loss = 0.18339150\n",
      "Iteration 403, loss = 0.18350277\n",
      "Iteration 404, loss = 0.18360198\n",
      "Iteration 405, loss = 0.18302739\n",
      "Iteration 406, loss = 0.18291593\n",
      "Iteration 407, loss = 0.18283873\n",
      "Iteration 408, loss = 0.18269127\n",
      "Iteration 409, loss = 0.18254448\n",
      "Iteration 410, loss = 0.18264574\n",
      "Iteration 411, loss = 0.18214712\n",
      "Iteration 412, loss = 0.18276287\n",
      "Iteration 413, loss = 0.18194713\n",
      "Iteration 414, loss = 0.18268427\n",
      "Iteration 415, loss = 0.18220623\n",
      "Iteration 416, loss = 0.18220955\n",
      "Iteration 417, loss = 0.18188509\n",
      "Iteration 418, loss = 0.18183057\n",
      "Iteration 419, loss = 0.18166788\n",
      "Iteration 420, loss = 0.18185750\n",
      "Iteration 421, loss = 0.18184160\n",
      "Iteration 422, loss = 0.18124917\n",
      "Iteration 423, loss = 0.18140644\n",
      "Iteration 424, loss = 0.18135396\n",
      "Iteration 425, loss = 0.18095292\n",
      "Iteration 426, loss = 0.18133482\n",
      "Iteration 427, loss = 0.18094319\n",
      "Iteration 428, loss = 0.18137089\n",
      "Iteration 429, loss = 0.18154836\n",
      "Iteration 430, loss = 0.18088848\n",
      "Iteration 431, loss = 0.18103695\n",
      "Iteration 432, loss = 0.18126206\n",
      "Iteration 433, loss = 0.18040918\n",
      "Iteration 434, loss = 0.18025715\n",
      "Iteration 435, loss = 0.17971656\n",
      "Iteration 436, loss = 0.17880403\n",
      "Iteration 437, loss = 0.17852904\n",
      "Iteration 438, loss = 0.17795254\n",
      "Iteration 439, loss = 0.17779323\n",
      "Iteration 440, loss = 0.17803447\n",
      "Iteration 441, loss = 0.17787860\n",
      "Iteration 442, loss = 0.17787231\n",
      "Iteration 443, loss = 0.17782125\n",
      "Iteration 444, loss = 0.17756361\n",
      "Iteration 445, loss = 0.17776851\n",
      "Iteration 446, loss = 0.17712704\n",
      "Iteration 447, loss = 0.17792722\n",
      "Iteration 448, loss = 0.17772637\n",
      "Iteration 449, loss = 0.17785632\n",
      "Iteration 450, loss = 0.17784277\n",
      "Iteration 451, loss = 0.17737665\n",
      "Iteration 452, loss = 0.17691015\n",
      "Iteration 453, loss = 0.17709769\n",
      "Iteration 454, loss = 0.17712925\n",
      "Iteration 455, loss = 0.17712494\n",
      "Iteration 456, loss = 0.17723263\n",
      "Iteration 457, loss = 0.17757899\n",
      "Iteration 458, loss = 0.17689980\n",
      "Iteration 459, loss = 0.17728886\n",
      "Iteration 460, loss = 0.17681325\n",
      "Iteration 461, loss = 0.17667927\n",
      "Iteration 462, loss = 0.17633277\n",
      "Iteration 463, loss = 0.17725776\n",
      "Iteration 464, loss = 0.17699850\n",
      "Iteration 465, loss = 0.17647000\n",
      "Iteration 466, loss = 0.17666376\n",
      "Iteration 467, loss = 0.17612450\n",
      "Iteration 468, loss = 0.17631374\n",
      "Iteration 469, loss = 0.17614139\n",
      "Iteration 470, loss = 0.17642781\n",
      "Iteration 471, loss = 0.17604657\n",
      "Iteration 472, loss = 0.17611657\n",
      "Iteration 473, loss = 0.17657229\n",
      "Iteration 474, loss = 0.17629358\n",
      "Iteration 475, loss = 0.17665178\n",
      "Iteration 476, loss = 0.17587639\n",
      "Iteration 477, loss = 0.17614116\n",
      "Iteration 478, loss = 0.17625229\n",
      "Iteration 479, loss = 0.17592327\n",
      "Iteration 480, loss = 0.17599682\n",
      "Iteration 481, loss = 0.17690144\n",
      "Iteration 482, loss = 0.17554168\n",
      "Iteration 483, loss = 0.17610778\n",
      "Iteration 484, loss = 0.17625106\n",
      "Iteration 485, loss = 0.17564567\n",
      "Iteration 486, loss = 0.17568683\n",
      "Iteration 487, loss = 0.17580180\n",
      "Iteration 488, loss = 0.17553303\n",
      "Iteration 489, loss = 0.17557603\n",
      "Iteration 490, loss = 0.17548631\n",
      "Iteration 491, loss = 0.17536950\n",
      "Iteration 492, loss = 0.17546144\n",
      "Iteration 493, loss = 0.17568843\n",
      "Iteration 494, loss = 0.17565613\n",
      "Iteration 495, loss = 0.17519469\n",
      "Iteration 496, loss = 0.17537785\n",
      "Iteration 497, loss = 0.17531159\n",
      "Iteration 498, loss = 0.17525640\n",
      "Iteration 499, loss = 0.17542008\n",
      "Iteration 500, loss = 0.17506846\n",
      "Iteration 501, loss = 0.17582174\n",
      "Iteration 502, loss = 0.17488796\n",
      "Iteration 503, loss = 0.17475832\n",
      "Iteration 504, loss = 0.17514041\n",
      "Iteration 505, loss = 0.17485519\n",
      "Iteration 506, loss = 0.17479277\n",
      "Iteration 507, loss = 0.17494847\n",
      "Iteration 508, loss = 0.17472878\n",
      "Iteration 509, loss = 0.17478503\n",
      "Iteration 510, loss = 0.17460203\n",
      "Iteration 511, loss = 0.17480680\n",
      "Iteration 512, loss = 0.17491964\n",
      "Iteration 513, loss = 0.17488609\n",
      "Iteration 514, loss = 0.17467319\n",
      "Iteration 515, loss = 0.17479317\n",
      "Iteration 516, loss = 0.17443639\n",
      "Iteration 517, loss = 0.17592306\n",
      "Iteration 518, loss = 0.17553037\n",
      "Iteration 519, loss = 0.17397922\n",
      "Iteration 520, loss = 0.17411488\n",
      "Iteration 521, loss = 0.17416115\n",
      "Iteration 522, loss = 0.17432476\n",
      "Iteration 523, loss = 0.17418506\n",
      "Iteration 524, loss = 0.17488059\n",
      "Iteration 525, loss = 0.17348407\n",
      "Iteration 526, loss = 0.17331573\n",
      "Iteration 527, loss = 0.17328237\n",
      "Iteration 528, loss = 0.17410763\n",
      "Iteration 529, loss = 0.17404053\n",
      "Iteration 530, loss = 0.17451055\n",
      "Iteration 531, loss = 0.17327695\n",
      "Iteration 532, loss = 0.17324007\n",
      "Iteration 533, loss = 0.17278561\n",
      "Iteration 534, loss = 0.17318553\n",
      "Iteration 535, loss = 0.17299450\n",
      "Iteration 536, loss = 0.17282195\n",
      "Iteration 537, loss = 0.17272539\n",
      "Iteration 538, loss = 0.17282400\n",
      "Iteration 539, loss = 0.17288179\n",
      "Iteration 540, loss = 0.17287246\n",
      "Iteration 541, loss = 0.17271328\n",
      "Iteration 542, loss = 0.17246860\n",
      "Iteration 543, loss = 0.17241756\n",
      "Iteration 544, loss = 0.17251768\n",
      "Iteration 545, loss = 0.17235465\n",
      "Iteration 546, loss = 0.17255268\n",
      "Iteration 547, loss = 0.17240061\n",
      "Iteration 548, loss = 0.17213417\n",
      "Iteration 549, loss = 0.17228308\n",
      "Iteration 550, loss = 0.17252973\n",
      "Iteration 551, loss = 0.17211597\n",
      "Iteration 552, loss = 0.17289325\n",
      "Iteration 553, loss = 0.17222562\n",
      "Iteration 554, loss = 0.17310973\n",
      "Iteration 555, loss = 0.17365053\n",
      "Iteration 556, loss = 0.17270980\n",
      "Iteration 557, loss = 0.17248913\n",
      "Iteration 558, loss = 0.17238519\n",
      "Iteration 559, loss = 0.17257358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76899394\n",
      "Iteration 2, loss = 0.74338963\n",
      "Iteration 3, loss = 0.72168329\n",
      "Iteration 4, loss = 0.70456270\n",
      "Iteration 5, loss = 0.69094712\n",
      "Iteration 6, loss = 0.67781357\n",
      "Iteration 7, loss = 0.66401612\n",
      "Iteration 8, loss = 0.64832408\n",
      "Iteration 9, loss = 0.62983949\n",
      "Iteration 10, loss = 0.61043177\n",
      "Iteration 11, loss = 0.59061494\n",
      "Iteration 12, loss = 0.57198986\n",
      "Iteration 13, loss = 0.55565525\n",
      "Iteration 14, loss = 0.54129926\n",
      "Iteration 15, loss = 0.52887510\n",
      "Iteration 16, loss = 0.51859578\n",
      "Iteration 17, loss = 0.50941426\n",
      "Iteration 18, loss = 0.50116461\n",
      "Iteration 19, loss = 0.49373720\n",
      "Iteration 20, loss = 0.48720239\n",
      "Iteration 21, loss = 0.48099759\n",
      "Iteration 22, loss = 0.47540956\n",
      "Iteration 23, loss = 0.47012153\n",
      "Iteration 24, loss = 0.46518434\n",
      "Iteration 25, loss = 0.46053474\n",
      "Iteration 26, loss = 0.45620999\n",
      "Iteration 27, loss = 0.45183808\n",
      "Iteration 28, loss = 0.44766923\n",
      "Iteration 29, loss = 0.44353144\n",
      "Iteration 30, loss = 0.43940464\n",
      "Iteration 31, loss = 0.43560542\n",
      "Iteration 32, loss = 0.43203161\n",
      "Iteration 33, loss = 0.42834556\n",
      "Iteration 34, loss = 0.42477932\n",
      "Iteration 35, loss = 0.42162533\n",
      "Iteration 36, loss = 0.41813242\n",
      "Iteration 37, loss = 0.41491115\n",
      "Iteration 38, loss = 0.41176526\n",
      "Iteration 39, loss = 0.40844099\n",
      "Iteration 40, loss = 0.40558082\n",
      "Iteration 41, loss = 0.40241555\n",
      "Iteration 42, loss = 0.39957640\n",
      "Iteration 43, loss = 0.39655559\n",
      "Iteration 44, loss = 0.39376089\n",
      "Iteration 45, loss = 0.39083715\n",
      "Iteration 46, loss = 0.38831772\n",
      "Iteration 47, loss = 0.38554729\n",
      "Iteration 48, loss = 0.38316699\n",
      "Iteration 49, loss = 0.38050550\n",
      "Iteration 50, loss = 0.37819785\n",
      "Iteration 51, loss = 0.37594567\n",
      "Iteration 52, loss = 0.37347951\n",
      "Iteration 53, loss = 0.37138674\n",
      "Iteration 54, loss = 0.36888156\n",
      "Iteration 55, loss = 0.36685797\n",
      "Iteration 56, loss = 0.36479317\n",
      "Iteration 57, loss = 0.36242169\n",
      "Iteration 58, loss = 0.36047466\n",
      "Iteration 59, loss = 0.35846892\n",
      "Iteration 60, loss = 0.35613978\n",
      "Iteration 61, loss = 0.35445890\n",
      "Iteration 62, loss = 0.35284822\n",
      "Iteration 63, loss = 0.35074110\n",
      "Iteration 64, loss = 0.34866610\n",
      "Iteration 65, loss = 0.34646480\n",
      "Iteration 66, loss = 0.34465596\n",
      "Iteration 67, loss = 0.34285689\n",
      "Iteration 68, loss = 0.34125557\n",
      "Iteration 69, loss = 0.33922685\n",
      "Iteration 70, loss = 0.33745276\n",
      "Iteration 71, loss = 0.33584521\n",
      "Iteration 72, loss = 0.33416963\n",
      "Iteration 73, loss = 0.33248289\n",
      "Iteration 74, loss = 0.33096670\n",
      "Iteration 75, loss = 0.32913490\n",
      "Iteration 76, loss = 0.32794127\n",
      "Iteration 77, loss = 0.32623812\n",
      "Iteration 78, loss = 0.32461143\n",
      "Iteration 79, loss = 0.32301690\n",
      "Iteration 80, loss = 0.32156151\n",
      "Iteration 81, loss = 0.32020525\n",
      "Iteration 82, loss = 0.31893404\n",
      "Iteration 83, loss = 0.31719020\n",
      "Iteration 84, loss = 0.31575801\n",
      "Iteration 85, loss = 0.31420114\n",
      "Iteration 86, loss = 0.31299465\n",
      "Iteration 87, loss = 0.31165513\n",
      "Iteration 88, loss = 0.31039416\n",
      "Iteration 89, loss = 0.30925479\n",
      "Iteration 90, loss = 0.30780079\n",
      "Iteration 91, loss = 0.30676895\n",
      "Iteration 92, loss = 0.30522337\n",
      "Iteration 93, loss = 0.30384261\n",
      "Iteration 94, loss = 0.30262176\n",
      "Iteration 95, loss = 0.30175268\n",
      "Iteration 96, loss = 0.30038659\n",
      "Iteration 97, loss = 0.29926855\n",
      "Iteration 98, loss = 0.29833887\n",
      "Iteration 99, loss = 0.29733954\n",
      "Iteration 100, loss = 0.29562003\n",
      "Iteration 101, loss = 0.29463970\n",
      "Iteration 102, loss = 0.29346875\n",
      "Iteration 103, loss = 0.29269567\n",
      "Iteration 104, loss = 0.29136009\n",
      "Iteration 105, loss = 0.29052157\n",
      "Iteration 106, loss = 0.28901819\n",
      "Iteration 107, loss = 0.28828929\n",
      "Iteration 108, loss = 0.28718276\n",
      "Iteration 109, loss = 0.28644541\n",
      "Iteration 110, loss = 0.28540404\n",
      "Iteration 111, loss = 0.28415592\n",
      "Iteration 112, loss = 0.28307741\n",
      "Iteration 113, loss = 0.28222317\n",
      "Iteration 114, loss = 0.28156044\n",
      "Iteration 115, loss = 0.28048876\n",
      "Iteration 116, loss = 0.27960769\n",
      "Iteration 117, loss = 0.27852610\n",
      "Iteration 118, loss = 0.27741969\n",
      "Iteration 119, loss = 0.27670826\n",
      "Iteration 120, loss = 0.27567110\n",
      "Iteration 121, loss = 0.27537433\n",
      "Iteration 122, loss = 0.27398974\n",
      "Iteration 123, loss = 0.27326066\n",
      "Iteration 124, loss = 0.27216048\n",
      "Iteration 125, loss = 0.27157183\n",
      "Iteration 126, loss = 0.27058889\n",
      "Iteration 127, loss = 0.26988381\n",
      "Iteration 128, loss = 0.26923670\n",
      "Iteration 129, loss = 0.26803185\n",
      "Iteration 130, loss = 0.26757427\n",
      "Iteration 131, loss = 0.26679664\n",
      "Iteration 132, loss = 0.26584184\n",
      "Iteration 133, loss = 0.26521123\n",
      "Iteration 134, loss = 0.26468222\n",
      "Iteration 135, loss = 0.26373268\n",
      "Iteration 136, loss = 0.26333452\n",
      "Iteration 137, loss = 0.26256931\n",
      "Iteration 138, loss = 0.26179863\n",
      "Iteration 139, loss = 0.26091609\n",
      "Iteration 140, loss = 0.26050206\n",
      "Iteration 141, loss = 0.25980705\n",
      "Iteration 142, loss = 0.25899576\n",
      "Iteration 143, loss = 0.25816732\n",
      "Iteration 144, loss = 0.25797609\n",
      "Iteration 145, loss = 0.25715428\n",
      "Iteration 146, loss = 0.25612808\n",
      "Iteration 147, loss = 0.25588141\n",
      "Iteration 148, loss = 0.25506128\n",
      "Iteration 149, loss = 0.25491521\n",
      "Iteration 150, loss = 0.25380424\n",
      "Iteration 151, loss = 0.25353025\n",
      "Iteration 152, loss = 0.25270883\n",
      "Iteration 153, loss = 0.25204673\n",
      "Iteration 154, loss = 0.25185479\n",
      "Iteration 155, loss = 0.25080489\n",
      "Iteration 156, loss = 0.25036633\n",
      "Iteration 157, loss = 0.24967656\n",
      "Iteration 158, loss = 0.24921696\n",
      "Iteration 159, loss = 0.24861296\n",
      "Iteration 160, loss = 0.24807060\n",
      "Iteration 161, loss = 0.24769189\n",
      "Iteration 162, loss = 0.24738564\n",
      "Iteration 163, loss = 0.24642714\n",
      "Iteration 164, loss = 0.24591371\n",
      "Iteration 165, loss = 0.24546178\n",
      "Iteration 166, loss = 0.24492176\n",
      "Iteration 167, loss = 0.24446067\n",
      "Iteration 168, loss = 0.24399524\n",
      "Iteration 169, loss = 0.24335772\n",
      "Iteration 170, loss = 0.24257697\n",
      "Iteration 171, loss = 0.24197306\n",
      "Iteration 172, loss = 0.24150594\n",
      "Iteration 173, loss = 0.24169073\n",
      "Iteration 174, loss = 0.24066477\n",
      "Iteration 175, loss = 0.24076917\n",
      "Iteration 176, loss = 0.24021004\n",
      "Iteration 177, loss = 0.23981202\n",
      "Iteration 178, loss = 0.23887885\n",
      "Iteration 179, loss = 0.23826712\n",
      "Iteration 180, loss = 0.23778381\n",
      "Iteration 181, loss = 0.23749538\n",
      "Iteration 182, loss = 0.23736837\n",
      "Iteration 183, loss = 0.23638293\n",
      "Iteration 184, loss = 0.23631959\n",
      "Iteration 185, loss = 0.23595425\n",
      "Iteration 186, loss = 0.23582873\n",
      "Iteration 187, loss = 0.23508949\n",
      "Iteration 188, loss = 0.23494281\n",
      "Iteration 189, loss = 0.23405499\n",
      "Iteration 190, loss = 0.23380686\n",
      "Iteration 191, loss = 0.23366568\n",
      "Iteration 192, loss = 0.23326486\n",
      "Iteration 193, loss = 0.23299315\n",
      "Iteration 194, loss = 0.23235564\n",
      "Iteration 195, loss = 0.23207329\n",
      "Iteration 196, loss = 0.23155842\n",
      "Iteration 197, loss = 0.23121643\n",
      "Iteration 198, loss = 0.23080287\n",
      "Iteration 199, loss = 0.23071290\n",
      "Iteration 200, loss = 0.22975839\n",
      "Iteration 201, loss = 0.22966960\n",
      "Iteration 202, loss = 0.22943102\n",
      "Iteration 203, loss = 0.22906794\n",
      "Iteration 204, loss = 0.22856140\n",
      "Iteration 205, loss = 0.22833204\n",
      "Iteration 206, loss = 0.22749975\n",
      "Iteration 207, loss = 0.22717702\n",
      "Iteration 208, loss = 0.22694017\n",
      "Iteration 209, loss = 0.22651738\n",
      "Iteration 210, loss = 0.22634858\n",
      "Iteration 211, loss = 0.22591398\n",
      "Iteration 212, loss = 0.22535446\n",
      "Iteration 213, loss = 0.22578582\n",
      "Iteration 214, loss = 0.22484185\n",
      "Iteration 215, loss = 0.22456758\n",
      "Iteration 216, loss = 0.22400262\n",
      "Iteration 217, loss = 0.22348738\n",
      "Iteration 218, loss = 0.22340820\n",
      "Iteration 219, loss = 0.22353286\n",
      "Iteration 220, loss = 0.22362573\n",
      "Iteration 221, loss = 0.22206736\n",
      "Iteration 222, loss = 0.22267429\n",
      "Iteration 223, loss = 0.22204976\n",
      "Iteration 224, loss = 0.22157166\n",
      "Iteration 225, loss = 0.22150324\n",
      "Iteration 226, loss = 0.22090621\n",
      "Iteration 227, loss = 0.22096086\n",
      "Iteration 228, loss = 0.22032777\n",
      "Iteration 229, loss = 0.22009414\n",
      "Iteration 230, loss = 0.21994122\n",
      "Iteration 231, loss = 0.21968864\n",
      "Iteration 232, loss = 0.21944168\n",
      "Iteration 233, loss = 0.21941518\n",
      "Iteration 234, loss = 0.21866478\n",
      "Iteration 235, loss = 0.21874757\n",
      "Iteration 236, loss = 0.21854774\n",
      "Iteration 237, loss = 0.21819983\n",
      "Iteration 238, loss = 0.21773165\n",
      "Iteration 239, loss = 0.21751885\n",
      "Iteration 240, loss = 0.21729395\n",
      "Iteration 241, loss = 0.21701658\n",
      "Iteration 242, loss = 0.21680321\n",
      "Iteration 243, loss = 0.21644759\n",
      "Iteration 244, loss = 0.21624267\n",
      "Iteration 245, loss = 0.21606032\n",
      "Iteration 246, loss = 0.21591032\n",
      "Iteration 247, loss = 0.21543501\n",
      "Iteration 248, loss = 0.21525530\n",
      "Iteration 249, loss = 0.21389990\n",
      "Iteration 250, loss = 0.21352398\n",
      "Iteration 251, loss = 0.21288390\n",
      "Iteration 252, loss = 0.21231496\n",
      "Iteration 253, loss = 0.21205068\n",
      "Iteration 254, loss = 0.21171445\n",
      "Iteration 255, loss = 0.21123806\n",
      "Iteration 256, loss = 0.21128648\n",
      "Iteration 257, loss = 0.21153071\n",
      "Iteration 258, loss = 0.21078320\n",
      "Iteration 259, loss = 0.21040616\n",
      "Iteration 260, loss = 0.21076548\n",
      "Iteration 261, loss = 0.20971873\n",
      "Iteration 262, loss = 0.20989359\n",
      "Iteration 263, loss = 0.20934189\n",
      "Iteration 264, loss = 0.20918641\n",
      "Iteration 265, loss = 0.20917317\n",
      "Iteration 266, loss = 0.20894815\n",
      "Iteration 267, loss = 0.20866129\n",
      "Iteration 268, loss = 0.20834922\n",
      "Iteration 269, loss = 0.20810101\n",
      "Iteration 270, loss = 0.20782353\n",
      "Iteration 271, loss = 0.20795317\n",
      "Iteration 272, loss = 0.20766954\n",
      "Iteration 273, loss = 0.20823287\n",
      "Iteration 274, loss = 0.20685907\n",
      "Iteration 275, loss = 0.20669050\n",
      "Iteration 276, loss = 0.20662660\n",
      "Iteration 277, loss = 0.20722101\n",
      "Iteration 278, loss = 0.20673301\n",
      "Iteration 279, loss = 0.20632402\n",
      "Iteration 280, loss = 0.20620139\n",
      "Iteration 281, loss = 0.20532541\n",
      "Iteration 282, loss = 0.20573770\n",
      "Iteration 283, loss = 0.20499789\n",
      "Iteration 284, loss = 0.20573664\n",
      "Iteration 285, loss = 0.20442527\n",
      "Iteration 286, loss = 0.20470609\n",
      "Iteration 287, loss = 0.20431445\n",
      "Iteration 288, loss = 0.20437848\n",
      "Iteration 289, loss = 0.20422956\n",
      "Iteration 290, loss = 0.20483661\n",
      "Iteration 291, loss = 0.20360351\n",
      "Iteration 292, loss = 0.20347344\n",
      "Iteration 293, loss = 0.20269004\n",
      "Iteration 294, loss = 0.20243278\n",
      "Iteration 295, loss = 0.20215309\n",
      "Iteration 296, loss = 0.20205051\n",
      "Iteration 297, loss = 0.20222931\n",
      "Iteration 298, loss = 0.20167824\n",
      "Iteration 299, loss = 0.20215916\n",
      "Iteration 300, loss = 0.20287895\n",
      "Iteration 301, loss = 0.20075084\n",
      "Iteration 302, loss = 0.20131804\n",
      "Iteration 303, loss = 0.20048272\n",
      "Iteration 304, loss = 0.20038604\n",
      "Iteration 305, loss = 0.20010779\n",
      "Iteration 306, loss = 0.19988049\n",
      "Iteration 307, loss = 0.19981078\n",
      "Iteration 308, loss = 0.19976488\n",
      "Iteration 309, loss = 0.19948618\n",
      "Iteration 310, loss = 0.19905018\n",
      "Iteration 311, loss = 0.19869214\n",
      "Iteration 312, loss = 0.19849747\n",
      "Iteration 313, loss = 0.19865900\n",
      "Iteration 314, loss = 0.19807367\n",
      "Iteration 315, loss = 0.19808008\n",
      "Iteration 316, loss = 0.19855595\n",
      "Iteration 317, loss = 0.19776862\n",
      "Iteration 318, loss = 0.19744072\n",
      "Iteration 319, loss = 0.19742849\n",
      "Iteration 320, loss = 0.19698709\n",
      "Iteration 321, loss = 0.19675067\n",
      "Iteration 322, loss = 0.19664335\n",
      "Iteration 323, loss = 0.19642100\n",
      "Iteration 324, loss = 0.19619354\n",
      "Iteration 325, loss = 0.19568133\n",
      "Iteration 326, loss = 0.19584574\n",
      "Iteration 327, loss = 0.19579540\n",
      "Iteration 328, loss = 0.19511895\n",
      "Iteration 329, loss = 0.19513803\n",
      "Iteration 330, loss = 0.19503055\n",
      "Iteration 331, loss = 0.19475330\n",
      "Iteration 332, loss = 0.19510942\n",
      "Iteration 333, loss = 0.19433767\n",
      "Iteration 334, loss = 0.19469502\n",
      "Iteration 335, loss = 0.19410854\n",
      "Iteration 336, loss = 0.19364242\n",
      "Iteration 337, loss = 0.19472838\n",
      "Iteration 338, loss = 0.19304874\n",
      "Iteration 339, loss = 0.19341883\n",
      "Iteration 340, loss = 0.19295155\n",
      "Iteration 341, loss = 0.19405573\n",
      "Iteration 342, loss = 0.19264136\n",
      "Iteration 343, loss = 0.19198246\n",
      "Iteration 344, loss = 0.19196214\n",
      "Iteration 345, loss = 0.19180459\n",
      "Iteration 346, loss = 0.19153925\n",
      "Iteration 347, loss = 0.19101181\n",
      "Iteration 348, loss = 0.19092001\n",
      "Iteration 349, loss = 0.19080051\n",
      "Iteration 350, loss = 0.19020218\n",
      "Iteration 351, loss = 0.18994457\n",
      "Iteration 352, loss = 0.18998099\n",
      "Iteration 353, loss = 0.19009612\n",
      "Iteration 354, loss = 0.18944382\n",
      "Iteration 355, loss = 0.18932746\n",
      "Iteration 356, loss = 0.18957581\n",
      "Iteration 357, loss = 0.18960602\n",
      "Iteration 358, loss = 0.18890354\n",
      "Iteration 359, loss = 0.18861684\n",
      "Iteration 360, loss = 0.18893387\n",
      "Iteration 361, loss = 0.18851970\n",
      "Iteration 362, loss = 0.18827991\n",
      "Iteration 363, loss = 0.18845274\n",
      "Iteration 364, loss = 0.18788965\n",
      "Iteration 365, loss = 0.18832093\n",
      "Iteration 366, loss = 0.18836777\n",
      "Iteration 367, loss = 0.18776258\n",
      "Iteration 368, loss = 0.18739207\n",
      "Iteration 369, loss = 0.18721560\n",
      "Iteration 370, loss = 0.18713458\n",
      "Iteration 371, loss = 0.18728725\n",
      "Iteration 372, loss = 0.18769874\n",
      "Iteration 373, loss = 0.18631301\n",
      "Iteration 374, loss = 0.18706611\n",
      "Iteration 375, loss = 0.18606396\n",
      "Iteration 376, loss = 0.18623060\n",
      "Iteration 377, loss = 0.18600594\n",
      "Iteration 378, loss = 0.18569059\n",
      "Iteration 379, loss = 0.18561165\n",
      "Iteration 380, loss = 0.18539234\n",
      "Iteration 381, loss = 0.18535359\n",
      "Iteration 382, loss = 0.18521675\n",
      "Iteration 383, loss = 0.18493339\n",
      "Iteration 384, loss = 0.18533101\n",
      "Iteration 385, loss = 0.18509938\n",
      "Iteration 386, loss = 0.18504447\n",
      "Iteration 387, loss = 0.18434374\n",
      "Iteration 388, loss = 0.18418792\n",
      "Iteration 389, loss = 0.18418528\n",
      "Iteration 390, loss = 0.18445516\n",
      "Iteration 391, loss = 0.18382967\n",
      "Iteration 392, loss = 0.18385330\n",
      "Iteration 393, loss = 0.18376675\n",
      "Iteration 394, loss = 0.18336542\n",
      "Iteration 395, loss = 0.18353618\n",
      "Iteration 396, loss = 0.18362314\n",
      "Iteration 397, loss = 0.18330491\n",
      "Iteration 398, loss = 0.18340621\n",
      "Iteration 399, loss = 0.18262510\n",
      "Iteration 400, loss = 0.18273564\n",
      "Iteration 401, loss = 0.18304576\n",
      "Iteration 402, loss = 0.18243743\n",
      "Iteration 403, loss = 0.18227972\n",
      "Iteration 404, loss = 0.18298848\n",
      "Iteration 405, loss = 0.18182057\n",
      "Iteration 406, loss = 0.18218822\n",
      "Iteration 407, loss = 0.18248906\n",
      "Iteration 408, loss = 0.18152569\n",
      "Iteration 409, loss = 0.18192850\n",
      "Iteration 410, loss = 0.18132777\n",
      "Iteration 411, loss = 0.18123858\n",
      "Iteration 412, loss = 0.18107572\n",
      "Iteration 413, loss = 0.18123705\n",
      "Iteration 414, loss = 0.18072169\n",
      "Iteration 415, loss = 0.18091326\n",
      "Iteration 416, loss = 0.18110838\n",
      "Iteration 417, loss = 0.18076891\n",
      "Iteration 418, loss = 0.18015736\n",
      "Iteration 419, loss = 0.18059774\n",
      "Iteration 420, loss = 0.18099661\n",
      "Iteration 421, loss = 0.18049105\n",
      "Iteration 422, loss = 0.18027727\n",
      "Iteration 423, loss = 0.17970731\n",
      "Iteration 424, loss = 0.17982262\n",
      "Iteration 425, loss = 0.18001003\n",
      "Iteration 426, loss = 0.18090556\n",
      "Iteration 427, loss = 0.17967321\n",
      "Iteration 428, loss = 0.17986784\n",
      "Iteration 429, loss = 0.17912474\n",
      "Iteration 430, loss = 0.17931728\n",
      "Iteration 431, loss = 0.17983533\n",
      "Iteration 432, loss = 0.17948682\n",
      "Iteration 433, loss = 0.17924324\n",
      "Iteration 434, loss = 0.17863799\n",
      "Iteration 435, loss = 0.17922839\n",
      "Iteration 436, loss = 0.17885652\n",
      "Iteration 437, loss = 0.17820166\n",
      "Iteration 438, loss = 0.17824610\n",
      "Iteration 439, loss = 0.17858760\n",
      "Iteration 440, loss = 0.17837890\n",
      "Iteration 441, loss = 0.17861338\n",
      "Iteration 442, loss = 0.17823595\n",
      "Iteration 443, loss = 0.17831243\n",
      "Iteration 444, loss = 0.17831784\n",
      "Iteration 445, loss = 0.17779245\n",
      "Iteration 446, loss = 0.17838534\n",
      "Iteration 447, loss = 0.17815417\n",
      "Iteration 448, loss = 0.17822253\n",
      "Iteration 449, loss = 0.17750103\n",
      "Iteration 450, loss = 0.17820340\n",
      "Iteration 451, loss = 0.17844243\n",
      "Iteration 452, loss = 0.17847523\n",
      "Iteration 453, loss = 0.17736945\n",
      "Iteration 454, loss = 0.17734101\n",
      "Iteration 455, loss = 0.17702758\n",
      "Iteration 456, loss = 0.17778009\n",
      "Iteration 457, loss = 0.17732561\n",
      "Iteration 458, loss = 0.17715734\n",
      "Iteration 459, loss = 0.17663244\n",
      "Iteration 460, loss = 0.17687338\n",
      "Iteration 461, loss = 0.17652324\n",
      "Iteration 462, loss = 0.17641273\n",
      "Iteration 463, loss = 0.17641489\n",
      "Iteration 464, loss = 0.17644015\n",
      "Iteration 465, loss = 0.17638988\n",
      "Iteration 466, loss = 0.17619917\n",
      "Iteration 467, loss = 0.17622946\n",
      "Iteration 468, loss = 0.17667495\n",
      "Iteration 469, loss = 0.17620208\n",
      "Iteration 470, loss = 0.17595295\n",
      "Iteration 471, loss = 0.17608255\n",
      "Iteration 472, loss = 0.17703106\n",
      "Iteration 473, loss = 0.17589979\n",
      "Iteration 474, loss = 0.17687933\n",
      "Iteration 475, loss = 0.17621201\n",
      "Iteration 476, loss = 0.17637459\n",
      "Iteration 477, loss = 0.17568699\n",
      "Iteration 478, loss = 0.17530599\n",
      "Iteration 479, loss = 0.17584808\n",
      "Iteration 480, loss = 0.17523006\n",
      "Iteration 481, loss = 0.17521809\n",
      "Iteration 482, loss = 0.17518316\n",
      "Iteration 483, loss = 0.17501030\n",
      "Iteration 484, loss = 0.17551395\n",
      "Iteration 485, loss = 0.17482788\n",
      "Iteration 486, loss = 0.17507251\n",
      "Iteration 487, loss = 0.17512834\n",
      "Iteration 488, loss = 0.17520160\n",
      "Iteration 489, loss = 0.17535963\n",
      "Iteration 490, loss = 0.17537403\n",
      "Iteration 491, loss = 0.17423519\n",
      "Iteration 492, loss = 0.17490751\n",
      "Iteration 493, loss = 0.17436911\n",
      "Iteration 494, loss = 0.17431924\n",
      "Iteration 495, loss = 0.17395981\n",
      "Iteration 496, loss = 0.17416665\n",
      "Iteration 497, loss = 0.17403441\n",
      "Iteration 498, loss = 0.17410555\n",
      "Iteration 499, loss = 0.17476864\n",
      "Iteration 500, loss = 0.17424749\n",
      "Iteration 501, loss = 0.17393227\n",
      "Iteration 502, loss = 0.17403216\n",
      "Iteration 503, loss = 0.17358870\n",
      "Iteration 504, loss = 0.17365770\n",
      "Iteration 505, loss = 0.17359665\n",
      "Iteration 506, loss = 0.17392608\n",
      "Iteration 507, loss = 0.17483454\n",
      "Iteration 508, loss = 0.17312170\n",
      "Iteration 509, loss = 0.17368572\n",
      "Iteration 510, loss = 0.17403645\n",
      "Iteration 511, loss = 0.17326437\n",
      "Iteration 512, loss = 0.17320886\n",
      "Iteration 513, loss = 0.17330889\n",
      "Iteration 514, loss = 0.17318129\n",
      "Iteration 515, loss = 0.17282254\n",
      "Iteration 516, loss = 0.17262081\n",
      "Iteration 517, loss = 0.17274767\n",
      "Iteration 518, loss = 0.17255501\n",
      "Iteration 519, loss = 0.17247573\n",
      "Iteration 520, loss = 0.17393778\n",
      "Iteration 521, loss = 0.17264989\n",
      "Iteration 522, loss = 0.17196921\n",
      "Iteration 523, loss = 0.17230438\n",
      "Iteration 524, loss = 0.17252656\n",
      "Iteration 525, loss = 0.17238462\n",
      "Iteration 526, loss = 0.17198765\n",
      "Iteration 527, loss = 0.17178712\n",
      "Iteration 528, loss = 0.17216742\n",
      "Iteration 529, loss = 0.17213274\n",
      "Iteration 530, loss = 0.17183241\n",
      "Iteration 531, loss = 0.17232349\n",
      "Iteration 532, loss = 0.17155673\n",
      "Iteration 533, loss = 0.17250023\n",
      "Iteration 534, loss = 0.17246268\n",
      "Iteration 535, loss = 0.17217402\n",
      "Iteration 536, loss = 0.17197311\n",
      "Iteration 537, loss = 0.17123231\n",
      "Iteration 538, loss = 0.17149129\n",
      "Iteration 539, loss = 0.17169420\n",
      "Iteration 540, loss = 0.17108826\n",
      "Iteration 541, loss = 0.17092459\n",
      "Iteration 542, loss = 0.17160950\n",
      "Iteration 543, loss = 0.17102129\n",
      "Iteration 544, loss = 0.17108052\n",
      "Iteration 545, loss = 0.17101114\n",
      "Iteration 546, loss = 0.17100252\n",
      "Iteration 547, loss = 0.17082571\n",
      "Iteration 548, loss = 0.17079541\n",
      "Iteration 549, loss = 0.17067656\n",
      "Iteration 550, loss = 0.17082878\n",
      "Iteration 551, loss = 0.17135549\n",
      "Iteration 552, loss = 0.17081483\n",
      "Iteration 553, loss = 0.17105770\n",
      "Iteration 554, loss = 0.17088286\n",
      "Iteration 555, loss = 0.17035361\n",
      "Iteration 556, loss = 0.17106742\n",
      "Iteration 557, loss = 0.17018869\n",
      "Iteration 558, loss = 0.17038041\n",
      "Iteration 559, loss = 0.17052283\n",
      "Iteration 560, loss = 0.17035162\n",
      "Iteration 561, loss = 0.17025767\n",
      "Iteration 562, loss = 0.17034733\n",
      "Iteration 563, loss = 0.16991046\n",
      "Iteration 564, loss = 0.17008379\n",
      "Iteration 565, loss = 0.16994728\n",
      "Iteration 566, loss = 0.16960951\n",
      "Iteration 567, loss = 0.17115271\n",
      "Iteration 568, loss = 0.17063142\n",
      "Iteration 569, loss = 0.17090699\n",
      "Iteration 570, loss = 0.17012767\n",
      "Iteration 571, loss = 0.16953563\n",
      "Iteration 572, loss = 0.16972201\n",
      "Iteration 573, loss = 0.16950373\n",
      "Iteration 574, loss = 0.16991429\n",
      "Iteration 575, loss = 0.16967731\n",
      "Iteration 576, loss = 0.16996447\n",
      "Iteration 577, loss = 0.16940151\n",
      "Iteration 578, loss = 0.16946109\n",
      "Iteration 579, loss = 0.16963017\n",
      "Iteration 580, loss = 0.16917353\n",
      "Iteration 581, loss = 0.17009623\n",
      "Iteration 582, loss = 0.16887407\n",
      "Iteration 583, loss = 0.16939748\n",
      "Iteration 584, loss = 0.16907133\n",
      "Iteration 585, loss = 0.16917515\n",
      "Iteration 586, loss = 0.16925837\n",
      "Iteration 587, loss = 0.16871667\n",
      "Iteration 588, loss = 0.16921138\n",
      "Iteration 589, loss = 0.16879304\n",
      "Iteration 590, loss = 0.16919320\n",
      "Iteration 591, loss = 0.16865461\n",
      "Iteration 592, loss = 0.16891703\n",
      "Iteration 593, loss = 0.16878276\n",
      "Iteration 594, loss = 0.16848211\n",
      "Iteration 595, loss = 0.16914651\n",
      "Iteration 596, loss = 0.16962636\n",
      "Iteration 597, loss = 0.16887153\n",
      "Iteration 598, loss = 0.16890527\n",
      "Iteration 599, loss = 0.16835544\n",
      "Iteration 600, loss = 0.16860502\n",
      "Iteration 601, loss = 0.16952735\n",
      "Iteration 602, loss = 0.16817873\n",
      "Iteration 603, loss = 0.16851381\n",
      "Iteration 604, loss = 0.16866696\n",
      "Iteration 605, loss = 0.16854341\n",
      "Iteration 606, loss = 0.16847466\n",
      "Iteration 607, loss = 0.16832804\n",
      "Iteration 608, loss = 0.16827696\n",
      "Iteration 609, loss = 0.16829861\n",
      "Iteration 610, loss = 0.16871003\n",
      "Iteration 611, loss = 0.16817289\n",
      "Iteration 612, loss = 0.16864322\n",
      "Iteration 613, loss = 0.16790078\n",
      "Iteration 614, loss = 0.16824097\n",
      "Iteration 615, loss = 0.16869569\n",
      "Iteration 616, loss = 0.16799004\n",
      "Iteration 617, loss = 0.16765668\n",
      "Iteration 618, loss = 0.16786869\n",
      "Iteration 619, loss = 0.16798963\n",
      "Iteration 620, loss = 0.16777907\n",
      "Iteration 621, loss = 0.16758509\n",
      "Iteration 622, loss = 0.16798580\n",
      "Iteration 623, loss = 0.16771468\n",
      "Iteration 624, loss = 0.16963351\n",
      "Iteration 625, loss = 0.16846129\n",
      "Iteration 626, loss = 0.16817932\n",
      "Iteration 627, loss = 0.16778025\n",
      "Iteration 628, loss = 0.16858102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77150446\n",
      "Iteration 2, loss = 0.74500488\n",
      "Iteration 3, loss = 0.72295418\n",
      "Iteration 4, loss = 0.70565621\n",
      "Iteration 5, loss = 0.69137251\n",
      "Iteration 6, loss = 0.67875352\n",
      "Iteration 7, loss = 0.66573145\n",
      "Iteration 8, loss = 0.65162214\n",
      "Iteration 9, loss = 0.63461595\n",
      "Iteration 10, loss = 0.61589997\n",
      "Iteration 11, loss = 0.59699723\n",
      "Iteration 12, loss = 0.57890151\n",
      "Iteration 13, loss = 0.56284977\n",
      "Iteration 14, loss = 0.54893270\n",
      "Iteration 15, loss = 0.53683266\n",
      "Iteration 16, loss = 0.52621894\n",
      "Iteration 17, loss = 0.51691966\n",
      "Iteration 18, loss = 0.50816454\n",
      "Iteration 19, loss = 0.50032340\n",
      "Iteration 20, loss = 0.49333634\n",
      "Iteration 21, loss = 0.48666513\n",
      "Iteration 22, loss = 0.48075014\n",
      "Iteration 23, loss = 0.47548178\n",
      "Iteration 24, loss = 0.47030274\n",
      "Iteration 25, loss = 0.46552180\n",
      "Iteration 26, loss = 0.46080943\n",
      "Iteration 27, loss = 0.45637524\n",
      "Iteration 28, loss = 0.45203974\n",
      "Iteration 29, loss = 0.44785427\n",
      "Iteration 30, loss = 0.44372620\n",
      "Iteration 31, loss = 0.43967933\n",
      "Iteration 32, loss = 0.43598544\n",
      "Iteration 33, loss = 0.43238025\n",
      "Iteration 34, loss = 0.42879936\n",
      "Iteration 35, loss = 0.42522393\n",
      "Iteration 36, loss = 0.42203925\n",
      "Iteration 37, loss = 0.41854418\n",
      "Iteration 38, loss = 0.41526101\n",
      "Iteration 39, loss = 0.41210607\n",
      "Iteration 40, loss = 0.40904202\n",
      "Iteration 41, loss = 0.40606744\n",
      "Iteration 42, loss = 0.40323082\n",
      "Iteration 43, loss = 0.40039205\n",
      "Iteration 44, loss = 0.39765438\n",
      "Iteration 45, loss = 0.39491710\n",
      "Iteration 46, loss = 0.39224593\n",
      "Iteration 47, loss = 0.38959805\n",
      "Iteration 48, loss = 0.38727763\n",
      "Iteration 49, loss = 0.38481381\n",
      "Iteration 50, loss = 0.38209019\n",
      "Iteration 51, loss = 0.37977087\n",
      "Iteration 52, loss = 0.37737904\n",
      "Iteration 53, loss = 0.37495964\n",
      "Iteration 54, loss = 0.37267823\n",
      "Iteration 55, loss = 0.37013783\n",
      "Iteration 56, loss = 0.36818278\n",
      "Iteration 57, loss = 0.36612101\n",
      "Iteration 58, loss = 0.36392295\n",
      "Iteration 59, loss = 0.36195923\n",
      "Iteration 60, loss = 0.36001598\n",
      "Iteration 61, loss = 0.35803135\n",
      "Iteration 62, loss = 0.35619214\n",
      "Iteration 63, loss = 0.35423402\n",
      "Iteration 64, loss = 0.35237889\n",
      "Iteration 65, loss = 0.35043075\n",
      "Iteration 66, loss = 0.34870790\n",
      "Iteration 67, loss = 0.34684086\n",
      "Iteration 68, loss = 0.34504117\n",
      "Iteration 69, loss = 0.34309109\n",
      "Iteration 70, loss = 0.34133476\n",
      "Iteration 71, loss = 0.33953519\n",
      "Iteration 72, loss = 0.33771267\n",
      "Iteration 73, loss = 0.33600100\n",
      "Iteration 74, loss = 0.33427667\n",
      "Iteration 75, loss = 0.33242029\n",
      "Iteration 76, loss = 0.33124276\n",
      "Iteration 77, loss = 0.32910856\n",
      "Iteration 78, loss = 0.32771538\n",
      "Iteration 79, loss = 0.32594256\n",
      "Iteration 80, loss = 0.32438115\n",
      "Iteration 81, loss = 0.32285180\n",
      "Iteration 82, loss = 0.32133331\n",
      "Iteration 83, loss = 0.31988519\n",
      "Iteration 84, loss = 0.31846257\n",
      "Iteration 85, loss = 0.31691323\n",
      "Iteration 86, loss = 0.31554388\n",
      "Iteration 87, loss = 0.31416117\n",
      "Iteration 88, loss = 0.31297580\n",
      "Iteration 89, loss = 0.31124952\n",
      "Iteration 90, loss = 0.30990071\n",
      "Iteration 91, loss = 0.30873777\n",
      "Iteration 92, loss = 0.30771063\n",
      "Iteration 93, loss = 0.30623599\n",
      "Iteration 94, loss = 0.30463184\n",
      "Iteration 95, loss = 0.30351235\n",
      "Iteration 96, loss = 0.30231945\n",
      "Iteration 97, loss = 0.30107801\n",
      "Iteration 98, loss = 0.30002927\n",
      "Iteration 99, loss = 0.29890838\n",
      "Iteration 100, loss = 0.29760917\n",
      "Iteration 101, loss = 0.29676807\n",
      "Iteration 102, loss = 0.29532183\n",
      "Iteration 103, loss = 0.29447006\n",
      "Iteration 104, loss = 0.29335176\n",
      "Iteration 105, loss = 0.29237342\n",
      "Iteration 106, loss = 0.29109743\n",
      "Iteration 107, loss = 0.29011909\n",
      "Iteration 108, loss = 0.28916917\n",
      "Iteration 109, loss = 0.28830960\n",
      "Iteration 110, loss = 0.28740057\n",
      "Iteration 111, loss = 0.28609833\n",
      "Iteration 112, loss = 0.28518839\n",
      "Iteration 113, loss = 0.28421351\n",
      "Iteration 114, loss = 0.28349194\n",
      "Iteration 115, loss = 0.28253946\n",
      "Iteration 116, loss = 0.28132603\n",
      "Iteration 117, loss = 0.28051163\n",
      "Iteration 118, loss = 0.27953429\n",
      "Iteration 119, loss = 0.27923051\n",
      "Iteration 120, loss = 0.27813521\n",
      "Iteration 121, loss = 0.27687192\n",
      "Iteration 122, loss = 0.27636991\n",
      "Iteration 123, loss = 0.27578649\n",
      "Iteration 124, loss = 0.27482757\n",
      "Iteration 125, loss = 0.27360856\n",
      "Iteration 126, loss = 0.27343619\n",
      "Iteration 127, loss = 0.27228506\n",
      "Iteration 128, loss = 0.27191863\n",
      "Iteration 129, loss = 0.27072179\n",
      "Iteration 130, loss = 0.26996670\n",
      "Iteration 131, loss = 0.26909331\n",
      "Iteration 132, loss = 0.26830398\n",
      "Iteration 133, loss = 0.26787726\n",
      "Iteration 134, loss = 0.26699052\n",
      "Iteration 135, loss = 0.26635991\n",
      "Iteration 136, loss = 0.26570452\n",
      "Iteration 137, loss = 0.26525691\n",
      "Iteration 138, loss = 0.26451604\n",
      "Iteration 139, loss = 0.26387953\n",
      "Iteration 140, loss = 0.26334409\n",
      "Iteration 141, loss = 0.26259639\n",
      "Iteration 142, loss = 0.26175716\n",
      "Iteration 143, loss = 0.26149596\n",
      "Iteration 144, loss = 0.26060475\n",
      "Iteration 145, loss = 0.26065693\n",
      "Iteration 146, loss = 0.25927702\n",
      "Iteration 147, loss = 0.25900921\n",
      "Iteration 148, loss = 0.25826697\n",
      "Iteration 149, loss = 0.25768222\n",
      "Iteration 150, loss = 0.25728672\n",
      "Iteration 151, loss = 0.25700520\n",
      "Iteration 152, loss = 0.25623274\n",
      "Iteration 153, loss = 0.25538614\n",
      "Iteration 154, loss = 0.25535431\n",
      "Iteration 155, loss = 0.25437405\n",
      "Iteration 156, loss = 0.25384449\n",
      "Iteration 157, loss = 0.25369566\n",
      "Iteration 158, loss = 0.25289419\n",
      "Iteration 159, loss = 0.25226224\n",
      "Iteration 160, loss = 0.25186003\n",
      "Iteration 161, loss = 0.25141980\n",
      "Iteration 162, loss = 0.25117782\n",
      "Iteration 163, loss = 0.25140987\n",
      "Iteration 164, loss = 0.24994646\n",
      "Iteration 165, loss = 0.25012068\n",
      "Iteration 166, loss = 0.24914561\n",
      "Iteration 167, loss = 0.24868029\n",
      "Iteration 168, loss = 0.24804617\n",
      "Iteration 169, loss = 0.24768806\n",
      "Iteration 170, loss = 0.24758289\n",
      "Iteration 171, loss = 0.24667344\n",
      "Iteration 172, loss = 0.24638399\n",
      "Iteration 173, loss = 0.24571581\n",
      "Iteration 174, loss = 0.24535509\n",
      "Iteration 175, loss = 0.24454195\n",
      "Iteration 176, loss = 0.24402879\n",
      "Iteration 177, loss = 0.24361013\n",
      "Iteration 178, loss = 0.24326059\n",
      "Iteration 179, loss = 0.24289199\n",
      "Iteration 180, loss = 0.24243499\n",
      "Iteration 181, loss = 0.24144164\n",
      "Iteration 182, loss = 0.24139428\n",
      "Iteration 183, loss = 0.24076330\n",
      "Iteration 184, loss = 0.24006815\n",
      "Iteration 185, loss = 0.23928474\n",
      "Iteration 186, loss = 0.23899939\n",
      "Iteration 187, loss = 0.23816301\n",
      "Iteration 188, loss = 0.23764016\n",
      "Iteration 189, loss = 0.23703296\n",
      "Iteration 190, loss = 0.23662808\n",
      "Iteration 191, loss = 0.23622182\n",
      "Iteration 192, loss = 0.23618261\n",
      "Iteration 193, loss = 0.23522761\n",
      "Iteration 194, loss = 0.23494344\n",
      "Iteration 195, loss = 0.23385554\n",
      "Iteration 196, loss = 0.23364035\n",
      "Iteration 197, loss = 0.23304044\n",
      "Iteration 198, loss = 0.23232835\n",
      "Iteration 199, loss = 0.23219420\n",
      "Iteration 200, loss = 0.23146292\n",
      "Iteration 201, loss = 0.23108987\n",
      "Iteration 202, loss = 0.23061567\n",
      "Iteration 203, loss = 0.22998589\n",
      "Iteration 204, loss = 0.22974806\n",
      "Iteration 205, loss = 0.22984736\n",
      "Iteration 206, loss = 0.22848801\n",
      "Iteration 207, loss = 0.22803898\n",
      "Iteration 208, loss = 0.22770277\n",
      "Iteration 209, loss = 0.22748986\n",
      "Iteration 210, loss = 0.22667461\n",
      "Iteration 211, loss = 0.22681895\n",
      "Iteration 212, loss = 0.22607594\n",
      "Iteration 213, loss = 0.22584055\n",
      "Iteration 214, loss = 0.22515005\n",
      "Iteration 215, loss = 0.22478169\n",
      "Iteration 216, loss = 0.22410899\n",
      "Iteration 217, loss = 0.22379868\n",
      "Iteration 218, loss = 0.22349998\n",
      "Iteration 219, loss = 0.22310883\n",
      "Iteration 220, loss = 0.22308200\n",
      "Iteration 221, loss = 0.22267118\n",
      "Iteration 222, loss = 0.22168169\n",
      "Iteration 223, loss = 0.22158380\n",
      "Iteration 224, loss = 0.22122357\n",
      "Iteration 225, loss = 0.22108182\n",
      "Iteration 226, loss = 0.22058364\n",
      "Iteration 227, loss = 0.22038567\n",
      "Iteration 228, loss = 0.21962546\n",
      "Iteration 229, loss = 0.21979560\n",
      "Iteration 230, loss = 0.21932764\n",
      "Iteration 231, loss = 0.21887038\n",
      "Iteration 232, loss = 0.21842906\n",
      "Iteration 233, loss = 0.21831334\n",
      "Iteration 234, loss = 0.21774556\n",
      "Iteration 235, loss = 0.21766693\n",
      "Iteration 236, loss = 0.21686506\n",
      "Iteration 237, loss = 0.21675884\n",
      "Iteration 238, loss = 0.21622629\n",
      "Iteration 239, loss = 0.21634530\n",
      "Iteration 240, loss = 0.21607114\n",
      "Iteration 241, loss = 0.21550620\n",
      "Iteration 242, loss = 0.21550269\n",
      "Iteration 243, loss = 0.21519648\n",
      "Iteration 244, loss = 0.21517909\n",
      "Iteration 245, loss = 0.21491047\n",
      "Iteration 246, loss = 0.21460791\n",
      "Iteration 247, loss = 0.21453649\n",
      "Iteration 248, loss = 0.21381941\n",
      "Iteration 249, loss = 0.21332488\n",
      "Iteration 250, loss = 0.21320016\n",
      "Iteration 251, loss = 0.21298110\n",
      "Iteration 252, loss = 0.21293262\n",
      "Iteration 253, loss = 0.21251889\n",
      "Iteration 254, loss = 0.21257932\n",
      "Iteration 255, loss = 0.21209794\n",
      "Iteration 256, loss = 0.21206364\n",
      "Iteration 257, loss = 0.21157465\n",
      "Iteration 258, loss = 0.21146292\n",
      "Iteration 259, loss = 0.21133136\n",
      "Iteration 260, loss = 0.21172058\n",
      "Iteration 261, loss = 0.21062621\n",
      "Iteration 262, loss = 0.21076216\n",
      "Iteration 263, loss = 0.21058379\n",
      "Iteration 264, loss = 0.21040783\n",
      "Iteration 265, loss = 0.21026759\n",
      "Iteration 266, loss = 0.20961707\n",
      "Iteration 267, loss = 0.20950865\n",
      "Iteration 268, loss = 0.20938790\n",
      "Iteration 269, loss = 0.20927049\n",
      "Iteration 270, loss = 0.20918201\n",
      "Iteration 271, loss = 0.20900327\n",
      "Iteration 272, loss = 0.20850611\n",
      "Iteration 273, loss = 0.20853458\n",
      "Iteration 274, loss = 0.20838718\n",
      "Iteration 275, loss = 0.20765453\n",
      "Iteration 276, loss = 0.20780987\n",
      "Iteration 277, loss = 0.20781406\n",
      "Iteration 278, loss = 0.20746463\n",
      "Iteration 279, loss = 0.20729858\n",
      "Iteration 280, loss = 0.20693878\n",
      "Iteration 281, loss = 0.20694344\n",
      "Iteration 282, loss = 0.20689748\n",
      "Iteration 283, loss = 0.20676214\n",
      "Iteration 284, loss = 0.20594211\n",
      "Iteration 285, loss = 0.20610180\n",
      "Iteration 286, loss = 0.20582718\n",
      "Iteration 287, loss = 0.20548557\n",
      "Iteration 288, loss = 0.20554614\n",
      "Iteration 289, loss = 0.20506383\n",
      "Iteration 290, loss = 0.20522214\n",
      "Iteration 291, loss = 0.20515904\n",
      "Iteration 292, loss = 0.20445582\n",
      "Iteration 293, loss = 0.20450891\n",
      "Iteration 294, loss = 0.20430275\n",
      "Iteration 295, loss = 0.20403599\n",
      "Iteration 296, loss = 0.20394984\n",
      "Iteration 297, loss = 0.20371701\n",
      "Iteration 298, loss = 0.20428240\n",
      "Iteration 299, loss = 0.20483154\n",
      "Iteration 300, loss = 0.20304265\n",
      "Iteration 301, loss = 0.20340835\n",
      "Iteration 302, loss = 0.20277923\n",
      "Iteration 303, loss = 0.20298891\n",
      "Iteration 304, loss = 0.20342574\n",
      "Iteration 305, loss = 0.20306794\n",
      "Iteration 306, loss = 0.20303903\n",
      "Iteration 307, loss = 0.20274022\n",
      "Iteration 308, loss = 0.20181308\n",
      "Iteration 309, loss = 0.20177667\n",
      "Iteration 310, loss = 0.20198378\n",
      "Iteration 311, loss = 0.20167244\n",
      "Iteration 312, loss = 0.20173502\n",
      "Iteration 313, loss = 0.20153401\n",
      "Iteration 314, loss = 0.20136481\n",
      "Iteration 315, loss = 0.20108998\n",
      "Iteration 316, loss = 0.20116987\n",
      "Iteration 317, loss = 0.20080244\n",
      "Iteration 318, loss = 0.20051194\n",
      "Iteration 319, loss = 0.20059486\n",
      "Iteration 320, loss = 0.20010872\n",
      "Iteration 321, loss = 0.19992872\n",
      "Iteration 322, loss = 0.20017707\n",
      "Iteration 323, loss = 0.19972587\n",
      "Iteration 324, loss = 0.19974145\n",
      "Iteration 325, loss = 0.19988436\n",
      "Iteration 326, loss = 0.19950530\n",
      "Iteration 327, loss = 0.19948546\n",
      "Iteration 328, loss = 0.19955451\n",
      "Iteration 329, loss = 0.19924291\n",
      "Iteration 330, loss = 0.19927594\n",
      "Iteration 331, loss = 0.19894523\n",
      "Iteration 332, loss = 0.19916669\n",
      "Iteration 333, loss = 0.19908859\n",
      "Iteration 334, loss = 0.19861658\n",
      "Iteration 335, loss = 0.19886465\n",
      "Iteration 336, loss = 0.19824063\n",
      "Iteration 337, loss = 0.19819456\n",
      "Iteration 338, loss = 0.19818157\n",
      "Iteration 339, loss = 0.19811764\n",
      "Iteration 340, loss = 0.19808802\n",
      "Iteration 341, loss = 0.19812559\n",
      "Iteration 342, loss = 0.19760368\n",
      "Iteration 343, loss = 0.19782289\n",
      "Iteration 344, loss = 0.19747498\n",
      "Iteration 345, loss = 0.19785011\n",
      "Iteration 346, loss = 0.19765197\n",
      "Iteration 347, loss = 0.19736715\n",
      "Iteration 348, loss = 0.19722231\n",
      "Iteration 349, loss = 0.19740562\n",
      "Iteration 350, loss = 0.19652420\n",
      "Iteration 351, loss = 0.19738491\n",
      "Iteration 352, loss = 0.19693617\n",
      "Iteration 353, loss = 0.19681414\n",
      "Iteration 354, loss = 0.19712729\n",
      "Iteration 355, loss = 0.19680814\n",
      "Iteration 356, loss = 0.19679835\n",
      "Iteration 357, loss = 0.19649463\n",
      "Iteration 358, loss = 0.19685484\n",
      "Iteration 359, loss = 0.19682375\n",
      "Iteration 360, loss = 0.19612976\n",
      "Iteration 361, loss = 0.19666318\n",
      "Iteration 362, loss = 0.19629301\n",
      "Iteration 363, loss = 0.19618720\n",
      "Iteration 364, loss = 0.19647510\n",
      "Iteration 365, loss = 0.19578470\n",
      "Iteration 366, loss = 0.19577353\n",
      "Iteration 367, loss = 0.19565873\n",
      "Iteration 368, loss = 0.19561458\n",
      "Iteration 369, loss = 0.19558351\n",
      "Iteration 370, loss = 0.19543458\n",
      "Iteration 371, loss = 0.19564470\n",
      "Iteration 372, loss = 0.19532227\n",
      "Iteration 373, loss = 0.19507915\n",
      "Iteration 374, loss = 0.19537341\n",
      "Iteration 375, loss = 0.19555658\n",
      "Iteration 376, loss = 0.19493625\n",
      "Iteration 377, loss = 0.19486063\n",
      "Iteration 378, loss = 0.19557061\n",
      "Iteration 379, loss = 0.19503522\n",
      "Iteration 380, loss = 0.19455551\n",
      "Iteration 381, loss = 0.19458927\n",
      "Iteration 382, loss = 0.19462228\n",
      "Iteration 383, loss = 0.19462738\n",
      "Iteration 384, loss = 0.19462610\n",
      "Iteration 385, loss = 0.19428325\n",
      "Iteration 386, loss = 0.19446752\n",
      "Iteration 387, loss = 0.19409388\n",
      "Iteration 388, loss = 0.19441477\n",
      "Iteration 389, loss = 0.19407617\n",
      "Iteration 390, loss = 0.19399182\n",
      "Iteration 391, loss = 0.19410729\n",
      "Iteration 392, loss = 0.19392534\n",
      "Iteration 393, loss = 0.19421306\n",
      "Iteration 394, loss = 0.19367471\n",
      "Iteration 395, loss = 0.19373976\n",
      "Iteration 396, loss = 0.19427420\n",
      "Iteration 397, loss = 0.19351473\n",
      "Iteration 398, loss = 0.19365681\n",
      "Iteration 399, loss = 0.19352942\n",
      "Iteration 400, loss = 0.19353498\n",
      "Iteration 401, loss = 0.19326174\n",
      "Iteration 402, loss = 0.19344401\n",
      "Iteration 403, loss = 0.19339121\n",
      "Iteration 404, loss = 0.19330156\n",
      "Iteration 405, loss = 0.19332154\n",
      "Iteration 406, loss = 0.19327435\n",
      "Iteration 407, loss = 0.19345492\n",
      "Iteration 408, loss = 0.19319585\n",
      "Iteration 409, loss = 0.19316204\n",
      "Iteration 410, loss = 0.19324011\n",
      "Iteration 411, loss = 0.19305915\n",
      "Iteration 412, loss = 0.19275207\n",
      "Iteration 413, loss = 0.19267541\n",
      "Iteration 414, loss = 0.19252675\n",
      "Iteration 415, loss = 0.19278532\n",
      "Iteration 416, loss = 0.19247096\n",
      "Iteration 417, loss = 0.19298449\n",
      "Iteration 418, loss = 0.19250873\n",
      "Iteration 419, loss = 0.19256205\n",
      "Iteration 420, loss = 0.19364642\n",
      "Iteration 421, loss = 0.19242261\n",
      "Iteration 422, loss = 0.19232126\n",
      "Iteration 423, loss = 0.19191097\n",
      "Iteration 424, loss = 0.19401402\n",
      "Iteration 425, loss = 0.19246205\n",
      "Iteration 426, loss = 0.19211582\n",
      "Iteration 427, loss = 0.19214980\n",
      "Iteration 428, loss = 0.19219378\n",
      "Iteration 429, loss = 0.19183091\n",
      "Iteration 430, loss = 0.19183182\n",
      "Iteration 431, loss = 0.19208578\n",
      "Iteration 432, loss = 0.19217230\n",
      "Iteration 433, loss = 0.19181494\n",
      "Iteration 434, loss = 0.19190784\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76861620\n",
      "Iteration 2, loss = 0.74275797\n",
      "Iteration 3, loss = 0.72106504\n",
      "Iteration 4, loss = 0.70358528\n",
      "Iteration 5, loss = 0.68891104\n",
      "Iteration 6, loss = 0.67611127\n",
      "Iteration 7, loss = 0.66305734\n",
      "Iteration 8, loss = 0.64742236\n",
      "Iteration 9, loss = 0.62938610\n",
      "Iteration 10, loss = 0.60936412\n",
      "Iteration 11, loss = 0.58920686\n",
      "Iteration 12, loss = 0.57096068\n",
      "Iteration 13, loss = 0.55457300\n",
      "Iteration 14, loss = 0.54119615\n",
      "Iteration 15, loss = 0.52888455\n",
      "Iteration 16, loss = 0.51869468\n",
      "Iteration 17, loss = 0.50906024\n",
      "Iteration 18, loss = 0.50059544\n",
      "Iteration 19, loss = 0.49300337\n",
      "Iteration 20, loss = 0.48617862\n",
      "Iteration 21, loss = 0.48007301\n",
      "Iteration 22, loss = 0.47391026\n",
      "Iteration 23, loss = 0.46858188\n",
      "Iteration 24, loss = 0.46323372\n",
      "Iteration 25, loss = 0.45839764\n",
      "Iteration 26, loss = 0.45363303\n",
      "Iteration 27, loss = 0.44921252\n",
      "Iteration 28, loss = 0.44512711\n",
      "Iteration 29, loss = 0.44093319\n",
      "Iteration 30, loss = 0.43689094\n",
      "Iteration 31, loss = 0.43301334\n",
      "Iteration 32, loss = 0.42932015\n",
      "Iteration 33, loss = 0.42555357\n",
      "Iteration 34, loss = 0.42180774\n",
      "Iteration 35, loss = 0.41810037\n",
      "Iteration 36, loss = 0.41450498\n",
      "Iteration 37, loss = 0.41113750\n",
      "Iteration 38, loss = 0.40799785\n",
      "Iteration 39, loss = 0.40459536\n",
      "Iteration 40, loss = 0.40148933\n",
      "Iteration 41, loss = 0.39864251\n",
      "Iteration 42, loss = 0.39577227\n",
      "Iteration 43, loss = 0.39273906\n",
      "Iteration 44, loss = 0.38993936\n",
      "Iteration 45, loss = 0.38718224\n",
      "Iteration 46, loss = 0.38462747\n",
      "Iteration 47, loss = 0.38241259\n",
      "Iteration 48, loss = 0.37942969\n",
      "Iteration 49, loss = 0.37687141\n",
      "Iteration 50, loss = 0.37445128\n",
      "Iteration 51, loss = 0.37207932\n",
      "Iteration 52, loss = 0.36953603\n",
      "Iteration 53, loss = 0.36722822\n",
      "Iteration 54, loss = 0.36481025\n",
      "Iteration 55, loss = 0.36305462\n",
      "Iteration 56, loss = 0.36047700\n",
      "Iteration 57, loss = 0.35852564\n",
      "Iteration 58, loss = 0.35667328\n",
      "Iteration 59, loss = 0.35406300\n",
      "Iteration 60, loss = 0.35222829\n",
      "Iteration 61, loss = 0.35018470\n",
      "Iteration 62, loss = 0.34850955\n",
      "Iteration 63, loss = 0.34677536\n",
      "Iteration 64, loss = 0.34457948\n",
      "Iteration 65, loss = 0.34286639\n",
      "Iteration 66, loss = 0.34113959\n",
      "Iteration 67, loss = 0.33895758\n",
      "Iteration 68, loss = 0.33744071\n",
      "Iteration 69, loss = 0.33561926\n",
      "Iteration 70, loss = 0.33417662\n",
      "Iteration 71, loss = 0.33219533\n",
      "Iteration 72, loss = 0.33080516\n",
      "Iteration 73, loss = 0.32895704\n",
      "Iteration 74, loss = 0.32739103\n",
      "Iteration 75, loss = 0.32566302\n",
      "Iteration 76, loss = 0.32408199\n",
      "Iteration 77, loss = 0.32241709\n",
      "Iteration 78, loss = 0.32046217\n",
      "Iteration 79, loss = 0.31894936\n",
      "Iteration 80, loss = 0.31756022\n",
      "Iteration 81, loss = 0.31566096\n",
      "Iteration 82, loss = 0.31433164\n",
      "Iteration 83, loss = 0.31265117\n",
      "Iteration 84, loss = 0.31106210\n",
      "Iteration 85, loss = 0.30991505\n",
      "Iteration 86, loss = 0.30852914\n",
      "Iteration 87, loss = 0.30681887\n",
      "Iteration 88, loss = 0.30596779\n",
      "Iteration 89, loss = 0.30397386\n",
      "Iteration 90, loss = 0.30286809\n",
      "Iteration 91, loss = 0.30141812\n",
      "Iteration 92, loss = 0.29997362\n",
      "Iteration 93, loss = 0.29874404\n",
      "Iteration 94, loss = 0.29719289\n",
      "Iteration 95, loss = 0.29604402\n",
      "Iteration 96, loss = 0.29489854\n",
      "Iteration 97, loss = 0.29355356\n",
      "Iteration 98, loss = 0.29216685\n",
      "Iteration 99, loss = 0.29113955\n",
      "Iteration 100, loss = 0.28978853\n",
      "Iteration 101, loss = 0.28848481\n",
      "Iteration 102, loss = 0.28772379\n",
      "Iteration 103, loss = 0.28605182\n",
      "Iteration 104, loss = 0.28493872\n",
      "Iteration 105, loss = 0.28472153\n",
      "Iteration 106, loss = 0.28267909\n",
      "Iteration 107, loss = 0.28177423\n",
      "Iteration 108, loss = 0.28074989\n",
      "Iteration 109, loss = 0.27948449\n",
      "Iteration 110, loss = 0.27823490\n",
      "Iteration 111, loss = 0.27741026\n",
      "Iteration 112, loss = 0.27628901\n",
      "Iteration 113, loss = 0.27505085\n",
      "Iteration 114, loss = 0.27405770\n",
      "Iteration 115, loss = 0.27270772\n",
      "Iteration 116, loss = 0.27183519\n",
      "Iteration 117, loss = 0.27047924\n",
      "Iteration 118, loss = 0.26942326\n",
      "Iteration 119, loss = 0.26835553\n",
      "Iteration 120, loss = 0.26747611\n",
      "Iteration 121, loss = 0.26658239\n",
      "Iteration 122, loss = 0.26538904\n",
      "Iteration 123, loss = 0.26510332\n",
      "Iteration 124, loss = 0.26357720\n",
      "Iteration 125, loss = 0.26296567\n",
      "Iteration 126, loss = 0.26190141\n",
      "Iteration 127, loss = 0.26047238\n",
      "Iteration 128, loss = 0.25988479\n",
      "Iteration 129, loss = 0.25877679\n",
      "Iteration 130, loss = 0.25793742\n",
      "Iteration 131, loss = 0.25712587\n",
      "Iteration 132, loss = 0.25601181\n",
      "Iteration 133, loss = 0.25526698\n",
      "Iteration 134, loss = 0.25468892\n",
      "Iteration 135, loss = 0.25360516\n",
      "Iteration 136, loss = 0.25285545\n",
      "Iteration 137, loss = 0.25200495\n",
      "Iteration 138, loss = 0.25134799\n",
      "Iteration 139, loss = 0.25039046\n",
      "Iteration 140, loss = 0.24966014\n",
      "Iteration 141, loss = 0.24876533\n",
      "Iteration 142, loss = 0.24823164\n",
      "Iteration 143, loss = 0.24717782\n",
      "Iteration 144, loss = 0.24670760\n",
      "Iteration 145, loss = 0.24605236\n",
      "Iteration 146, loss = 0.24502442\n",
      "Iteration 147, loss = 0.24432003\n",
      "Iteration 148, loss = 0.24295112\n",
      "Iteration 149, loss = 0.24210635\n",
      "Iteration 150, loss = 0.24068231\n",
      "Iteration 151, loss = 0.24004002\n",
      "Iteration 152, loss = 0.23927244\n",
      "Iteration 153, loss = 0.23813046\n",
      "Iteration 154, loss = 0.23760516\n",
      "Iteration 155, loss = 0.23687220\n",
      "Iteration 156, loss = 0.23634493\n",
      "Iteration 157, loss = 0.23568308\n",
      "Iteration 158, loss = 0.23543455\n",
      "Iteration 159, loss = 0.23408200\n",
      "Iteration 160, loss = 0.23334757\n",
      "Iteration 161, loss = 0.23255614\n",
      "Iteration 162, loss = 0.23226839\n",
      "Iteration 163, loss = 0.23139695\n",
      "Iteration 164, loss = 0.23113281\n",
      "Iteration 165, loss = 0.23030630\n",
      "Iteration 166, loss = 0.22970313\n",
      "Iteration 167, loss = 0.23031610\n",
      "Iteration 168, loss = 0.22876743\n",
      "Iteration 169, loss = 0.22830701\n",
      "Iteration 170, loss = 0.22780291\n",
      "Iteration 171, loss = 0.22702842\n",
      "Iteration 172, loss = 0.22608426\n",
      "Iteration 173, loss = 0.22626154\n",
      "Iteration 174, loss = 0.22555493\n",
      "Iteration 175, loss = 0.22492599\n",
      "Iteration 176, loss = 0.22451330\n",
      "Iteration 177, loss = 0.22400300\n",
      "Iteration 178, loss = 0.22335180\n",
      "Iteration 179, loss = 0.22339043\n",
      "Iteration 180, loss = 0.22236201\n",
      "Iteration 181, loss = 0.22191186\n",
      "Iteration 182, loss = 0.22159365\n",
      "Iteration 183, loss = 0.22076262\n",
      "Iteration 184, loss = 0.22074486\n",
      "Iteration 185, loss = 0.22042801\n",
      "Iteration 186, loss = 0.21949737\n",
      "Iteration 187, loss = 0.21892019\n",
      "Iteration 188, loss = 0.21880037\n",
      "Iteration 189, loss = 0.21820441\n",
      "Iteration 190, loss = 0.21787725\n",
      "Iteration 191, loss = 0.21741934\n",
      "Iteration 192, loss = 0.21717181\n",
      "Iteration 193, loss = 0.21635167\n",
      "Iteration 194, loss = 0.21617295\n",
      "Iteration 195, loss = 0.21578960\n",
      "Iteration 196, loss = 0.21538488\n",
      "Iteration 197, loss = 0.21535488\n",
      "Iteration 198, loss = 0.21476428\n",
      "Iteration 199, loss = 0.21459581\n",
      "Iteration 200, loss = 0.21412558\n",
      "Iteration 201, loss = 0.21352257\n",
      "Iteration 202, loss = 0.21314363\n",
      "Iteration 203, loss = 0.21281603\n",
      "Iteration 204, loss = 0.21229446\n",
      "Iteration 205, loss = 0.21197872\n",
      "Iteration 206, loss = 0.21191769\n",
      "Iteration 207, loss = 0.21183187\n",
      "Iteration 208, loss = 0.21132857\n",
      "Iteration 209, loss = 0.21165989\n",
      "Iteration 210, loss = 0.21075090\n",
      "Iteration 211, loss = 0.21019977\n",
      "Iteration 212, loss = 0.20968104\n",
      "Iteration 213, loss = 0.20966545\n",
      "Iteration 214, loss = 0.20953551\n",
      "Iteration 215, loss = 0.20912152\n",
      "Iteration 216, loss = 0.20895387\n",
      "Iteration 217, loss = 0.20861847\n",
      "Iteration 218, loss = 0.20802730\n",
      "Iteration 219, loss = 0.20798125\n",
      "Iteration 220, loss = 0.20785726\n",
      "Iteration 221, loss = 0.20753737\n",
      "Iteration 222, loss = 0.20716421\n",
      "Iteration 223, loss = 0.20669464\n",
      "Iteration 224, loss = 0.20649732\n",
      "Iteration 225, loss = 0.20652106\n",
      "Iteration 226, loss = 0.20630346\n",
      "Iteration 227, loss = 0.20582771\n",
      "Iteration 228, loss = 0.20580650\n",
      "Iteration 229, loss = 0.20587574\n",
      "Iteration 230, loss = 0.20547535\n",
      "Iteration 231, loss = 0.20509070\n",
      "Iteration 232, loss = 0.20463880\n",
      "Iteration 233, loss = 0.20433409\n",
      "Iteration 234, loss = 0.20421339\n",
      "Iteration 235, loss = 0.20400723\n",
      "Iteration 236, loss = 0.20407519\n",
      "Iteration 237, loss = 0.20358261\n",
      "Iteration 238, loss = 0.20349023\n",
      "Iteration 239, loss = 0.20336577\n",
      "Iteration 240, loss = 0.20297187\n",
      "Iteration 241, loss = 0.20262981\n",
      "Iteration 242, loss = 0.20239849\n",
      "Iteration 243, loss = 0.20293537\n",
      "Iteration 244, loss = 0.20340356\n",
      "Iteration 245, loss = 0.20265093\n",
      "Iteration 246, loss = 0.20164871\n",
      "Iteration 247, loss = 0.20135380\n",
      "Iteration 248, loss = 0.20134180\n",
      "Iteration 249, loss = 0.20113209\n",
      "Iteration 250, loss = 0.20091459\n",
      "Iteration 251, loss = 0.20044062\n",
      "Iteration 252, loss = 0.20058812\n",
      "Iteration 253, loss = 0.20018757\n",
      "Iteration 254, loss = 0.20021394\n",
      "Iteration 255, loss = 0.19995769\n",
      "Iteration 256, loss = 0.19999489\n",
      "Iteration 257, loss = 0.20003632\n",
      "Iteration 258, loss = 0.19942711\n",
      "Iteration 259, loss = 0.19931479\n",
      "Iteration 260, loss = 0.19993151\n",
      "Iteration 261, loss = 0.19986016\n",
      "Iteration 262, loss = 0.19882480\n",
      "Iteration 263, loss = 0.19877491\n",
      "Iteration 264, loss = 0.19847598\n",
      "Iteration 265, loss = 0.19849136\n",
      "Iteration 266, loss = 0.19853380\n",
      "Iteration 267, loss = 0.19792307\n",
      "Iteration 268, loss = 0.19827068\n",
      "Iteration 269, loss = 0.19779374\n",
      "Iteration 270, loss = 0.19765370\n",
      "Iteration 271, loss = 0.19762722\n",
      "Iteration 272, loss = 0.19720876\n",
      "Iteration 273, loss = 0.19731517\n",
      "Iteration 274, loss = 0.19679531\n",
      "Iteration 275, loss = 0.19669884\n",
      "Iteration 276, loss = 0.19696998\n",
      "Iteration 277, loss = 0.19622546\n",
      "Iteration 278, loss = 0.19657680\n",
      "Iteration 279, loss = 0.19636961\n",
      "Iteration 280, loss = 0.19591947\n",
      "Iteration 281, loss = 0.19577861\n",
      "Iteration 282, loss = 0.19560582\n",
      "Iteration 283, loss = 0.19550274\n",
      "Iteration 284, loss = 0.19528006\n",
      "Iteration 285, loss = 0.19549958\n",
      "Iteration 286, loss = 0.19486766\n",
      "Iteration 287, loss = 0.19484648\n",
      "Iteration 288, loss = 0.19462629\n",
      "Iteration 289, loss = 0.19450875\n",
      "Iteration 290, loss = 0.19462261\n",
      "Iteration 291, loss = 0.19428046\n",
      "Iteration 292, loss = 0.19440262\n",
      "Iteration 293, loss = 0.19473861\n",
      "Iteration 294, loss = 0.19397680\n",
      "Iteration 295, loss = 0.19399977\n",
      "Iteration 296, loss = 0.19337896\n",
      "Iteration 297, loss = 0.19354826\n",
      "Iteration 298, loss = 0.19332561\n",
      "Iteration 299, loss = 0.19343219\n",
      "Iteration 300, loss = 0.19306371\n",
      "Iteration 301, loss = 0.19293746\n",
      "Iteration 302, loss = 0.19331125\n",
      "Iteration 303, loss = 0.19258048\n",
      "Iteration 304, loss = 0.19351051\n",
      "Iteration 305, loss = 0.19263549\n",
      "Iteration 306, loss = 0.19258934\n",
      "Iteration 307, loss = 0.19251291\n",
      "Iteration 308, loss = 0.19224084\n",
      "Iteration 309, loss = 0.19206026\n",
      "Iteration 310, loss = 0.19197688\n",
      "Iteration 311, loss = 0.19173839\n",
      "Iteration 312, loss = 0.19157847\n",
      "Iteration 313, loss = 0.19169329\n",
      "Iteration 314, loss = 0.19170763\n",
      "Iteration 315, loss = 0.19122897\n",
      "Iteration 316, loss = 0.19106025\n",
      "Iteration 317, loss = 0.19103365\n",
      "Iteration 318, loss = 0.19134752\n",
      "Iteration 319, loss = 0.19136596\n",
      "Iteration 320, loss = 0.19084157\n",
      "Iteration 321, loss = 0.19121877\n",
      "Iteration 322, loss = 0.19070944\n",
      "Iteration 323, loss = 0.19044082\n",
      "Iteration 324, loss = 0.19057653\n",
      "Iteration 325, loss = 0.19029091\n",
      "Iteration 326, loss = 0.18992131\n",
      "Iteration 327, loss = 0.19034765\n",
      "Iteration 328, loss = 0.19007011\n",
      "Iteration 329, loss = 0.18993993\n",
      "Iteration 330, loss = 0.18969385\n",
      "Iteration 331, loss = 0.19012117\n",
      "Iteration 332, loss = 0.19026249\n",
      "Iteration 333, loss = 0.19005417\n",
      "Iteration 334, loss = 0.19018926\n",
      "Iteration 335, loss = 0.19083273\n",
      "Iteration 336, loss = 0.18969047\n",
      "Iteration 337, loss = 0.18924236\n",
      "Iteration 338, loss = 0.18963069\n",
      "Iteration 339, loss = 0.18952627\n",
      "Iteration 340, loss = 0.18930142\n",
      "Iteration 341, loss = 0.18876456\n",
      "Iteration 342, loss = 0.18905940\n",
      "Iteration 343, loss = 0.18915298\n",
      "Iteration 344, loss = 0.18868825\n",
      "Iteration 345, loss = 0.18867203\n",
      "Iteration 346, loss = 0.18845629\n",
      "Iteration 347, loss = 0.18839284\n",
      "Iteration 348, loss = 0.18872440\n",
      "Iteration 349, loss = 0.18844768\n",
      "Iteration 350, loss = 0.18827522\n",
      "Iteration 351, loss = 0.18830868\n",
      "Iteration 352, loss = 0.18852856\n",
      "Iteration 353, loss = 0.18876115\n",
      "Iteration 354, loss = 0.18925279\n",
      "Iteration 355, loss = 0.18795294\n",
      "Iteration 356, loss = 0.18797077\n",
      "Iteration 357, loss = 0.18773113\n",
      "Iteration 358, loss = 0.18776603\n",
      "Iteration 359, loss = 0.18790891\n",
      "Iteration 360, loss = 0.18765224\n",
      "Iteration 361, loss = 0.18766844\n",
      "Iteration 362, loss = 0.18760707\n",
      "Iteration 363, loss = 0.18779181\n",
      "Iteration 364, loss = 0.18749101\n",
      "Iteration 365, loss = 0.18746984\n",
      "Iteration 366, loss = 0.18720087\n",
      "Iteration 367, loss = 0.18715538\n",
      "Iteration 368, loss = 0.18743710\n",
      "Iteration 369, loss = 0.18731922\n",
      "Iteration 370, loss = 0.18686674\n",
      "Iteration 371, loss = 0.18722144\n",
      "Iteration 372, loss = 0.18692948\n",
      "Iteration 373, loss = 0.18720917\n",
      "Iteration 374, loss = 0.18693594\n",
      "Iteration 375, loss = 0.18683135\n",
      "Iteration 376, loss = 0.18717456\n",
      "Iteration 377, loss = 0.18671561\n",
      "Iteration 378, loss = 0.18690500\n",
      "Iteration 379, loss = 0.18672356\n",
      "Iteration 380, loss = 0.18718063\n",
      "Iteration 381, loss = 0.18704627\n",
      "Iteration 382, loss = 0.18670508\n",
      "Iteration 383, loss = 0.18618040\n",
      "Iteration 384, loss = 0.18666672\n",
      "Iteration 385, loss = 0.18697697\n",
      "Iteration 386, loss = 0.18625831\n",
      "Iteration 387, loss = 0.18698651\n",
      "Iteration 388, loss = 0.18662827\n",
      "Iteration 389, loss = 0.18631847\n",
      "Iteration 390, loss = 0.18638428\n",
      "Iteration 391, loss = 0.18640124\n",
      "Iteration 392, loss = 0.18608380\n",
      "Iteration 393, loss = 0.18574294\n",
      "Iteration 394, loss = 0.18613491\n",
      "Iteration 395, loss = 0.18568974\n",
      "Iteration 396, loss = 0.18579769\n",
      "Iteration 397, loss = 0.18625683\n",
      "Iteration 398, loss = 0.18601555\n",
      "Iteration 399, loss = 0.18655724\n",
      "Iteration 400, loss = 0.18578990\n",
      "Iteration 401, loss = 0.18576559\n",
      "Iteration 402, loss = 0.18573184\n",
      "Iteration 403, loss = 0.18560716\n",
      "Iteration 404, loss = 0.18561039\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76955030\n",
      "Iteration 2, loss = 0.74319206\n",
      "Iteration 3, loss = 0.72147774\n",
      "Iteration 4, loss = 0.70427749\n",
      "Iteration 5, loss = 0.69023185\n",
      "Iteration 6, loss = 0.67711358\n",
      "Iteration 7, loss = 0.66330349\n",
      "Iteration 8, loss = 0.64804737\n",
      "Iteration 9, loss = 0.62976592\n",
      "Iteration 10, loss = 0.61082499\n",
      "Iteration 11, loss = 0.59116177\n",
      "Iteration 12, loss = 0.57373646\n",
      "Iteration 13, loss = 0.55739755\n",
      "Iteration 14, loss = 0.54331558\n",
      "Iteration 15, loss = 0.53160292\n",
      "Iteration 16, loss = 0.52158451\n",
      "Iteration 17, loss = 0.51205947\n",
      "Iteration 18, loss = 0.50383239\n",
      "Iteration 19, loss = 0.49655460\n",
      "Iteration 20, loss = 0.48961031\n",
      "Iteration 21, loss = 0.48345639\n",
      "Iteration 22, loss = 0.47762808\n",
      "Iteration 23, loss = 0.47221267\n",
      "Iteration 24, loss = 0.46709101\n",
      "Iteration 25, loss = 0.46211965\n",
      "Iteration 26, loss = 0.45753382\n",
      "Iteration 27, loss = 0.45299982\n",
      "Iteration 28, loss = 0.44890704\n",
      "Iteration 29, loss = 0.44485775\n",
      "Iteration 30, loss = 0.44109405\n",
      "Iteration 31, loss = 0.43746447\n",
      "Iteration 32, loss = 0.43369579\n",
      "Iteration 33, loss = 0.43004759\n",
      "Iteration 34, loss = 0.42666778\n",
      "Iteration 35, loss = 0.42322096\n",
      "Iteration 36, loss = 0.41977201\n",
      "Iteration 37, loss = 0.41664765\n",
      "Iteration 38, loss = 0.41341416\n",
      "Iteration 39, loss = 0.41036076\n",
      "Iteration 40, loss = 0.40724015\n",
      "Iteration 41, loss = 0.40436141\n",
      "Iteration 42, loss = 0.40140071\n",
      "Iteration 43, loss = 0.39870030\n",
      "Iteration 44, loss = 0.39587310\n",
      "Iteration 45, loss = 0.39323823\n",
      "Iteration 46, loss = 0.39060753\n",
      "Iteration 47, loss = 0.38819191\n",
      "Iteration 48, loss = 0.38544895\n",
      "Iteration 49, loss = 0.38323208\n",
      "Iteration 50, loss = 0.38063172\n",
      "Iteration 51, loss = 0.37835158\n",
      "Iteration 52, loss = 0.37610991\n",
      "Iteration 53, loss = 0.37409098\n",
      "Iteration 54, loss = 0.37177964\n",
      "Iteration 55, loss = 0.36931628\n",
      "Iteration 56, loss = 0.36729633\n",
      "Iteration 57, loss = 0.36520039\n",
      "Iteration 58, loss = 0.36306052\n",
      "Iteration 59, loss = 0.36115752\n",
      "Iteration 60, loss = 0.35915197\n",
      "Iteration 61, loss = 0.35708842\n",
      "Iteration 62, loss = 0.35511461\n",
      "Iteration 63, loss = 0.35319791\n",
      "Iteration 64, loss = 0.35137406\n",
      "Iteration 65, loss = 0.34937196\n",
      "Iteration 66, loss = 0.34771663\n",
      "Iteration 67, loss = 0.34583146\n",
      "Iteration 68, loss = 0.34389082\n",
      "Iteration 69, loss = 0.34209691\n",
      "Iteration 70, loss = 0.34051041\n",
      "Iteration 71, loss = 0.33859277\n",
      "Iteration 72, loss = 0.33688001\n",
      "Iteration 73, loss = 0.33522034\n",
      "Iteration 74, loss = 0.33370459\n",
      "Iteration 75, loss = 0.33211879\n",
      "Iteration 76, loss = 0.33060908\n",
      "Iteration 77, loss = 0.32890009\n",
      "Iteration 78, loss = 0.32782330\n",
      "Iteration 79, loss = 0.32613493\n",
      "Iteration 80, loss = 0.32433575\n",
      "Iteration 81, loss = 0.32300279\n",
      "Iteration 82, loss = 0.32178043\n",
      "Iteration 83, loss = 0.32039947\n",
      "Iteration 84, loss = 0.31886631\n",
      "Iteration 85, loss = 0.31759330\n",
      "Iteration 86, loss = 0.31618686\n",
      "Iteration 87, loss = 0.31525123\n",
      "Iteration 88, loss = 0.31338835\n",
      "Iteration 89, loss = 0.31197517\n",
      "Iteration 90, loss = 0.31096672\n",
      "Iteration 91, loss = 0.30947118\n",
      "Iteration 92, loss = 0.30808658\n",
      "Iteration 93, loss = 0.30713825\n",
      "Iteration 94, loss = 0.30569480\n",
      "Iteration 95, loss = 0.30480256\n",
      "Iteration 96, loss = 0.30339960\n",
      "Iteration 97, loss = 0.30225253\n",
      "Iteration 98, loss = 0.30121737\n",
      "Iteration 99, loss = 0.30009246\n",
      "Iteration 100, loss = 0.29886450\n",
      "Iteration 101, loss = 0.29838020\n",
      "Iteration 102, loss = 0.29646973\n",
      "Iteration 103, loss = 0.29591051\n",
      "Iteration 104, loss = 0.29469790\n",
      "Iteration 105, loss = 0.29347961\n",
      "Iteration 106, loss = 0.29254691\n",
      "Iteration 107, loss = 0.29124653\n",
      "Iteration 108, loss = 0.29044606\n",
      "Iteration 109, loss = 0.28936318\n",
      "Iteration 110, loss = 0.28844052\n",
      "Iteration 111, loss = 0.28744737\n",
      "Iteration 112, loss = 0.28681663\n",
      "Iteration 113, loss = 0.28567582\n",
      "Iteration 114, loss = 0.28508196\n",
      "Iteration 115, loss = 0.28401669\n",
      "Iteration 116, loss = 0.28293764\n",
      "Iteration 117, loss = 0.28190682\n",
      "Iteration 118, loss = 0.28114140\n",
      "Iteration 119, loss = 0.28099162\n",
      "Iteration 120, loss = 0.27923858\n",
      "Iteration 121, loss = 0.27853418\n",
      "Iteration 122, loss = 0.27806978\n",
      "Iteration 123, loss = 0.27700490\n",
      "Iteration 124, loss = 0.27632658\n",
      "Iteration 125, loss = 0.27544628\n",
      "Iteration 126, loss = 0.27473532\n",
      "Iteration 127, loss = 0.27360609\n",
      "Iteration 128, loss = 0.27314007\n",
      "Iteration 129, loss = 0.27230607\n",
      "Iteration 130, loss = 0.27133429\n",
      "Iteration 131, loss = 0.27078744\n",
      "Iteration 132, loss = 0.26989157\n",
      "Iteration 133, loss = 0.26933097\n",
      "Iteration 134, loss = 0.26836488\n",
      "Iteration 135, loss = 0.26770135\n",
      "Iteration 136, loss = 0.26705345\n",
      "Iteration 137, loss = 0.26636896\n",
      "Iteration 138, loss = 0.26565926\n",
      "Iteration 139, loss = 0.26481961\n",
      "Iteration 140, loss = 0.26457243\n",
      "Iteration 141, loss = 0.26376298\n",
      "Iteration 142, loss = 0.26274942\n",
      "Iteration 143, loss = 0.26177668\n",
      "Iteration 144, loss = 0.26144346\n",
      "Iteration 145, loss = 0.25997322\n",
      "Iteration 146, loss = 0.25909205\n",
      "Iteration 147, loss = 0.25848309\n",
      "Iteration 148, loss = 0.25750284\n",
      "Iteration 149, loss = 0.25703079\n",
      "Iteration 150, loss = 0.25635808\n",
      "Iteration 151, loss = 0.25583635\n",
      "Iteration 152, loss = 0.25505325\n",
      "Iteration 153, loss = 0.25454673\n",
      "Iteration 154, loss = 0.25394725\n",
      "Iteration 155, loss = 0.25308700\n",
      "Iteration 156, loss = 0.25249028\n",
      "Iteration 157, loss = 0.25213407\n",
      "Iteration 158, loss = 0.25133098\n",
      "Iteration 159, loss = 0.25081723\n",
      "Iteration 160, loss = 0.25023864\n",
      "Iteration 161, loss = 0.24983399\n",
      "Iteration 162, loss = 0.24949188\n",
      "Iteration 163, loss = 0.24876749\n",
      "Iteration 164, loss = 0.24812173\n",
      "Iteration 165, loss = 0.24811959\n",
      "Iteration 166, loss = 0.24731500\n",
      "Iteration 167, loss = 0.24677932\n",
      "Iteration 168, loss = 0.24604815\n",
      "Iteration 169, loss = 0.24585915\n",
      "Iteration 170, loss = 0.24522333\n",
      "Iteration 171, loss = 0.24467477\n",
      "Iteration 172, loss = 0.24408999\n",
      "Iteration 173, loss = 0.24378809\n",
      "Iteration 174, loss = 0.24331973\n",
      "Iteration 175, loss = 0.24295695\n",
      "Iteration 176, loss = 0.24242181\n",
      "Iteration 177, loss = 0.24192096\n",
      "Iteration 178, loss = 0.24171723\n",
      "Iteration 179, loss = 0.24092626\n",
      "Iteration 180, loss = 0.24068696\n",
      "Iteration 181, loss = 0.23997957\n",
      "Iteration 182, loss = 0.24083105\n",
      "Iteration 183, loss = 0.23955375\n",
      "Iteration 184, loss = 0.23897463\n",
      "Iteration 185, loss = 0.23870661\n",
      "Iteration 186, loss = 0.23846071\n",
      "Iteration 187, loss = 0.23842144\n",
      "Iteration 188, loss = 0.23698159\n",
      "Iteration 189, loss = 0.23669914\n",
      "Iteration 190, loss = 0.23612046\n",
      "Iteration 191, loss = 0.23576547\n",
      "Iteration 192, loss = 0.23513166\n",
      "Iteration 193, loss = 0.23524135\n",
      "Iteration 194, loss = 0.23439427\n",
      "Iteration 195, loss = 0.23451004\n",
      "Iteration 196, loss = 0.23393125\n",
      "Iteration 197, loss = 0.23341597\n",
      "Iteration 198, loss = 0.23340473\n",
      "Iteration 199, loss = 0.23270688\n",
      "Iteration 200, loss = 0.23281081\n",
      "Iteration 201, loss = 0.23213060\n",
      "Iteration 202, loss = 0.23140931\n",
      "Iteration 203, loss = 0.23103138\n",
      "Iteration 204, loss = 0.23078222\n",
      "Iteration 205, loss = 0.23159385\n",
      "Iteration 206, loss = 0.23032588\n",
      "Iteration 207, loss = 0.22998072\n",
      "Iteration 208, loss = 0.22929844\n",
      "Iteration 209, loss = 0.22933895\n",
      "Iteration 210, loss = 0.22891538\n",
      "Iteration 211, loss = 0.22828693\n",
      "Iteration 212, loss = 0.22791100\n",
      "Iteration 213, loss = 0.22784947\n",
      "Iteration 214, loss = 0.22776799\n",
      "Iteration 215, loss = 0.22712766\n",
      "Iteration 216, loss = 0.22682198\n",
      "Iteration 217, loss = 0.22631385\n",
      "Iteration 218, loss = 0.22620956\n",
      "Iteration 219, loss = 0.22565089\n",
      "Iteration 220, loss = 0.22602157\n",
      "Iteration 221, loss = 0.22511534\n",
      "Iteration 222, loss = 0.22528283\n",
      "Iteration 223, loss = 0.22452364\n",
      "Iteration 224, loss = 0.22418748\n",
      "Iteration 225, loss = 0.22382495\n",
      "Iteration 226, loss = 0.22373599\n",
      "Iteration 227, loss = 0.22339218\n",
      "Iteration 228, loss = 0.22322567\n",
      "Iteration 229, loss = 0.22282494\n",
      "Iteration 230, loss = 0.22228319\n",
      "Iteration 231, loss = 0.22238675\n",
      "Iteration 232, loss = 0.22171741\n",
      "Iteration 233, loss = 0.22138527\n",
      "Iteration 234, loss = 0.22109142\n",
      "Iteration 235, loss = 0.22120909\n",
      "Iteration 236, loss = 0.22069887\n",
      "Iteration 237, loss = 0.22009942\n",
      "Iteration 238, loss = 0.21997097\n",
      "Iteration 239, loss = 0.21995042\n",
      "Iteration 240, loss = 0.21949483\n",
      "Iteration 241, loss = 0.21933718\n",
      "Iteration 242, loss = 0.21943776\n",
      "Iteration 243, loss = 0.21867391\n",
      "Iteration 244, loss = 0.21886938\n",
      "Iteration 245, loss = 0.21871932\n",
      "Iteration 246, loss = 0.21815221\n",
      "Iteration 247, loss = 0.21818502\n",
      "Iteration 248, loss = 0.21797537\n",
      "Iteration 249, loss = 0.21757437\n",
      "Iteration 250, loss = 0.21725747\n",
      "Iteration 251, loss = 0.21777032\n",
      "Iteration 252, loss = 0.21820719\n",
      "Iteration 253, loss = 0.21720673\n",
      "Iteration 254, loss = 0.21637675\n",
      "Iteration 255, loss = 0.21615048\n",
      "Iteration 256, loss = 0.21579682\n",
      "Iteration 257, loss = 0.21560199\n",
      "Iteration 258, loss = 0.21516153\n",
      "Iteration 259, loss = 0.21540526\n",
      "Iteration 260, loss = 0.21487458\n",
      "Iteration 261, loss = 0.21525476\n",
      "Iteration 262, loss = 0.21491591\n",
      "Iteration 263, loss = 0.21401128\n",
      "Iteration 264, loss = 0.21377907\n",
      "Iteration 265, loss = 0.21355881\n",
      "Iteration 266, loss = 0.21343816\n",
      "Iteration 267, loss = 0.21299766\n",
      "Iteration 268, loss = 0.21268336\n",
      "Iteration 269, loss = 0.21248308\n",
      "Iteration 270, loss = 0.21223246\n",
      "Iteration 271, loss = 0.21204375\n",
      "Iteration 272, loss = 0.21232391\n",
      "Iteration 273, loss = 0.21151204\n",
      "Iteration 274, loss = 0.21109314\n",
      "Iteration 275, loss = 0.21103095\n",
      "Iteration 276, loss = 0.21100767\n",
      "Iteration 277, loss = 0.21069237\n",
      "Iteration 278, loss = 0.21050291\n",
      "Iteration 279, loss = 0.21076626\n",
      "Iteration 280, loss = 0.20993093\n",
      "Iteration 281, loss = 0.21003362\n",
      "Iteration 282, loss = 0.20968098\n",
      "Iteration 283, loss = 0.20929308\n",
      "Iteration 284, loss = 0.20893757\n",
      "Iteration 285, loss = 0.20892872\n",
      "Iteration 286, loss = 0.20871161\n",
      "Iteration 287, loss = 0.20869717\n",
      "Iteration 288, loss = 0.20992817\n",
      "Iteration 289, loss = 0.20868183\n",
      "Iteration 290, loss = 0.20877012\n",
      "Iteration 291, loss = 0.20787707\n",
      "Iteration 292, loss = 0.20861773\n",
      "Iteration 293, loss = 0.20757069\n",
      "Iteration 294, loss = 0.20705915\n",
      "Iteration 295, loss = 0.20689687\n",
      "Iteration 296, loss = 0.20666736\n",
      "Iteration 297, loss = 0.20690630\n",
      "Iteration 298, loss = 0.20617034\n",
      "Iteration 299, loss = 0.20645929\n",
      "Iteration 300, loss = 0.20619451\n",
      "Iteration 301, loss = 0.20555012\n",
      "Iteration 302, loss = 0.20564194\n",
      "Iteration 303, loss = 0.20542460\n",
      "Iteration 304, loss = 0.20504882\n",
      "Iteration 305, loss = 0.20469851\n",
      "Iteration 306, loss = 0.20467522\n",
      "Iteration 307, loss = 0.20441190\n",
      "Iteration 308, loss = 0.20424587\n",
      "Iteration 309, loss = 0.20409653\n",
      "Iteration 310, loss = 0.20454894\n",
      "Iteration 311, loss = 0.20417302\n",
      "Iteration 312, loss = 0.20374681\n",
      "Iteration 313, loss = 0.20304016\n",
      "Iteration 314, loss = 0.20301346\n",
      "Iteration 315, loss = 0.20276400\n",
      "Iteration 316, loss = 0.20273562\n",
      "Iteration 317, loss = 0.20249643\n",
      "Iteration 318, loss = 0.20220184\n",
      "Iteration 319, loss = 0.20207937\n",
      "Iteration 320, loss = 0.20183601\n",
      "Iteration 321, loss = 0.20250928\n",
      "Iteration 322, loss = 0.20214213\n",
      "Iteration 323, loss = 0.20157463\n",
      "Iteration 324, loss = 0.20064738\n",
      "Iteration 325, loss = 0.20113101\n",
      "Iteration 326, loss = 0.20078771\n",
      "Iteration 327, loss = 0.20089400\n",
      "Iteration 328, loss = 0.20060869\n",
      "Iteration 329, loss = 0.20036462\n",
      "Iteration 330, loss = 0.20011343\n",
      "Iteration 331, loss = 0.19985593\n",
      "Iteration 332, loss = 0.19981103\n",
      "Iteration 333, loss = 0.20040953\n",
      "Iteration 334, loss = 0.19944536\n",
      "Iteration 335, loss = 0.19917239\n",
      "Iteration 336, loss = 0.19899365\n",
      "Iteration 337, loss = 0.19868741\n",
      "Iteration 338, loss = 0.19854716\n",
      "Iteration 339, loss = 0.19869423\n",
      "Iteration 340, loss = 0.19845933\n",
      "Iteration 341, loss = 0.19955460\n",
      "Iteration 342, loss = 0.19797703\n",
      "Iteration 343, loss = 0.19797561\n",
      "Iteration 344, loss = 0.19759466\n",
      "Iteration 345, loss = 0.19708434\n",
      "Iteration 346, loss = 0.19699517\n",
      "Iteration 347, loss = 0.19718615\n",
      "Iteration 348, loss = 0.19667387\n",
      "Iteration 349, loss = 0.19668529\n",
      "Iteration 350, loss = 0.19710910\n",
      "Iteration 351, loss = 0.19604449\n",
      "Iteration 352, loss = 0.19629183\n",
      "Iteration 353, loss = 0.19581453\n",
      "Iteration 354, loss = 0.19562964\n",
      "Iteration 355, loss = 0.19558177\n",
      "Iteration 356, loss = 0.19530047\n",
      "Iteration 357, loss = 0.19521096\n",
      "Iteration 358, loss = 0.19525634\n",
      "Iteration 359, loss = 0.19489380\n",
      "Iteration 360, loss = 0.19499734\n",
      "Iteration 361, loss = 0.19465652\n",
      "Iteration 362, loss = 0.19424206\n",
      "Iteration 363, loss = 0.19430972\n",
      "Iteration 364, loss = 0.19427652\n",
      "Iteration 365, loss = 0.19424516\n",
      "Iteration 366, loss = 0.19414922\n",
      "Iteration 367, loss = 0.19482246\n",
      "Iteration 368, loss = 0.19427502\n",
      "Iteration 369, loss = 0.19324238\n",
      "Iteration 370, loss = 0.19368031\n",
      "Iteration 371, loss = 0.19394982\n",
      "Iteration 372, loss = 0.19342880\n",
      "Iteration 373, loss = 0.19286630\n",
      "Iteration 374, loss = 0.19405694\n",
      "Iteration 375, loss = 0.19323393\n",
      "Iteration 376, loss = 0.19369107\n",
      "Iteration 377, loss = 0.19277249\n",
      "Iteration 378, loss = 0.19220001\n",
      "Iteration 379, loss = 0.19209109\n",
      "Iteration 380, loss = 0.19219778\n",
      "Iteration 381, loss = 0.19180549\n",
      "Iteration 382, loss = 0.19170058\n",
      "Iteration 383, loss = 0.19186622\n",
      "Iteration 384, loss = 0.19158296\n",
      "Iteration 385, loss = 0.19181343\n",
      "Iteration 386, loss = 0.19156247\n",
      "Iteration 387, loss = 0.19127718\n",
      "Iteration 388, loss = 0.19123669\n",
      "Iteration 389, loss = 0.19091435\n",
      "Iteration 390, loss = 0.19118524\n",
      "Iteration 391, loss = 0.19105159\n",
      "Iteration 392, loss = 0.19078732\n",
      "Iteration 393, loss = 0.19046960\n",
      "Iteration 394, loss = 0.19032010\n",
      "Iteration 395, loss = 0.19059156\n",
      "Iteration 396, loss = 0.19060504\n",
      "Iteration 397, loss = 0.19060570\n",
      "Iteration 398, loss = 0.19013088\n",
      "Iteration 399, loss = 0.19006136\n",
      "Iteration 400, loss = 0.18970287\n",
      "Iteration 401, loss = 0.18978358\n",
      "Iteration 402, loss = 0.19002463\n",
      "Iteration 403, loss = 0.18965848\n",
      "Iteration 404, loss = 0.18927523\n",
      "Iteration 405, loss = 0.18962171\n",
      "Iteration 406, loss = 0.19049204\n",
      "Iteration 407, loss = 0.18893622\n",
      "Iteration 408, loss = 0.18940228\n",
      "Iteration 409, loss = 0.18937106\n",
      "Iteration 410, loss = 0.18926230\n",
      "Iteration 411, loss = 0.18918122\n",
      "Iteration 412, loss = 0.18836390\n",
      "Iteration 413, loss = 0.18888313\n",
      "Iteration 414, loss = 0.18857164\n",
      "Iteration 415, loss = 0.18820107\n",
      "Iteration 416, loss = 0.18830861\n",
      "Iteration 417, loss = 0.18816927\n",
      "Iteration 418, loss = 0.18767043\n",
      "Iteration 419, loss = 0.18819000\n",
      "Iteration 420, loss = 0.18762072\n",
      "Iteration 421, loss = 0.18782542\n",
      "Iteration 422, loss = 0.18738021\n",
      "Iteration 423, loss = 0.18751205\n",
      "Iteration 424, loss = 0.18730163\n",
      "Iteration 425, loss = 0.18859046\n",
      "Iteration 426, loss = 0.18759499\n",
      "Iteration 427, loss = 0.18695547\n",
      "Iteration 428, loss = 0.18695821\n",
      "Iteration 429, loss = 0.18793485\n",
      "Iteration 430, loss = 0.18693861\n",
      "Iteration 431, loss = 0.18738143\n",
      "Iteration 432, loss = 0.18661927\n",
      "Iteration 433, loss = 0.18652158\n",
      "Iteration 434, loss = 0.18642926\n",
      "Iteration 435, loss = 0.18639647\n",
      "Iteration 436, loss = 0.18701869\n",
      "Iteration 437, loss = 0.18646693\n",
      "Iteration 438, loss = 0.18648088\n",
      "Iteration 439, loss = 0.18601577\n",
      "Iteration 440, loss = 0.18568022\n",
      "Iteration 441, loss = 0.18657762\n",
      "Iteration 442, loss = 0.18579592\n",
      "Iteration 443, loss = 0.18578179\n",
      "Iteration 444, loss = 0.18660436\n",
      "Iteration 445, loss = 0.18557690\n",
      "Iteration 446, loss = 0.18478264\n",
      "Iteration 447, loss = 0.18455625\n",
      "Iteration 448, loss = 0.18422757\n",
      "Iteration 449, loss = 0.18380861\n",
      "Iteration 450, loss = 0.18297979\n",
      "Iteration 451, loss = 0.18282147\n",
      "Iteration 452, loss = 0.18291451\n",
      "Iteration 453, loss = 0.18271454\n",
      "Iteration 454, loss = 0.18257287\n",
      "Iteration 455, loss = 0.18259625\n",
      "Iteration 456, loss = 0.18221574\n",
      "Iteration 457, loss = 0.18188932\n",
      "Iteration 458, loss = 0.18237737\n",
      "Iteration 459, loss = 0.18172941\n",
      "Iteration 460, loss = 0.18208450\n",
      "Iteration 461, loss = 0.18177044\n",
      "Iteration 462, loss = 0.18157876\n",
      "Iteration 463, loss = 0.18131067\n",
      "Iteration 464, loss = 0.18126184\n",
      "Iteration 465, loss = 0.18138989\n",
      "Iteration 466, loss = 0.18170918\n",
      "Iteration 467, loss = 0.18128121\n",
      "Iteration 468, loss = 0.18192041\n",
      "Iteration 469, loss = 0.18236337\n",
      "Iteration 470, loss = 0.18182914\n",
      "Iteration 471, loss = 0.18074659\n",
      "Iteration 472, loss = 0.18179163\n",
      "Iteration 473, loss = 0.18102700\n",
      "Iteration 474, loss = 0.18076255\n",
      "Iteration 475, loss = 0.18038777\n",
      "Iteration 476, loss = 0.18081593\n",
      "Iteration 477, loss = 0.18098108\n",
      "Iteration 478, loss = 0.18081995\n",
      "Iteration 479, loss = 0.18042146\n",
      "Iteration 480, loss = 0.17971206\n",
      "Iteration 481, loss = 0.17979842\n",
      "Iteration 482, loss = 0.18024544\n",
      "Iteration 483, loss = 0.17974993\n",
      "Iteration 484, loss = 0.17979503\n",
      "Iteration 485, loss = 0.17953710\n",
      "Iteration 486, loss = 0.17986142\n",
      "Iteration 487, loss = 0.17939312\n",
      "Iteration 488, loss = 0.17901747\n",
      "Iteration 489, loss = 0.17961855\n",
      "Iteration 490, loss = 0.17880949\n",
      "Iteration 491, loss = 0.17957263\n",
      "Iteration 492, loss = 0.18016530\n",
      "Iteration 493, loss = 0.17908561\n",
      "Iteration 494, loss = 0.17904256\n",
      "Iteration 495, loss = 0.17872961\n",
      "Iteration 496, loss = 0.17923950\n",
      "Iteration 497, loss = 0.17956015\n",
      "Iteration 498, loss = 0.17870544\n",
      "Iteration 499, loss = 0.17945674\n",
      "Iteration 500, loss = 0.17870737\n",
      "Iteration 501, loss = 0.17918559\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "modelo = MLPClassifier(\n",
    "    hidden_layer_sizes = (7, 7, 7), activation = 'relu',\n",
    "    solver = 'adam', max_iter = 1000, alpha = 0.1,\n",
    "    batch_size = 128, learning_rate = 'adaptive',\n",
    "    learning_rate_init = 0.001,\n",
    "    tol = 0.0001, random_state = 3, verbose = True\n",
    ")\n",
    "resultado = cross_val_score(\n",
    "    modelo, previsoresHot_esc,\n",
    "    alvo, cv = kfold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Média: 91.23%\n"
     ]
    }
   ],
   "source": [
    "print(\"Acurácia Média: %.2f%%\" % (resultado.mean() * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
